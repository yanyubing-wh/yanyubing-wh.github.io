<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="鄢玉兵的博客" type="application/atom+xml" />






<meta name="description" content="1：离线数仓-数据采集1：项目需求 12341：数据采集平台搭建2：实现用户行为数仓的分层搭建3：实现业务数仓的分层搭建4：针对数据仓库中的数据进行，留存，转换率，复购率，活跃等报表分析  2：项目选型 1234数据采集传输：Flume,Kafka,Sqoop 	     	Logstash DataX数据储存：MySql,HDFS						 Hbase,Redis,MongoDB数据计算：Hi">
<meta property="og:type" content="article">
<meta property="og:title" content="离线数仓-数据采集总结">
<meta property="og:url" content="https:&#x2F;&#x2F;yanyubing.xyz&#x2F;2019&#x2F;12&#x2F;06&#x2F;%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%80%BB%E7%BB%93&#x2F;index.html">
<meta property="og:site_name" content="鄢玉兵的博客">
<meta property="og:description" content="1：离线数仓-数据采集1：项目需求 12341：数据采集平台搭建2：实现用户行为数仓的分层搭建3：实现业务数仓的分层搭建4：针对数据仓库中的数据进行，留存，转换率，复购率，活跃等报表分析  2：项目选型 1234数据采集传输：Flume,Kafka,Sqoop 	     	Logstash DataX数据储存：MySql,HDFS						 Hbase,Redis,MongoDB数据计算：Hi">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-12-06T08:23:58.540Z">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://yanyubing.xyz/2019/12/06/离线数仓-数据采集总结/"/>





  <title>离线数仓-数据采集总结 | 鄢玉兵的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
<a href="https://yanyubing.xyz"><img width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_red_aa0000.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">鄢玉兵的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yanyubing.xyz/2019/12/06/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="鄢玉兵">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="鄢玉兵的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">离线数仓-数据采集总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-06T11:02:55+08:00">
                2019-12-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1：离线数仓-数据采集"><a href="#1：离线数仓-数据采集" class="headerlink" title="1：离线数仓-数据采集"></a>1：离线数仓-数据采集</h1><p>1：项目需求</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：数据采集平台搭建</span><br><span class="line">2：实现用户行为数仓的分层搭建</span><br><span class="line">3：实现业务数仓的分层搭建</span><br><span class="line">4：针对数据仓库中的数据进行，留存，转换率，复购率，活跃等报表分析</span><br></pre></td></tr></table></figure>

<p>2：项目选型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">数据采集传输：Flume,Kafka,Sqoop 	     	Logstash DataX</span><br><span class="line">数据储存：MySql,HDFS						 Hbase,Redis,MongoDB</span><br><span class="line">数据计算：Hive,Tez,Spark 				 Flink,Storm</span><br><span class="line">数据查询：Presto,Druid					 Impala,Kylin</span><br></pre></td></tr></table></figure>

<p>3：框架版本选择</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1)Apache:一般大厂使用，运维麻烦</span><br><span class="line">2）CDH：中小公司使用，主件兼容性好</span><br><span class="line">3）HDP：可以进行二次开发，但是没有CDH稳定</span><br><span class="line"></span><br><span class="line">注意：框架不要选择最新，一般选择半年前左右的稳定版本</span><br></pre></td></tr></table></figure>

<p>4：服务器选择</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：机器成本考虑</span><br><span class="line">2：运维成本考虑</span><br><span class="line"></span><br><span class="line">云主机价格贵，但是运维轻松，不需要专业运维人员，省去运维成本；但是数据安全性不可靠！</span><br><span class="line">有实力就物理机，省钱就云主机。</span><br></pre></td></tr></table></figure>

<p>5：如何确定集群规模</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1：第一个方面就是日活跃数据，和多久不增加集群规模</span><br><span class="line">2：数仓分层，数据需要扩容</span><br></pre></td></tr></table></figure>

<p>6：HDFS储存多目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1：在hdfs-site.xml文件中配置多目录，最好提前配置好，否则更改目录需要重新启动集群</span><br></pre></td></tr></table></figure>

<p>7：支持LZO压缩配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">1）先下载lzo的jar项目</span><br><span class="line">https://github.com/twitter/hadoop-lzo/archive/master.zip</span><br><span class="line"></span><br><span class="line">2）下载后的文件名是hadoop-lzo-master，它是一个zip格式的压缩包，先进行解压，然后用maven编译。生成hadoop-lzo-0.4.20.jar。</span><br><span class="line"></span><br><span class="line">3）将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/</span><br><span class="line"> pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/share/hadoop/common</span><br><span class="line"> ls</span><br><span class="line">hadoop-lzo-0.4.20.jar</span><br><span class="line"></span><br><span class="line">4）同步hadoop-lzo-0.4.20.jar到hadoop103、hadoop104</span><br><span class="line">$ xsync hadoop-lzo-0.4.20.jar</span><br><span class="line"></span><br><span class="line">5）core-site.xml增加配置支持LZO压缩</span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">6）同步core-site.xml到hadoop103、hadoop104</span><br><span class="line">$ xsync core-site.xml</span><br><span class="line"></span><br><span class="line">7）启动及查看集群</span><br><span class="line">$ sbin/start-dfs.sh</span><br><span class="line">$ sbin/start-yarn.sh</span><br><span class="line">（1）web和进程查看</span><br><span class="line">Web查看：http://hadoop102:50070</span><br><span class="line">进程查看：jps查看各个节点状态。</span><br></pre></td></tr></table></figure>

<p>8：基准测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：测试HDFS写性能</span><br><span class="line">	测试内容：向HDFS集群写10个128M的文件</span><br><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2：测试HDFS读性能</span><br><span class="line">	测试内容：读取HDFS集群10个128M的文件</span><br><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">3：使用Sort程序评测MapReduce</span><br><span class="line">（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数</span><br><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar randomwriter random-data</span><br><span class="line"></span><br><span class="line">（2）执行Sort程序</span><br><span class="line"> hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar sort random-data sorted-data</span><br><span class="line"></span><br><span class="line">（3）验证数据是否真正排好序了</span><br><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar testmapredsort -sortInput random-data -sortOutput sorted-data</span><br></pre></td></tr></table></figure>

<p>9：Hadoop参数调优</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）HDFS参数调优hdfs-site.xml</span><br><span class="line">（1）dfs.namenode.handler.count=20 * log2(Cluster Size)，比如集群规模为8台时，此参数设置为60</span><br><span class="line">The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.</span><br><span class="line">NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。</span><br><span class="line">（2）编辑日志存储路径dfs.namenode.edits.dir设置与镜像文件存储路径dfs.namenode.name.dir尽量分开，达到最低写入延迟</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2）YARN参数调优yarn-site.xml</span><br><span class="line">（1）情景描述：总共7台机器，每天几亿条数据，数据源-&gt;Flume-&gt;Kafka-&gt;HDFS-&gt;Hive</span><br><span class="line">面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。</span><br><span class="line">（2）解决办法：</span><br><span class="line">内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。</span><br><span class="line">（a）yarn.nodemanager.resource.memory-mb</span><br><span class="line">表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。</span><br><span class="line">（b）yarn.scheduler.maximum-allocation-mb</span><br><span class="line">单个任务可申请的最多物理内存量，默认是8192（MB）。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3）Hadoop宕机</span><br><span class="line">（1）如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是8192MB）</span><br><span class="line">（2）如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。高峰期的时候用Kafka进行缓存，高峰期过去数据同步会自动跟上。</span><br></pre></td></tr></table></figure>

<p>10：Linux环境变量：执行脚本文件需要设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）修改/etc/profile文件：用来设置系统环境参数，比如$PATH. 这里面的环境变量是对系统内所有用户生效。使用bash命令，需要source  /etc/profile一下。</span><br><span class="line">2）修改~/.bashrc文件：针对某一个特定的用户，环境变量的设置只对该用户自己有效。使用bash命令，只要以该用户身份运行命令行就会读取该文件。</span><br><span class="line">3）把/etc/profile里面的环境变量追加到~/.bashrc目录，需要执行的文件都需要追加文件</span><br><span class="line">cat /etc/profile &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>11：集群脚本分类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：单启服务的脚本</span><br><span class="line">2：多启有依赖关系的服务脚本，这时候需要考虑到服务启动的延迟时间，sleep一下。</span><br><span class="line">3：查看所有服务进程的脚本</span><br><span class="line">4：集群时间同步脚本</span><br><span class="line">5：集群分发sxync脚本</span><br></pre></td></tr></table></figure>

<p>12：日志采集对应的Flume组件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1）Source</span><br><span class="line">（1）Taildir Source相比Exec Source、Spooling Directory Source的优势</span><br><span class="line">TailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。</span><br><span class="line">Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。</span><br><span class="line">Spooling Directory Source监控目录，不支持断点续传。</span><br><span class="line">（2）batchSize大小如何设置？</span><br><span class="line">答：Event 1K左右时，500-1000合适（默认为100）</span><br><span class="line">2）Channel</span><br><span class="line">采用Kafka Channel，省去了Sink，提高了效率。</span><br></pre></td></tr></table></figure>

<p>13：Flume的ETL和分类型拦截器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">本项目中自定义了两个拦截器，分别是：ETL拦截器、日志类型区分拦截器。</span><br><span class="line">ETL拦截器主要用于，过滤时间戳不合法和Json数据不完整的日志</span><br><span class="line">日志类型区分拦截器主要用于，将启动日志和事件日志区分开来，方便发往Kafka的不同Topic。</span><br></pre></td></tr></table></figure>

<p>14：kafka监控——Kafka Manager安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Kafka Manager是yahoo的一个Kafka监控管理项目。</span><br><span class="line">1）下载地址</span><br><span class="line">https://github.com/yahoo/kafka-manager</span><br><span class="line">下载之后编译源码，编译完成后，拷贝出：kafka-manager-1.3.3.22.zip</span><br><span class="line"></span><br><span class="line">2）拷贝kafka-manager-1.3.3.22.zip到hadoop102的/opt/module目录</span><br><span class="line">$ pwd</span><br><span class="line">/opt/module</span><br><span class="line"></span><br><span class="line">3）解压kafka-manager-1.3.3.22.zip到/opt/module目录</span><br><span class="line">$ unzip kafka-manager-1.3.3.22.zip</span><br><span class="line"></span><br><span class="line">4）进入到/opt/module/kafka-manager-1.3.3.22/conf目录，在application.conf文件中修改kafka-manager.zkhosts</span><br><span class="line">$ vim application.conf</span><br><span class="line">修改为：</span><br><span class="line">kafka-manager.zkhosts=&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span><br><span class="line"></span><br><span class="line">5）启动KafkaManager</span><br><span class="line">$ nohup bin/kafka-manager   -Dhttp.port=7456 &gt;/opt/module/kafka-manager-1.3.3.22/start.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">6）在浏览器中打开</span><br><span class="line">http://hadoop102:7456</span><br></pre></td></tr></table></figure>

<p>15：Kafka压力测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">1）Kafka压测</span><br><span class="line">用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。 </span><br><span class="line">kafka-consumer-perf-test.sh</span><br><span class="line">kafka-producer-perf-test.sh</span><br><span class="line"></span><br><span class="line">2）Kafka Producer压力测试</span><br><span class="line">（1）在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下</span><br><span class="line">$ bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput 1000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">说明：record-size是一条信息有多大，单位是字节。num-records是总共发送多少条信息。throughput 是每秒多少条信息。</span><br><span class="line">（2）Kafka会打印下面的信息</span><br><span class="line">5000 records sent, 999.4 records/sec (0.10 MB/sec), 1.9 ms avg latency, 254.0 max latency.</span><br><span class="line">5002 records sent, 1000.4 records/sec (0.10 MB/sec), 0.7 ms avg latency, 12.0 max latency.</span><br><span class="line">5001 records sent, 1000.0 records/sec (0.10 MB/sec), 0.8 ms avg latency, 4.0 max latency.</span><br><span class="line">5000 records sent, 1000.0 records/sec (0.10 MB/sec), 0.7 ms avg latency, 3.0 max latency.</span><br><span class="line">5000 records sent, 1000.0 records/sec (0.10 MB/sec), 0.8 ms avg latency, 5.0 max latency.</span><br><span class="line">参数解析：本例中一共写入10w条消息，每秒向Kafka写入了0.10MB的数据，平均是1000条消息/秒，每次写入的平均延迟为0.8毫秒，最大的延迟为254毫秒。</span><br><span class="line"></span><br><span class="line">3）Kafka Consumer压力测试</span><br><span class="line">Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。</span><br><span class="line">$ bin/kafka-consumer-perf-test.sh --zookeeper hadoop102:2181 --topic test --fetch-size 10000 --messages 10000000 --threads 1</span><br><span class="line">参数说明：</span><br><span class="line">--zookeeper 指定zookeeper的链接信息</span><br><span class="line">--topic 指定topic的名称</span><br><span class="line">--fetch-size 指定每次fetch的数据的大小</span><br><span class="line">--messages 总共要消费的消息个数</span><br><span class="line">测试结果说明：</span><br><span class="line">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec</span><br><span class="line">2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153</span><br><span class="line">开始测试时间，测试结束数据，最大吞吐率9.5368MB/s，平均每秒消费2.0714MB/s，最大每秒消费100010条，平均每秒消费21722.4153条。</span><br></pre></td></tr></table></figure>

<p>16：Kafka机器数量计算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Kafka机器数量（经验公式）=2*（峰值生产速度*副本数/100）+1</span><br><span class="line">	</span><br><span class="line">1:先要预估一天大概产生多少数据，然后用Kafka自带的生产压测（只测试Kafka的写入速度，保证数据不积压），计算出峰值生产速度。再根据设定的副本数，就能预估出需要部署Kafka的数量</span><br><span class="line"></span><br><span class="line">2:比如我们采用压力测试测出写入的速度是10M/s一台，峰值的业务数据的速度是50M/s。副本数为2。</span><br><span class="line">Kafka机器数量=2*（50*2/100）+ 1=3台</span><br></pre></td></tr></table></figure>

<p>17：Flume链接Kafka-hdfs，内存优化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1）问题描述：如果启动消费Flume抛出如下异常</span><br><span class="line">ERROR hdfs.HDFSEventSink: process failed</span><br><span class="line">java.lang.OutOfMemoryError: GC overhead limit exceeded</span><br><span class="line"></span><br><span class="line">2）解决方案步骤：</span><br><span class="line">（1）在hadoop102服务器的/opt/module/flume/conf/flume-env.sh文件中增加如下配置</span><br><span class="line">export JAVA_OPTS=&quot;-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote&quot;</span><br><span class="line">（2）同步配置到hadoop103、hadoop104服务器</span><br><span class="line">$ xsync flume-env.sh</span><br><span class="line"></span><br><span class="line">3）Flume内存参数设置及优化</span><br><span class="line">JVM heap一般设置为4G或更高，部署在单独的服务器上（4核8线程16G内存）</span><br><span class="line">-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。</span><br></pre></td></tr></table></figure>

<p>18：Flume组件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1）FileChannel和MemoryChannel区别</span><br><span class="line">MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求</span><br><span class="line">FileChannel传输速度相对于Memory慢，但数据安全保障高，Agent进程挂掉也可以从失败中恢复数据。</span><br><span class="line"></span><br><span class="line">2）FileChannel优化</span><br><span class="line">通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。</span><br><span class="line">官方说明如下：</span><br><span class="line">Comma separated list of directories for storing log files. Using multiple directories on separate disks can improve file channel peformance</span><br><span class="line">checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据</span><br><span class="line"></span><br><span class="line">3）Sink：HDFS Sink</span><br><span class="line">（1）HDFS存入大量小文件，有什么影响？</span><br><span class="line">元数据层面：每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命</span><br><span class="line">计算层面：默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。</span><br><span class="line"></span><br><span class="line">（2）HDFS小文件处理</span><br><span class="line">官方默认的这三个参数配置写入HDFS后会产生小文件，hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount</span><br><span class="line">基于以上hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0，hdfs.roundValue=10，hdfs.roundUnit= second几个参数综合作用，效果如下：</span><br><span class="line">（1）tmp文件在达到128M时会滚动生成正式文件</span><br><span class="line">（2）tmp文件创建超10秒时会滚动生成正式文件</span><br><span class="line">举例：在2018-01-01 05:23的时侯sink接收到数据，那会产生如下tmp文件：</span><br><span class="line">201801010520.tmp</span><br><span class="line">即使文件内容没有达到128M，也会在05:33时滚动生成正式文件</span><br></pre></td></tr></table></figure>

<p>19：数仓概念总结</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1）数据仓库的输入数据源和输出系统分别是什么？</span><br><span class="line">输入系统：埋点产生的用户行为数据、JavaEE后台产生的业务数据。</span><br><span class="line">输出系统：报表系统、用户画像系统、推荐系统</span><br></pre></td></tr></table></figure>

<p>20：Linux&amp;Shell相关总结</p>
<p>1）Linux常用命令</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>命令</th>
<th>命令解释</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>top</td>
<td>查看内存</td>
</tr>
<tr>
<td>2</td>
<td>df -h</td>
<td>查看磁盘存储情况</td>
</tr>
<tr>
<td>3</td>
<td>iotop</td>
<td>查看磁盘IO读写(yum install iotop安装）</td>
</tr>
<tr>
<td>4</td>
<td>iotop -o</td>
<td>直接查看比较高的磁盘读写程序</td>
</tr>
<tr>
<td>5</td>
<td>netstat -tunlp | grep 端口号</td>
<td>查看端口占用情况</td>
</tr>
<tr>
<td>6</td>
<td>uptime</td>
<td>查看报告系统运行时长及平均负载</td>
</tr>
<tr>
<td>7</td>
<td>ps  aux</td>
<td>查看进程</td>
</tr>
</tbody></table>
<p>2）Shell常用工具</p>
<p>awk、sed、cut、sort</p>
<p>21：Hadoop相关总结</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1）Hadoop默认不支持LZO压缩，如果需要支持LZO压缩，需要添加jar包，并在hadoop的cores-site.xml文件中添加相关压缩配置。</span><br><span class="line">2）Hadoop常用端口号，50070，50090，8020，9000等</span><br><span class="line">3）Hadoop配置文件以及简单的Hadoop集群搭建</span><br><span class="line">4）HDFS读流程和写流程</span><br><span class="line">5）MapReduce的Shuffle过程及Hadoop优化（包括：压缩、小文件、集群优化）</span><br><span class="line">6）Yarn的Job提交流程</span><br><span class="line">7）Yarn的默认调度器、调度器分类、以及他们之间的区别</span><br><span class="line">8）HDFS存储多目录</span><br><span class="line">9）Hadoop参数调优</span><br><span class="line">10）项目经验之基准测试</span><br></pre></td></tr></table></figure>



<p>22： Zookeeper相关总结</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）选举机制</span><br><span class="line">	半数机制</span><br><span class="line">2）常用命令</span><br><span class="line">	ls、get、create</span><br></pre></td></tr></table></figure>



<p>23：Flume相关总结</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">1）Flume组成，Put事务，Take事务</span><br><span class="line">	Taildir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。</span><br><span class="line">	File Channel：数据存储在磁盘，宕机数据可以保存。但是传输速率慢。适合对数据传输可靠性要求高的场景，比如，金融行业。</span><br><span class="line">	Memory Channel：数据存储在内存中，宕机数据丢失。传输速率快。适合对数据传输可靠性要求不高的场景，比如，普通的日志数据。</span><br><span class="line">	Kafka Channel：减少了Flume的Sink阶段，提高了传输效率。           </span><br><span class="line">	Source到Channel是Put事务</span><br><span class="line">	Channel到Sink是Take事务</span><br><span class="line">	</span><br><span class="line">2）Flume拦截器</span><br><span class="line">	（1）拦截器注意事项</span><br><span class="line">		项目中自定义了：ETL拦截器和区分类型拦截器。</span><br><span class="line">采用两个拦截器的优缺点：优点，模块化开发和可移植性；缺点，性能会低一些</span><br><span class="line">	（2）自定义拦截器步骤</span><br><span class="line">a）实现 Interceptor</span><br><span class="line">b）重写四个方法</span><br><span class="line">initialize 初始化</span><br><span class="line">public Event intercept(Event event) 处理单个Event</span><br><span class="line">public List&lt;Event&gt; intercept(List&lt;Event&gt; events) 处理多个Event，在这个方法中调用Event intercept(Event event)</span><br><span class="line">close 方法</span><br><span class="line">c）静态内部类，实现Interceptor.Builder</span><br><span class="line"></span><br><span class="line">3）Flume Channel选择器</span><br><span class="line"></span><br><span class="line">4）Flume 监控器</span><br><span class="line">Ganglia</span><br><span class="line"></span><br><span class="line">5）Flume采集数据会丢失吗?</span><br><span class="line">不会，Channel存储可以存储在File中，数据传输自身有事务。</span><br><span class="line"></span><br><span class="line">6）Flume内存</span><br><span class="line">开发中在flume-env.sh中设置JVM heap为4G或更高，部署在单独的服务器上（4核8线程16G内存）</span><br><span class="line">-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。</span><br><span class="line"></span><br><span class="line">7）FileChannel优化</span><br><span class="line">通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。</span><br><span class="line">官方说明如下：</span><br><span class="line">Comma separated list of directories for storing log files. Using multiple directories on separate disks can improve file channel peformance</span><br><span class="line">checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据</span><br><span class="line"></span><br><span class="line">8）Sink：HDFS Sink小文件处理</span><br><span class="line">（1）HDFS存入大量小文件，有什么影响？</span><br><span class="line">元数据层面：每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命</span><br><span class="line">计算层面：默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。</span><br><span class="line">（2）HDFS小文件处理</span><br><span class="line">官方默认的这三个参数配置写入HDFS后会产生小文件，hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount</span><br><span class="line">基于以上hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0，hdfs.roundValue=10，hdfs.roundUnit= second几个参数综合作用，效果如下：</span><br><span class="line">（1）tmp文件在达到128M时会滚动生成正式文件</span><br><span class="line">（2）tmp文件创建超10秒时会滚动生成正式文件</span><br><span class="line">举例：在2018-01-01 05:23的时侯sink接收到数据，那会产生如下tmp文件：</span><br><span class="line">201801010520.tmp</span><br><span class="line">即使文件内容没有达到128M，也会在05:33时滚动生成正式文件</span><br></pre></td></tr></table></figure>



<p>24：Kafka相关总结</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">1）Kafka压测</span><br><span class="line">Kafka官方自带压力测试脚本（kafka-consumer-perf-test.sh、kafka-producer-perf-test.sh）。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。</span><br><span class="line"></span><br><span class="line">2）Kafka的机器数量</span><br><span class="line">Kafka机器数量=2*（峰值生产速度*副本数/100）+1</span><br><span class="line"></span><br><span class="line">3）Kafka的日志保存时间</span><br><span class="line">7天</span><br><span class="line"></span><br><span class="line">4）Kafka的硬盘大小</span><br><span class="line">每天的数据量*7天</span><br><span class="line"></span><br><span class="line">5）Kafka监控</span><br><span class="line">公司自己开发的监控器；</span><br><span class="line">开源的监控器：KafkaManager、KafkaMonitor</span><br><span class="line"></span><br><span class="line">6）Kakfa分区数。</span><br><span class="line">分区数并不是越多越好，一般分区数不要超过集群机器数量。分区数越多占用内存越大（ISR等），一个节点集中的分区也就越多，当它宕机的时候，对系统的影响也就越大。</span><br><span class="line">分区数一般设置为：3-10个</span><br><span class="line"></span><br><span class="line">7）副本数设定</span><br><span class="line">一般我们设置成2个或3个，很多企业设置为2个。</span><br><span class="line"></span><br><span class="line">8）多少个Topic</span><br><span class="line">  	通常情况：多少个日志类型就多少个Topic。也有对日志类型进行合并的。</span><br><span class="line"></span><br><span class="line">9）Kafka丢不丢数据</span><br><span class="line">Ack=0，相当于异步发送，消息发送完毕即offset增加，继续生产。</span><br><span class="line">Ack=1，leader收到leader replica 对一个消息的接受ack才增加offset，然后继续生产。</span><br><span class="line">Ack=-1，leader收到所有replica 对一个消息的接受ack才增加offset，然后继续生产。</span><br><span class="line"></span><br><span class="line">10）Kafka的ISR副本同步队列</span><br><span class="line">ISR（In-Sync Replicas），副本同步队列。ISR中包括Leader和Follower。如果Leader进程挂掉，会在ISR队列中选择一个服务作为新的Leader。有replica.lag.max.messages（延迟条数）和replica.lag.time.max.ms（延迟时间）两个参数决定一台服务是否可以加入ISR副本队列，在0.10版本移除了replica.lag.max.messages参数，防止服务频繁的进去队列。</span><br><span class="line">任意一个维度超过阈值都会把Follower剔除出ISR，存入OSR（Outof-Sync Replicas）列表，新加入的Follower也会先存放在OSR中。</span><br><span class="line"></span><br><span class="line">11）Kafka分区分配策略</span><br><span class="line">在 Kafka内部存在两种默认的分区分配策略：Range和 RoundRobin。</span><br><span class="line">Range是默认策略。Range是对每个Topic而言的（即一个Topic一个Topic分），首先对同一个Topic里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。</span><br><span class="line">例如：我们有10个分区，两个消费者（C1，C2），3个消费者线程，10 / 3 = 3而且除不尽。</span><br><span class="line">C1-0 将消费 0, 1, 2, 3 分区</span><br><span class="line">C2-0 将消费 4, 5, 6 分区</span><br><span class="line">C2-1 将消费 7, 8, 9 分区</span><br><span class="line">RoundRobin：前提：同一个Consumer Group里面的所有消费者的num.streams（消费者消费线程数）必须相等；每个消费者订阅的主题必须相同。</span><br><span class="line">第一步：将所有主题分区组成TopicAndPartition列表，然后对TopicAndPartition列表按照hashCode进行排序，最后按照轮询的方式发给每一个消费线程。</span><br><span class="line"></span><br><span class="line">12）Kafka中数据量计算</span><br><span class="line">每天总数据量100g，每天产生1亿条日志， 10000万/24/60/60=1150条/每秒钟</span><br><span class="line">平均每秒钟：1150条</span><br><span class="line">低谷每秒钟：400条</span><br><span class="line">高峰每秒钟：1150条*（2-20倍）=2300条-23000条</span><br><span class="line">每条日志大小：0.5k-2k</span><br><span class="line">每秒多少数据量：2.3M-20MB</span><br><span class="line"></span><br><span class="line">13） Kafka挂掉</span><br><span class="line">（1）Flume记录</span><br><span class="line">（2）日志有记录</span><br><span class="line">（3）短期没事</span><br><span class="line"></span><br><span class="line">14） Kafka消息数据积压，Kafka消费能力不足怎么处理？ </span><br><span class="line">（1）如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）</span><br><span class="line">（2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间&lt;生产速度），使处理的数据小于生产的数据，也会造成数据积压。</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

	<div>
  
    ﻿<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

    

		

    

    


	
    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/10/java%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%EF%BC%8810%EF%BC%8910%E7%A7%8D%E7%AE%97%E6%B3%95/" rel="next" title="java数据结构和算法（10）10种算法">
                <i class="fa fa-chevron-left"></i> java数据结构和算法（10）10种算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/06/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93-%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%93%E5%88%86%E5%B1%82%E6%90%AD%E5%BB%BA%E6%80%BB%E7%BB%93/" rel="prev" title="离线数仓-用户行为数仓分层搭建总结">
                离线数仓-用户行为数仓分层搭建总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">鄢玉兵</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1：离线数仓-数据采集"><span class="nav-number">1.</span> <span class="nav-text">1：离线数仓-数据采集</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">鄢玉兵</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
