<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="鄢玉兵的博客" type="application/atom+xml" />






<meta name="description" content="深度学习和神经网络一：深度学习概论1：吴恩达简介 123吴恩达（1976-，英文名：Andrew Ng），华裔美国人，是斯坦福大学计算机科学系和电子工程系副教授，人工智能实验室主任。吴恩达是人工智能和机器学习领域国际上最权威的学者之一。吴恩达也是在线教育平台Coursera的联合创始人（with Daphne Koller）。网站：https:&#x2F;&#x2F;www.deeplearning.ai邮箱：fee">
<meta property="og:type" content="article">
<meta property="og:title" content="AI学习（吴恩达）01—深度学习和神经网络">
<meta property="og:url" content="https:&#x2F;&#x2F;yanyubing.xyz&#x2F;2019&#x2F;12&#x2F;24&#x2F;AI%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%8901%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;index.html">
<meta property="og:site_name" content="鄢玉兵的博客">
<meta property="og:description" content="深度学习和神经网络一：深度学习概论1：吴恩达简介 123吴恩达（1976-，英文名：Andrew Ng），华裔美国人，是斯坦福大学计算机科学系和电子工程系副教授，人工智能实验室主任。吴恩达是人工智能和机器学习领域国际上最权威的学者之一。吴恩达也是在线教育平台Coursera的联合创始人（with Daphne Koller）。网站：https:&#x2F;&#x2F;www.deeplearning.ai邮箱：fee">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="c:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191224174920564.png">
<meta property="og:updated_time" content="2019-12-24T10:39:40.476Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191224174920564.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://yanyubing.xyz/2019/12/24/AI学习（吴恩达）01—深度学习和神经网络/"/>





  <title>AI学习（吴恩达）01—深度学习和神经网络 | 鄢玉兵的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
<a href="https://yanyubing.xyz"><img width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_red_aa0000.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">鄢玉兵的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yanyubing.xyz/2019/12/24/AI%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%8901%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="鄢玉兵">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="鄢玉兵的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">AI学习（吴恩达）01—深度学习和神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-24T09:25:43+08:00">
                2019-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="深度学习和神经网络"><a href="#深度学习和神经网络" class="headerlink" title="深度学习和神经网络"></a>深度学习和神经网络</h1><h3 id="一：深度学习概论"><a href="#一：深度学习概论" class="headerlink" title="一：深度学习概论"></a>一：深度学习概论</h3><p>1：吴恩达简介</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">吴恩达（1976-，英文名：Andrew Ng），华裔美国人，是斯坦福大学计算机科学系和电子工程系副教授，人工智能实验室主任。吴恩达是人工智能和机器学习领域国际上最权威的学者之一。吴恩达也是在线教育平台Coursera的联合创始人（with Daphne Koller）。</span><br><span class="line">网站：https://www.deeplearning.ai</span><br><span class="line">邮箱：feedback@deeplearning.ai</span><br></pre></td></tr></table></figure>

<p>2：神经网络</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">神经网路简单看来就是一个函数，通过给定的输入得到&apos;预测&apos;的输出。</span><br><span class="line">例如：拿到了房屋面积和房价的数据集，可以得到房屋面积和放假的拟合曲线，即为一个神经元。</span><br><span class="line">最简单的神经元：Relu，修正线性单元</span><br><span class="line"></span><br><span class="line">多个神经元组成了神经网络。</span><br><span class="line">例如：影响放假的因数可能还有地址，附件学校数量等等...</span><br><span class="line">所有这些因数都会影响最终的房屋价格，即输入x有多个值</span><br><span class="line"></span><br><span class="line">神经网络的工作的工作就是当你完成对应输入时，会给你一个输出结果。中间可能会有隐藏层，即你的输入可能会经过多层才得到最终的输出。只要你给足够多的输入和输出的数据集，神经网络就能足够准确。</span><br></pre></td></tr></table></figure>

<p>3：深度学习</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">深度学习指的是训练神经网络，有时候规模很大</span><br></pre></td></tr></table></figure>

<p>4：机器学习应用-监督学习</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">目前为止，基本上所有由神经网络创造的经济价值都基于机器学习中的一种，监督学习。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">房价预测：输出价格   			标准神经网络</span><br><span class="line">在线广告：输出是否点击广告		 标准神经网络</span><br><span class="line">图片识别：输出对象 				图像领域CNN（卷积神经网络）</span><br><span class="line">音频识别：输出文本  				序列领域RNN（循环神经网络）</span><br><span class="line">在线翻译：输出语言				序列领域RNNs（更复杂的循环神经网络）</span><br><span class="line">自动驾驶：其他车辆的位置		  混合神经网络（最复杂）</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据源分类</span><br><span class="line">结构化数据：数据库</span><br><span class="line">非结构化数据：音频，视频，图片，文本</span><br></pre></td></tr></table></figure>

<p>5：一个问题，为什么深度学习和人工智能在近几年才兴起？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1：数据量爆炸性增长，传统的分析工具已经无法满足需求</span><br><span class="line">2：需要更大的神经网络来得到更加优秀的模型</span><br><span class="line">3：数据量大和神经网络的性能高是相辅相成</span><br><span class="line">4：神经网络的大不仅仅指的是输入的参数和输入的数据大量（带有标签），还有一点就是中间隐藏层的连接数多</span><br><span class="line">5：想要得到一个优秀的神经网络，在一定程度构建一个大的神经网络和喂食多的输入数据是很好的，同时耗时长</span><br><span class="line">6：在数据集较小的时候，算法的优劣往往取决于手工组件的设计，和细节处理</span><br><span class="line">7：在数据集很大的时候，神经网络的优势往往高于其他方法</span><br><span class="line">8：算法的创新，例如从sigmod函数转换为ReLU函数。sigmod函数的梯度下降在趋近0的时候变得很缓慢，但是转换成ReLU（线性修正单元）之后梯度都为1，使得梯度下降运行速度大大加快。使得可以训练更大的训练单元，同时也可以快速的验证你的神经网络优劣，从而修正。</span><br><span class="line">9：硬件的提升</span><br></pre></td></tr></table></figure>

<h3 id="二：神经网络基础"><a href="#二：神经网络基础" class="headerlink" title="二：神经网络基础"></a>二：神经网络基础</h3><p>1：二分分类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1：逻辑回归是一个用于二分分类的算法</span><br><span class="line">2：例如：</span><br><span class="line">判断一个图片结果是不是猫？1是，0不是</span><br><span class="line">输入图片是64*64像素，就有三个64*64的矩阵，对应红绿蓝3原色的亮度</span><br><span class="line">特征向量x为对应提取的所有像素的亮度值，该向量的维度是64*64*3</span><br><span class="line">3：相关参数</span><br><span class="line">（x,y）：表示单独样本</span><br><span class="line">x:为n维的特征向量，</span><br><span class="line">y:输出，1或者0</span><br><span class="line">m:训练集的样本数</span><br><span class="line">（x（1），y(1)）表示样本1的输入和输出</span><br><span class="line">（x（m），y(m)）表示样本m的输入和输出</span><br><span class="line">m_train表示训练集样本数</span><br><span class="line">x_test表示测试集样本数</span><br><span class="line">矩阵X=[x1,x2...xm]组成，即X是一个nx*m的矩阵,X.shape=(n*m)</span><br><span class="line">矩阵Y=[y1,y2...ym]组成，即Y使一个1*m的矩阵，Y.shape=(1,m)</span><br><span class="line">4:逻辑回归</span><br><span class="line">输入x（一张图片），输出Y^=P(1|x)（该图片是猫的概率），w是输入x集合中的一个</span><br><span class="line">y^=wTx+b，w的转置乘以x，加上一个常数b（这里采用正则化）</span><br><span class="line">因为y^是一个0-1之间的概率，所有加上sigmod（逻辑回归），y^=σ（wTx+b），</span><br><span class="line">其中σ（z）=1/(1+e^(-z))，主要就是训练参数w和b</span><br></pre></td></tr></table></figure>

<p>2：逻辑回归损失函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：一般可以将损失函数定义为：L(y^,y)=1/2(y^-y)^2，但是这样会使得损失函数式非凸的，使用梯度下降法之后会得到多个局部最优解，不是全局最优解</span><br><span class="line">2：逻辑回归中一般定义损失函数为：L（y^，y）=-(log(y^)+(1-y)log(1-y^))，同样损失函数越小越好</span><br><span class="line">3：成本函数是基于整个数据集的损失函数，J(w,b)，找到合适的参数w，b使得J（w,b）尽可能的小</span><br></pre></td></tr></table></figure>

<p>3：梯度下降</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：对于成本函数J（w,b），要求的合适的w,b使得成本函数J（w,b）最小。</span><br><span class="line">2：此时的成本函数式凸函数，只有一个全局最优解，同时也是局部最优解。</span><br><span class="line">3：对于任意初始w,b都能得到最优解</span><br><span class="line">4：梯度下降的思想是从初始点开始，朝着最陡的下坡方向走，走了一次之后会停留在当前的点，此时的w,b都会改变，此时就完成了一次下降的迭代。第二次迭代...，一直到接近全局最优解。</span><br><span class="line">对于w而言，一次更新记作w:=w-∂(dJ(w)/dw),a表示学习率（代表步长）。 对于w,b两个参数而言是一样的逻辑，即求偏导数。代码通常直接用dw和db来表示</span><br></pre></td></tr></table></figure>

<p>4：导数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">导数是函数的局部性质。一个函数在某一点的导数描述了这个函数在这一点附近的变化率。如果函数的自变量和取值都是实数的话，函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率。导数的本质是通过极限的概念对函数进行局部的线性逼近。</span><br></pre></td></tr></table></figure>

<p>5：计算图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">例如J(a,b,c)=3(a+bc)</span><br><span class="line">图的步骤为:</span><br><span class="line">1:U=bc       输入为bc</span><br><span class="line">2:V=a+u		 输入为a和u</span><br><span class="line">3:J=3V 		 输入为v</span><br><span class="line">从左到右(1.2.3)的过程就是流程图的正向计算过程，用于计算最终的输出J</span><br><span class="line">dJ/da=(dJ/dV)*(dv/da)=3*1=3，根据了导数的链式法则，这里则是用到了反向传播求导数。在程序中使用dvar来表示dJ对某个中间变量来求导，例如da=3</span><br></pre></td></tr></table></figure>

<p>6：逻辑回归中的梯度下降</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1：已知数据</span><br><span class="line">偏差为：z=wTx+b</span><br><span class="line">概率为：y^=a=σ（z）=1/(1+e^(-z))</span><br><span class="line">单个样本损失函数为：L(a,y)=-(ylog(a)+(1-y)log(1-a))</span><br><span class="line">假设样本两个特征x1，x2。</span><br><span class="line">使用w1.w2和b：</span><br><span class="line">2：向前传播计算z</span><br><span class="line">输入为w1,x1,w2,x2,b；我们需要计算z=w1*x1+w2*x2+b→y^=a=σ（z）→L(a,y)</span><br><span class="line">w=[w1,w2],x=[x1,x2]所以z=wTx+b=w1*x1+w2*x2+b</span><br><span class="line">3：向后传播计算偏导数</span><br><span class="line">da=dL(a,y)/da=-y/a+(1-y)/(1-a)</span><br><span class="line">dz=dL(a,y)/dz=(dL(a,y)/da)*(da/dz)=a(1-a)*[-y/a+(1-y)/(1-a)]=a-y</span><br><span class="line">dw1</span><br><span class="line">dw1=x1*dz</span><br><span class="line">dw2=x2*dz</span><br><span class="line">db=dz</span><br></pre></td></tr></table></figure>

<p>7：m个样本的梯度下降</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">成本函数J(w,b)=(1/m)*∑L(a(i),y)  a(i)=y^(i)=σ（z（i））=σ(wTx(i)+b)</span><br><span class="line">对w1求导，d(J(w,b))/dw1=(1/m)*∑[L(a(i),y)/dw1]</span><br><span class="line">用代码操作如下：</span><br><span class="line">J=0,dw1=0,dw2=0,db=0  #作为累加器</span><br><span class="line">for i =1 to m</span><br><span class="line"> z(i)=wTx(i)+b</span><br><span class="line"> a(i)=σ(z(i))</span><br><span class="line"> J+=-[y(i)loga(i)+(1-a)log(1-a(i))]</span><br><span class="line"> dz(i)=a(i)-y(i)</span><br><span class="line"> dw1+=x1(i)dz(i)</span><br><span class="line"> dw2+=x2(i)dz(i)  #这里假设只有两个特征，如果有n个特征，则需要遍历</span><br><span class="line"> db+=dz(i)</span><br><span class="line">#循环结束之后除以样本数m</span><br><span class="line">J/=m	 #成本函数</span><br><span class="line">dw1/=m	 #损失函数对w1求导，注意这里是求导之后的累加</span><br><span class="line">dw2/=m 	 #损失函数对w2求导，同上</span><br><span class="line">#m个样本遍历完成之后需要更新w1和w2</span><br><span class="line">w1:=w1-∂dw1   #∂为学习率</span><br><span class="line">w2:=w2-∂dw2</span><br><span class="line">b:=b-∂db</span><br><span class="line">#到这里完成之后只进行了一次梯度下降，上述过程需要重复完成</span><br></pre></td></tr></table></figure>

<p>8：向量化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">前面提到了如果需要完成梯度下降代码，则需要2个for循环，外层m次为遍历所有样本数，内层n为遍历所有特征量，使得代码执行效率大大降低。所以引入向量化，大大提高代码的运行效率！</span><br><span class="line">注意：尽量不要在代码中显示的使用for循环，numpy库中有很多的内置函数</span><br><span class="line"></span><br><span class="line">需求：计算z=wTx+b  w和x为1位列向量</span><br><span class="line">使用for循环代码如下：</span><br><span class="line">z=0</span><br><span class="line">for i in rang(n_x) # 效率低</span><br><span class="line">	z+=w[i]*x[i]</span><br><span class="line">z+=b</span><br><span class="line"></span><br><span class="line">使用python中的numpy</span><br><span class="line">z=np.dot(w,x)+b  #非常快</span><br><span class="line"></span><br><span class="line">所以前面代码中显示把dw1,dw2置为0可以改为dw=np.Zeros((n_x,1)),</span><br><span class="line">继而求dw1，dw2...的过程就是求dw的过程dw+=x(i)dz(i)</span><br><span class="line">最后dw1/=m,dw2/=m...改为dw/=m</span><br><span class="line">最终，1次梯度下降的整个过程就是对整个训练集遍历了1次，而不需要再遍历特征值</span><br></pre></td></tr></table></figure>

<p>9：向量化logistic回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">前面提到，使用向量化可以去除内层对特征值的循环遍历，但是还是会保留对样本数的遍历；这里使用logistic回归同时可以出去对样本数的遍历。</span><br><span class="line">继而训练一次样本都不会使用到for循环！（多么牛皮啊！！）</span><br><span class="line"></span><br><span class="line">m个样本的训练集：</span><br><span class="line">z(1)=wTx(1)+b 	a(1)=σ（z(1)）</span><br><span class="line">z(2)=wTx(2)+b 	a(2)=σ（z(2)）</span><br><span class="line">z(3)=wTx(3)+b 	a(3)=σ（z(3)）</span><br><span class="line">...</span><br><span class="line">z(m)=wTx(m)+b 	a(m)=σ（z(m)）</span><br><span class="line">然后求和得到z</span><br><span class="line"></span><br><span class="line">优化：</span><br><span class="line">X=[x(1),x(2),x(3)...x(m)]矩阵，为(n_x,m),n为每个样本的特征数据量</span><br><span class="line">1:构建Z矩阵：Z=[z(1),z(2),z(3)...z(m)]</span><br><span class="line">2:Z=wTX+[b,b,b...b]=[wTX(1),wTX(2),wTX(3)...wTX(m)]+[b,b,b...b]=</span><br><span class="line">[z(1),z(2),z(3)...z(m)]</span><br><span class="line">3：代码表示Z=np.dot(w.T,x)+b,python中向量加上常数b，会自动把b转换成（1，m）的向量，这个在python中叫做广播</span><br><span class="line">4：A=σ(Z)=[σ(a(1)),σ(a(2))...σ(a(m))]=[σ(z(1)),σ(z(2)),...σ(z(m))]</span><br><span class="line">5：不需要for循环直接可以得到一次样本数m的所有偏差，或者是损失率</span><br><span class="line">总结：即同一时间可以完成全部的z(i)计算</span><br></pre></td></tr></table></figure>

<p>10：向量化逻辑回归的梯度输出（一次迭代最终代码实现）</p>
<p><img src="C:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191224174920564.png" alt="image-20191224174920564"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：每进行一次梯度下降都是为了调整w和b</span><br></pre></td></tr></table></figure>

<p>11：广播（Broadcasting）</p>

      
    </div>
    
    
    

	<div>
  
    ﻿<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

    

		

    

    


	
    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/23/AI%E5%AD%A6%E4%B9%A002%E2%80%94%E5%AE%9E%E7%8E%B0/" rel="next" title="AI学习02—实现">
                <i class="fa fa-chevron-left"></i> AI学习02—实现
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">鄢玉兵</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习和神经网络"><span class="nav-number">1.</span> <span class="nav-text">深度学习和神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一：深度学习概论"><span class="nav-number">1.0.1.</span> <span class="nav-text">一：深度学习概论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二：神经网络基础"><span class="nav-number">1.0.2.</span> <span class="nav-text">二：神经网络基础</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">鄢玉兵</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
