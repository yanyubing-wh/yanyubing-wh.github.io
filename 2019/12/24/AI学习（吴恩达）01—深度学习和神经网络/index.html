<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="鄢玉兵的博客" type="application/atom+xml" />






<meta name="description" content="深度学习和神经网络一：深度学习概论1：吴恩达简介 123吴恩达（1976-，英文名：Andrew Ng），华裔美国人，是斯坦福大学计算机科学系和电子工程系副教授，人工智能实验室主任。吴恩达是人工智能和机器学习领域国际上最权威的学者之一。吴恩达也是在线教育平台Coursera的联合创始人（with Daphne Koller）。网站：https:&#x2F;&#x2F;www.deeplearning.ai邮箱：fee">
<meta property="og:type" content="article">
<meta property="og:title" content="AI学习（吴恩达）01—深度学习和神经网络">
<meta property="og:url" content="https:&#x2F;&#x2F;yanyubing.xyz&#x2F;2019&#x2F;12&#x2F;24&#x2F;AI%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%8901%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;index.html">
<meta property="og:site_name" content="鄢玉兵的博客">
<meta property="og:description" content="深度学习和神经网络一：深度学习概论1：吴恩达简介 123吴恩达（1976-，英文名：Andrew Ng），华裔美国人，是斯坦福大学计算机科学系和电子工程系副教授，人工智能实验室主任。吴恩达是人工智能和机器学习领域国际上最权威的学者之一。吴恩达也是在线教育平台Coursera的联合创始人（with Daphne Koller）。网站：https:&#x2F;&#x2F;www.deeplearning.ai邮箱：fee">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="c:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191224174920564.png">
<meta property="og:image" content="c:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191225141747807.png">
<meta property="og:image" content="c:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191225150320753.png">
<meta property="og:updated_time" content="2019-12-25T07:22:26.379Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191224174920564.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://yanyubing.xyz/2019/12/24/AI学习（吴恩达）01—深度学习和神经网络/"/>





  <title>AI学习（吴恩达）01—深度学习和神经网络 | 鄢玉兵的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
<a href="https://yanyubing.xyz"><img width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_red_aa0000.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">鄢玉兵的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yanyubing.xyz/2019/12/24/AI%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%8901%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="鄢玉兵">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="鄢玉兵的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">AI学习（吴恩达）01—深度学习和神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-24T09:25:43+08:00">
                2019-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="深度学习和神经网络"><a href="#深度学习和神经网络" class="headerlink" title="深度学习和神经网络"></a>深度学习和神经网络</h1><h3 id="一：深度学习概论"><a href="#一：深度学习概论" class="headerlink" title="一：深度学习概论"></a>一：深度学习概论</h3><p>1：吴恩达简介</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">吴恩达（1976-，英文名：Andrew Ng），华裔美国人，是斯坦福大学计算机科学系和电子工程系副教授，人工智能实验室主任。吴恩达是人工智能和机器学习领域国际上最权威的学者之一。吴恩达也是在线教育平台Coursera的联合创始人（with Daphne Koller）。</span><br><span class="line">网站：https://www.deeplearning.ai</span><br><span class="line">邮箱：feedback@deeplearning.ai</span><br></pre></td></tr></table></figure>

<p>2：神经网络</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">神经网路简单看来就是一个函数，通过给定的输入得到&apos;预测&apos;的输出。</span><br><span class="line">例如：拿到了房屋面积和房价的数据集，可以得到房屋面积和放假的拟合曲线，即为一个神经元。</span><br><span class="line">最简单的神经元：Relu，修正线性单元</span><br><span class="line"></span><br><span class="line">多个神经元组成了神经网络。</span><br><span class="line">例如：影响放假的因数可能还有地址，附件学校数量等等...</span><br><span class="line">所有这些因数都会影响最终的房屋价格，即输入x有多个值</span><br><span class="line"></span><br><span class="line">神经网络的工作的工作就是当你完成对应输入时，会给你一个输出结果。中间可能会有隐藏层，即你的输入可能会经过多层才得到最终的输出。只要你给足够多的输入和输出的数据集，神经网络就能足够准确。</span><br></pre></td></tr></table></figure>

<p>3：深度学习</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">深度学习指的是训练神经网络，有时候规模很大</span><br></pre></td></tr></table></figure>

<p>4：机器学习应用-监督学习</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">目前为止，基本上所有由神经网络创造的经济价值都基于机器学习中的一种，监督学习。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">房价预测：输出价格   			标准神经网络</span><br><span class="line">在线广告：输出是否点击广告		 标准神经网络</span><br><span class="line">图片识别：输出对象 				图像领域CNN（卷积神经网络）</span><br><span class="line">音频识别：输出文本  				序列领域RNN（循环神经网络）</span><br><span class="line">在线翻译：输出语言				序列领域RNNs（更复杂的循环神经网络）</span><br><span class="line">自动驾驶：其他车辆的位置		  混合神经网络（最复杂）</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据源分类</span><br><span class="line">结构化数据：数据库</span><br><span class="line">非结构化数据：音频，视频，图片，文本</span><br></pre></td></tr></table></figure>

<p>5：一个问题，为什么深度学习和人工智能在近几年才兴起？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1：数据量爆炸性增长，传统的分析工具已经无法满足需求</span><br><span class="line">2：需要更大的神经网络来得到更加优秀的模型</span><br><span class="line">3：数据量大和神经网络的性能高是相辅相成</span><br><span class="line">4：神经网络的大不仅仅指的是输入的参数和输入的数据大量（带有标签），还有一点就是中间隐藏层的连接数多</span><br><span class="line">5：想要得到一个优秀的神经网络，在一定程度构建一个大的神经网络和喂食多的输入数据是很好的，同时耗时长</span><br><span class="line">6：在数据集较小的时候，算法的优劣往往取决于手工组件的设计，和细节处理</span><br><span class="line">7：在数据集很大的时候，神经网络的优势往往高于其他方法</span><br><span class="line">8：算法的创新，例如从sigmod函数转换为ReLU函数。sigmod函数的梯度下降在趋近0的时候变得很缓慢，但是转换成ReLU（线性修正单元）之后梯度都为1，使得梯度下降运行速度大大加快。使得可以训练更大的训练单元，同时也可以快速的验证你的神经网络优劣，从而修正。</span><br><span class="line">9：硬件的提升</span><br></pre></td></tr></table></figure>

<h3 id="二：神经网络基础"><a href="#二：神经网络基础" class="headerlink" title="二：神经网络基础"></a>二：神经网络基础</h3><p>1：二分分类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1：逻辑回归是一个用于二分分类的算法</span><br><span class="line">2：例如：</span><br><span class="line">判断一个图片结果是不是猫？1是，0不是</span><br><span class="line">输入图片是64*64像素，就有三个64*64的矩阵，对应红绿蓝3原色的亮度</span><br><span class="line">特征向量x为对应提取的所有像素的亮度值，该向量的维度是64*64*3</span><br><span class="line">3：相关参数</span><br><span class="line">（x,y）：表示单独样本</span><br><span class="line">x:为n维的特征向量，</span><br><span class="line">y:输出，1或者0</span><br><span class="line">m:训练集的样本数</span><br><span class="line">（x（1），y(1)）表示样本1的输入和输出</span><br><span class="line">（x（m），y(m)）表示样本m的输入和输出</span><br><span class="line">m_train表示训练集样本数</span><br><span class="line">x_test表示测试集样本数</span><br><span class="line">矩阵X=[x(1),x(2)...x(m)]组成，即X是一个nx*m的矩阵,X.shape=(n*m)</span><br><span class="line">矩阵Y=[y(1),y(2)...y(m)]组成，即Y使一个1*m的矩阵，Y.shape=(1,m)</span><br><span class="line">4:逻辑回归</span><br><span class="line">输入x（一张图片），输出Y^=P(1|x)（该图片是猫的概率），w是输入x集合中的一个</span><br><span class="line">y^=wTx+b，w的转置乘以x，加上一个常数b（这里采用正则化）</span><br><span class="line">因为y^是一个0-1之间的概率，所有加上sigmod（逻辑回归），y^=σ（wTx+b），</span><br><span class="line">其中σ（z）=1/(1+e^(-z))，主要就是训练参数w和b</span><br></pre></td></tr></table></figure>

<p>2：逻辑回归损失函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：一般可以将损失函数定义为：L(y^,y)=1/2(y^-y)^2，但是这样会使得损失函数式非凸的，使用梯度下降法之后会得到多个局部最优解，不是全局最优解</span><br><span class="line">2：逻辑回归中一般定义损失函数为：L（y^，y）=-(log(y^)+(1-y)log(1-y^))，同样损失函数越小越好</span><br><span class="line">3：成本函数是基于整个数据集的损失函数，J(w,b)，找到合适的参数w，b使得J（w,b）尽可能的小</span><br></pre></td></tr></table></figure>

<p>3：梯度下降</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：对于成本函数J（w,b），要求的合适的w,b使得成本函数J（w,b）最小。</span><br><span class="line">2：此时的成本函数式凸函数，只有一个全局最优解，同时也是局部最优解。</span><br><span class="line">3：对于任意初始w,b都能得到最优解</span><br><span class="line">4：梯度下降的思想是从初始点开始，朝着最陡的下坡方向走，走了一次之后会停留在当前的点，此时的w,b都会改变，此时就完成了一次下降的迭代。第二次迭代...，一直到接近全局最优解。</span><br><span class="line">对于w而言，一次更新记作w:=w-α(dJ(w)/dw),α表示学习率（代表步长）。 对于w,b两个参数而言是一样的逻辑，即求偏导数。代码通常直接用dw和db来表示</span><br></pre></td></tr></table></figure>

<p>4：导数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">导数是函数的局部性质。一个函数在某一点的导数描述了这个函数在这一点附近的变化率。如果函数的自变量和取值都是实数的话，函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率。导数的本质是通过极限的概念对函数进行局部的线性逼近。</span><br></pre></td></tr></table></figure>

<p>5：计算图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">例如J(a,b,c)=3(a+bc)</span><br><span class="line">图的步骤为:</span><br><span class="line">1:U=bc       输入为bc</span><br><span class="line">2:V=a+u		 输入为a和u</span><br><span class="line">3:J=3V 		 输入为v</span><br><span class="line">从左到右(1.2.3)的过程就是流程图的正向计算过程，用于计算最终的输出J</span><br><span class="line">dJ/da=(dJ/dV)*(dv/da)=3*1=3，根据了导数的链式法则，这里则是用到了反向传播求导数。在程序中使用dvar来表示dJ对某个中间变量来求导，例如da=3</span><br></pre></td></tr></table></figure>

<p>6：逻辑回归中的梯度下降</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1：已知数据</span><br><span class="line">偏差为：z=wTx+b</span><br><span class="line">概率为：y^=a=σ（z）=1/(1+e^(-z))</span><br><span class="line">单个样本损失函数为：L(a,y)=-(ylog(a)+(1-y)log(1-a))</span><br><span class="line">假设样本两个特征x1，x2。</span><br><span class="line">使用w1.w2和b：</span><br><span class="line">2：向前传播计算z</span><br><span class="line">输入为w1,x1,w2,x2,b；我们需要计算z=w1*x1+w2*x2+b→y^=a=σ（z）→L(a,y)</span><br><span class="line">w=[w1,w2],x=[x1,x2]所以z=wTx+b=w1*x1+w2*x2+b</span><br><span class="line">3：向后传播计算偏导数</span><br><span class="line">da=dL(a,y)/da=-y/a+(1-y)/(1-a)</span><br><span class="line">dz=dL(a,y)/dz=(dL(a,y)/da)*(da/dz)=a(1-a)*[-y/a+(1-y)/(1-a)]=a-y</span><br><span class="line">dw1</span><br><span class="line">dw1=x1*dz</span><br><span class="line">dw2=x2*dz</span><br><span class="line">db=dz</span><br></pre></td></tr></table></figure>

<p>7：m个样本的梯度下降</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">成本函数J(w,b)=(1/m)*∑L(a(i),y)  a(i)=y^(i)=σ（z（i））=σ(wTx(i)+b)</span><br><span class="line">对w1求导，d(J(w,b))/dw1=(1/m)*∑[L(a(i),y)/dw1]</span><br><span class="line">用代码操作如下：</span><br><span class="line">J=0,dw1=0,dw2=0,db=0  #作为累加器</span><br><span class="line">for i =1 to m</span><br><span class="line"> z(i)=wTx(i)+b</span><br><span class="line"> a(i)=σ(z(i))</span><br><span class="line"> J+=-[y(i)loga(i)+(1-a)log(1-a(i))]</span><br><span class="line"> dz(i)=a(i)-y(i)</span><br><span class="line"> dw1+=x1(i)dz(i)</span><br><span class="line"> dw2+=x2(i)dz(i)  #这里假设只有两个特征，如果有n个特征，则需要遍历</span><br><span class="line"> db+=dz(i)</span><br><span class="line">#循环结束之后除以样本数m</span><br><span class="line">J/=m	 #成本函数</span><br><span class="line">dw1/=m	 #损失函数对w1求导，注意这里是求导之后的累加</span><br><span class="line">dw2/=m 	 #损失函数对w2求导，同上</span><br><span class="line">#m个样本遍历完成之后需要更新w1和w2</span><br><span class="line">w1:=w1-αdw1   #α为学习率</span><br><span class="line">w2:=w2-αdw2</span><br><span class="line">b:=b-αdb</span><br><span class="line">#到这里完成之后只进行了一次梯度下降，上述过程需要重复完成</span><br></pre></td></tr></table></figure>

<p>8：向量化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">前面提到了如果需要完成梯度下降代码，则需要2个for循环，外层m次为遍历所有样本数，内层n为遍历所有特征量，使得代码执行效率大大降低。所以引入向量化，大大提高代码的运行效率！</span><br><span class="line">注意：尽量不要在代码中显示的使用for循环，numpy库中有很多的内置函数</span><br><span class="line"></span><br><span class="line">需求：计算z=wTx+b  w和x为1位列向量</span><br><span class="line">使用for循环代码如下：</span><br><span class="line">z=0</span><br><span class="line">for i in rang(n_x) # 效率低</span><br><span class="line">	z+=w[i]*x[i]</span><br><span class="line">z+=b</span><br><span class="line"></span><br><span class="line">使用python中的numpy</span><br><span class="line">z=np.dot(w,x)+b  #非常快</span><br><span class="line"></span><br><span class="line">所以前面代码中显示把dw1,dw2置为0可以改为dw=np.Zeros((n_x,1)),</span><br><span class="line">继而求dw1，dw2...的过程就是求dw的过程dw+=x(i)dz(i)</span><br><span class="line">最后dw1/=m,dw2/=m...改为dw/=m</span><br><span class="line">最终，1次梯度下降的整个过程就是对整个训练集遍历了1次，而不需要再遍历特征值</span><br></pre></td></tr></table></figure>

<p>9：向量化logistic回归</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">前面提到，使用向量化可以去除内层对特征值的循环遍历，但是还是会保留对样本数的遍历；这里使用logistic回归同时可以出去对样本数的遍历。</span><br><span class="line">继而训练一次样本都不会使用到for循环！（多么牛皮啊！！）</span><br><span class="line"></span><br><span class="line">m个样本的训练集：</span><br><span class="line">z(1)=wTx(1)+b 	a(1)=σ（z(1)）</span><br><span class="line">z(2)=wTx(2)+b 	a(2)=σ（z(2)）</span><br><span class="line">z(3)=wTx(3)+b 	a(3)=σ（z(3)）</span><br><span class="line">...</span><br><span class="line">z(m)=wTx(m)+b 	a(m)=σ（z(m)）</span><br><span class="line">然后求和得到z</span><br><span class="line"></span><br><span class="line">优化：</span><br><span class="line">X=[x(1),x(2),x(3)...x(m)]矩阵，为(n_x,m),n为每个样本的特征数据量</span><br><span class="line">1:构建Z矩阵：Z=[z(1),z(2),z(3)...z(m)]</span><br><span class="line">2:Z=wTX+[b,b,b...b]=[wTX(1),wTX(2),wTX(3)...wTX(m)]+[b,b,b...b]=</span><br><span class="line">[z(1),z(2),z(3)...z(m)]</span><br><span class="line">3：代码表示Z=np.dot(w.T,x)+b,python中向量加上常数b，会自动把b转换成（1，m）的向量，这个在python中叫做广播</span><br><span class="line">4：A=σ(Z)=[σ(a(1)),σ(a(2))...σ(a(m))]=[σ(z(1)),σ(z(2)),...σ(z(m))]</span><br><span class="line">5：不需要for循环直接可以得到一次样本数m的所有偏差，或者是损失率</span><br><span class="line">总结：即同一时间可以完成全部的z(i)计算</span><br></pre></td></tr></table></figure>

<p>10：向量化逻辑回归的梯度输出（一次迭代最终代码实现）</p>
<p><img src="C:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191224174920564.png" alt="image-20191224174920564"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：每进行一次梯度下降都是为了调整w和b</span><br></pre></td></tr></table></figure>

<p>11：广播（Broadcasting）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果你有一个m*n的矩阵，和另一个（1，n）的矩阵做四则运算，(1,n)会复制成(m,n)；</span><br><span class="line">如果你有一个m*n的矩阵，和另一个（m，1）的矩阵做四则运算，(m,1)会复制成(m,n)；</span><br><span class="line">（m,1）与R进行运算时，R会复制成（m,1）的矩阵形式；列向量同理。</span><br></pre></td></tr></table></figure>

<h3 id="12：python中numpy的注意点"><a href="#12：python中numpy的注意点" class="headerlink" title="12：python中numpy的注意点"></a>12：python中numpy的注意点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：a=np.random.randn(5)得到的数据结构是a.shape=（5，）秩为1的数组，运算a*aT的时候会得到一个常数，而不是想象中的矩阵</span><br><span class="line">2:不要写（n,）这种秩为1的数组</span><br><span class="line">3：如果要表示a为5*1的列向量；a=np.random.randn(5,1)，即为a.shape=（5，1）的矩阵</span><br><span class="line">4：始终使用5*1的矩阵，而不要使用秩为1的数组</span><br></pre></td></tr></table></figure>

<p>13：逻辑回归损失函数的解释</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if y=1:		p(y|x)=y^</span><br><span class="line">if y=0:		p(y|x)=1-y^</span><br><span class="line">二分分类的成本函数，y的值只能是0或者1</span><br><span class="line">合并：p(y|x)=[(y^)^y]*[(1-y^)^(1-y)]。即当y=0或者y=1时都成立</span><br><span class="line">因为log函数严格单调，即logp(y|x)=log[(y^)^y]*[(1-y^)^(1-y)]=ylogy^+(1-y)log(1-y^)</span><br><span class="line"></span><br><span class="line">1:逻辑回归的损失函数即为-logp(y|x)=-[ylogy^+(1-y)log(1-y^)],因为最大化p(y|x)就是最小化损失函数</span><br><span class="line">2:逻辑回归的成本函数即为J(w,b)=1/m*∑L（y^(i),y(i)）</span><br></pre></td></tr></table></figure>

<h3 id="三：浅层神经网络"><a href="#三：浅层神经网络" class="headerlink" title="三：浅层神经网络"></a>三：浅层神经网络</h3><p>1：神经网络概览</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">模型：</span><br><span class="line">x,w,z→z=wT+b→a=σ（z）→L（a,y）</span><br><span class="line">一些规定：</span><br><span class="line">z[1]=w[1]Tx+b[1]     #方括号表示层，此处表示第一层神经网络，神经网络存在多次计算z和a的值，最后一层完成之后计算损失函数，再反向求出损失函数对参数w和b的偏导数，然后进行步长迭代</span><br><span class="line">(1)					 #小括号表示样本个数，次数表示第1个样本</span><br></pre></td></tr></table></figure>

<p>2：神经网络表示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1：输入层→隐藏层1→隐藏层2...隐藏层n→输出层y^</span><br><span class="line">2：数据集是：给定了输入，同时知道输出，但是不知道隐藏层</span><br><span class="line">3：x表示输入特征值[x1,x2...xm]，另一种表示为a[0]=x，a表示‘激活’的意思</span><br><span class="line">4：隐藏层1中则表示为a[1]_1,a[1]_2...，最终输出层则为a[n+1]=y^</span><br><span class="line">5；一般来说不把输入层作为一层来计算，或者叫做第0层，所以最上面的神经网络一起为n+1层</span><br><span class="line">6：隐藏层的相关参数w,b，和隐藏层的上标[n]；w是一个4*3的矩阵，b是一个4*1的矩阵（这里的4是该层的激活单元个数，3是指的输入层的特征值个数）</span><br><span class="line">7：输出层w为1*4的矩阵，b为1*1的矩阵（4是因为上一个隐藏层有4个神经单元，而输出层只有一个神经单元）</span><br></pre></td></tr></table></figure>

<p>3：神经网络的输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1：隐藏层所有神经元都通过2个步骤计算当前神经元的激活值，①z=wTx+b  ②a=σ(z)=y^；这样就可以得到该层的所有神经元的激活值</span><br><span class="line">2：该隐藏层的所有z可以转换为4*3的矩阵Z（大Z），4代表该层有4个神经元w1,w2,w3,w4；3表示和三个输入特征有关x1,x2,x3</span><br><span class="line">3：Z（大Z）转置之后乘以x再加上b，最后取σ，最终得到A，A的每行正好对应该隐藏层的激活值a1,a2,a3,a4即σ（z1[1]）,σ（z2[1]）,σ（z3[1]）,σ（z4[1]）;这里再次强调[]表示是第几层隐藏层，下标表示第几个神经元</span><br><span class="line">4：流程，下面a[1]，代表第一层激活值堆叠的矩阵；再次强调w矩阵是m*n（m代表该层的神经元个数，n代表上一层神经元个数）</span><br><span class="line">z[1]=w[1]x+b[1]   	 （4,1）=（4,3）（3,1）+（4，1）</span><br><span class="line">a[1]=σ（z[1])  	 	（4,1）=（4，1）</span><br><span class="line">z[2]=w[2]a[1]+b[2] 	 (1,1) =(1,4)(4,1)+(1,1)</span><br><span class="line">a[2]=σ（z[2])=y^      #这里只有2层，所有最终输出y^就是a[2] 		</span><br><span class="line"></span><br><span class="line">也就是最终神经网络的输出就是这里的y^,(1*1)的矩阵</span><br><span class="line">总结：一般一层神经元的计算都把值堆叠起来形成矩阵</span><br></pre></td></tr></table></figure>

<p>4：多个例子的向量化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">上面提到的是对于单个训练样本来求输出，接下来是对多个样本向量化。</span><br><span class="line">流程：			  x→y^</span><br><span class="line">第一个样本：	x(1)→a[2]=y^(1)</span><br><span class="line">第二个样本：	x(2)→a[2](2)=y^(2)</span><br><span class="line">...</span><br><span class="line">第m个样本：	x(m)→a[2](m)=y^(m)</span><br><span class="line">再次强调[2]表示第二层，这里也就是输出层；m表示样本数。</span><br><span class="line">这里要得到所有样本的输出激活值，需要遍历样本for i=1 to m，然而前面提到，样本输入可以转换为:</span><br><span class="line">X=[x(1),x(2),x(3)...x(m)]</span><br><span class="line">Z[1]=W[1]X+b[1]   #Z[1]=[z[1](1),z[1](2),...z[1](m)]</span><br><span class="line">A[1]=σ[z[1]]       #A[1]=[a[1](1),a[1](2),...a[1](m)]</span><br><span class="line">Z[2]=w[2]A[1]+b[2]   </span><br><span class="line">A[2]=σ(z[2])；最终训练集的输出</span><br><span class="line">①即横向扫描矩阵的时候，就扫过了整个训练集；同时也印证了前面说到的，完成一次样本集的训练是不需要使用到for循环遍历的</span><br><span class="line">②竖向扫描，各项值就对应了神经网络的不同节点的激活值(这里不好用文本表示）</span><br><span class="line"></span><br><span class="line">总结就是：横向表示样本的个数，竖向表示各个节点的激活值（或者是输入特征值，输出激活值）</span><br></pre></td></tr></table></figure>

<p>5：激活函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Z[1]=W[1]X+b[1]   #Z[1]=[z[1](1),z[1](2),...z[1](m)]</span><br><span class="line">A[1]=σ[z[1]]       #A[1]=[a[1](1),a[1](2),...a[1](m)]</span><br><span class="line">Z[2]=w[2]A[1]+b[2]   </span><br><span class="line">A[2]=σ(z[2])</span><br><span class="line">这里的σ（sigmod）是激活函数，并且2层都使用的是σ作为激活函数。</span><br><span class="line">σ曲线0~1之间，σ（z）=1/(1+e^(-z))</span><br><span class="line">双曲正切函数g,-1~1之间。g(z)=tanh(z)=（e^z-e^(-z)）/(e^z+e^(-z))</span><br><span class="line">分析：这里的双曲正切函数在各个场合都要比σ曲线0优秀很多</span><br><span class="line">①tanh函数的平均值为0，更加好用于计算</span><br><span class="line">②特殊情况，使用二分分类（输出取0或者1）的时候，最终输出层的激活函数使用σ，因为希望得到的最终y^是一个0~1之间的概率。</span><br><span class="line">③其他优点后面再补充！</span><br><span class="line">另一个问题：tanz和σ函数的确定就是z值很大或者很小的时候，这个函数的梯度都很小，这一点就会影响梯度下降的速度！这里就需要上面提到过的线性修正单元来解决，ReLU:a=max(0,z)</span><br><span class="line">这里注意一点，就是当z=0时，这里的函数不可导；此时需要自己赋值导数为0或者1都行；当z&lt;0时，函数导数为0，但是实际不影响，这里也可以用leaky ReLU:a=max(0.01z,z)来处理当z&lt;0的情况！</span><br><span class="line"></span><br><span class="line">总结：仅在二分分类的时候，输出层激活函数可以使用σ；其他情况下，使用ReLU!主要考虑因数是①场景②快速迭代，加快学习速度。</span><br><span class="line">最后一点就是激活函数的选择对不同情况效果可能不一样，所有选择最合适的激活函数需要自己尝试对比！</span><br></pre></td></tr></table></figure>

<p>6：为什么需要非线性激活函数？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面提到的A[1]=σ[z[1]] ，为什么不是直接使A[1]=z[1]呢？这里直接赋值其实就是一种线性激活函数（也叫恒等激活函数）。why not?</span><br><span class="line">如果是线性激活函数：那么得到的a[1]=z[1]=w[1]x+b[1]，a为输入函数的线性组合，这会带来一个问题就是，不管神经网络有多少层，全部都是在做线性组合，这等价于只有一层的神经网络！这就说明了线性激活函数没起到任何作用！</span><br><span class="line"></span><br><span class="line">总结：</span><br><span class="line">①隐藏层使用线性激活函数没起到任何作用！</span><br><span class="line">②可能会存在使用与压缩相关的时候会使用到，但这种情况可以忽略！</span><br></pre></td></tr></table></figure>

<p>7：激活函数的导数（梯度下降）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：σ函数g（z）=1/(1+e^(-z))  导数为g(z)*(1-g(z))，使用导数传递法则既可以证明</span><br><span class="line">2：tanh函数g(z)=tanh(z)=（e^z-e^(-z)）/(e^z+e^(-z))，导数为1-(tanh(z))^2</span><br><span class="line">3：ReLU函数g(z)=max(0,z)，当z&lt;0时，导数=0；当z&gt;0时，导数=1;当z=0时（这个概率可以忽略），你也可以自己设置z=0时候的导数为0或者1</span><br><span class="line">4：Leaky ReLU函数g(z)=max(0.01z,z)，同上逻辑一样。这里需要注意的是0.01这个值，可以理解为经验值，系数为0.01时效果更好！</span><br></pre></td></tr></table></figure>

<p>8：单隐层神经网络的梯度下降（反向传播）法（实现）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1：参数w^[1],b^[1],w^[2],b^[2]；记住中括号[]表示的第几层</span><br><span class="line">2：n_x=n^[0]，也就是输入层的输入特征数，n^[1]表示第一层隐藏单元的个数，n^[2]表示输出单元的个数，此时的n^[2]=1，只有一个输出单元</span><br><span class="line">3：前面已经提到：</span><br><span class="line">w^[1]的维度是（n^[1],n^[0]），b^[1]维度就是（n^[1]，1）</span><br><span class="line">w^[2]的维度是（n^[2],n^[1]），b^[2]维度就是（n^[2]，1）</span><br><span class="line">4：成本函数（二元分类），J（w^[1],b^[1],w^[2],b^[2]）=（1/m）∑(L(j^,y))</span><br><span class="line">5：梯度:dw^[1]=∂J/∂w^[1],db^[1]=∂J/∂b^[1]...dw^[2]...</span><br><span class="line">6：更新：dw^[1]=w^[1]-αdw^[1]...一次更新完成之后就是对样本集全部扫描一遍</span><br></pre></td></tr></table></figure>

<h3 id="四：深层神经网络"><a href="#四：深层神经网络" class="headerlink" title="四：深层神经网络"></a>四：深层神经网络</h3><p>1：深度学习网络</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前面所学的是单隐藏神经网络，接下来介绍其他模型。</span><br><span class="line">逻辑回归是一个浅层神经网络，没有隐藏层；双隐神经网络和多隐层神经网络指的是对应隐藏层的层数。选择需要多少层的隐藏层，这个需要从最开始的逻辑回归开始，慢慢增加隐藏层的个数，来调试得到最优的层数！这时的层数也就是自由选择的超参数。</span><br></pre></td></tr></table></figure>

<p>2：深层网络中的前向传播</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">特征参数：x1,x2,x3</span><br><span class="line">第一层隐藏层：z^[1]=w^[1]x+b^[1],激活值a^[1]=g^[1](z^[1]):这里g^[1]表示第一层使用的激活函数,x为特征参数x1,x2,x3形成的矩阵也等于a[0]也就是第0层的激活单元</span><br><span class="line">第二层：z^[2]=w^[1]a^[1]+b^[2]，激活值a^[2]=g^[2](z^[2]):这里g^[2]表示第二层使用的激活函数</span><br><span class="line">...</span><br><span class="line">输出层z^[l]=w^[l-1]a^[l-1]+b^[l]，激活值(这里也就是输出值)a^[l]=g^[l](z^[l])</span><br><span class="line">处理隐藏层激活函数的时候也需要用到显示for循环，完成之后这里就处理了单个样本；接下来就是对整个样本集向量化处理，之后就是使用梯度下降，多次训练！</span><br><span class="line"></span><br><span class="line">总结：</span><br><span class="line">1.整个梯度下降有2个地方使用到了显示for循环，①遍历隐藏层的时候②遍历多个样本集的时候</span><br><span class="line">2.整个梯度下降有有2个地方使用向量化去除了显示for循环，①输入特征值，或者是隐藏层的激活值②完成一次样本集遍历的时候</span><br></pre></td></tr></table></figure>

<p>3：核对矩阵的维数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">神经网络大部分都是矩阵运算，为了避免出现bug，对矩阵维数的验证极其重要，示例：</span><br><span class="line">1：假设一个5层的神经网络（除去输入层），l=5；其中每一层的神经元个数n^[0]=2(输入层为2个特征值);n^[1]=3;n^[2]=5;n^[3]=4</span><br><span class="line">;n^[4]=2;n^[5]=1(最后一层为1个神经元)；y^。</span><br><span class="line">w[l]和b[l]</span><br><span class="line">2：z=wx+b（无向量化，即单一训练样本）</span><br><span class="line">z^[1]维度为（n^[1],1）,x维度为(n^[0],1)；所以w^[1]的维度就是(n^[1],n^[0])，也和我们前面说到w维度对应了起来；w^[l]的维度就是(n^[l],n^[l-1])；而b^[l]的维度必须与z^[l]的维度一样才能使得等式成立。b^[l]的维度就是（n^[l]，1）</span><br><span class="line">3：向量化（m个样本的样本集）</span><br><span class="line">Z^[1]维度就是（n^[1],m），X维度是（n^[0],m），w^[1]维度是一样,b^[1]维度还是（n^[1],1），只是这里注意b^[1]会广播维度变为（n^[1],m），这一点前面已经提到过！即此时的Z^[l]，A^[l]维度变为了（n^[l],m）</span><br><span class="line"></span><br><span class="line">总结：</span><br><span class="line">无向量化时，z^[l],a^[l]的维度是（n^[l],1）;w^[l]维度是(n^[1],n^[0])，b^[l]的维度是（n^[l]，1）；反向传播dz，da维度与z，a维度一样</span><br><span class="line">向量化时，Z^[l],A^[l]的维度是（n^[l],m）;w^[l]维度是(n^[1],n^[0])，b^[l]的维度是（n^[l]，m）；反向传播dZ，dA维度与Z，A维度一样</span><br></pre></td></tr></table></figure>

<p>4:为什么使用深层神经网络</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">为什么深层神经网络好用？神经网络不需要多大，但是只要深度足够就好用呢？</span><br><span class="line">1：深度神经网络在做什么？例如在人脸识别这一块（卷积神经网络）</span><br><span class="line">第一层用来识别图片的边缘；第二层把图片边缘组合作为输入，每个神经元去识别不同的部分（例如第二层第一个识别鼻子，第二个识别眼睛...）；第三层把所有识别的鼻子眼睛等等作为输入，输出的就是不同的人脸；整个过程可以理解成从细微到整体。</span><br><span class="line">2：另一个理论如下</span><br><span class="line">当需要计算亦或问题时y=x1 or x2 or x3 or x4...xn，这时转换为深层神经网络亦或门个数为O（logn）；如果隐藏层只有1层的神经网络，异或门个数为2^(n-1)个</span><br><span class="line">3：深度学习就是一种对多隐藏神经网络的重新包装！</span><br><span class="line">4：当一个问题出现之后还是会先从单层神经网络开始，把隐藏层个数当前一个超参数去调试！（这一点前面已经提到过）</span><br><span class="line"></span><br><span class="line">总结：深度神经网络就像人脑一样，从简单识别（每个单元）到复杂构建（重组单元）的过程。每一层要做的事情都比前一层要复杂。</span><br></pre></td></tr></table></figure>

<p>5：搭建神经网络块</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">对应第L层的计算：正向</span><br><span class="line">z[L]=w[L]a[L-1]+b[L]</span><br><span class="line">a[L]=g[L](z[L])</span><br><span class="line">已知输入a[L-1]通过参数w[L],b[L]可以得到z[L],通过该层的激活函数g[L]可以得到输出a[L]，以及输出到缓存的Z[L]</span><br><span class="line"></span><br><span class="line">反向：</span><br><span class="line">输入da[L]输出是da[L-1];参数是w[L],b[L],dz[L];通过参数可以得到dw[L],db[L]</span><br></pre></td></tr></table></figure>

<p><img src="C:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191225141747807.png" alt="image-20191225141747807"></p>
<p>6：前向和反向传播</p>
<p><img src="C:%5CUsers%5Cyanyubing%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191225150320753.png" alt="image-20191225150320753"></p>
<p>7：参数VS超参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">参数：w,b</span><br><span class="line">超参数：例如学习率α，隐层数L，隐藏单元数n^[1]，激活函数g(x)...</span><br><span class="line">①超参数需要自己设置</span><br><span class="line">②超参数控制了最后参数w,b的值；所以叫做超参数</span><br><span class="line"></span><br><span class="line">深度学习的过程：Idea→Code→Experiment→Idea...</span><br><span class="line">这个过程中，超参数需要你自己去调整，来确定最优的Experiment</span><br><span class="line"></span><br><span class="line">总结：深度学习的过程也就是确定超参数的过程（try try try...）！一个基于经验的过程。就像yolov3论文中所说的（darknet53层刚刚好），并且超参数在不同情况下可能会有区别，例如不同的CPU,GPU等硬件。</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

	<div>
  
    ﻿<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

    

		

    

    


	
    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/23/AI%E5%AD%A6%E4%B9%A002%E2%80%94%E5%AE%9E%E7%8E%B0/" rel="next" title="AI学习02—实现">
                <i class="fa fa-chevron-left"></i> AI学习02—实现
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/25/AI%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%8902%E2%80%94%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/" rel="prev" title="AI学习（吴恩达）02—超参数调试、正则化以及优化">
                AI学习（吴恩达）02—超参数调试、正则化以及优化 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">鄢玉兵</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习和神经网络"><span class="nav-number">1.</span> <span class="nav-text">深度学习和神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一：深度学习概论"><span class="nav-number">1.0.1.</span> <span class="nav-text">一：深度学习概论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二：神经网络基础"><span class="nav-number">1.0.2.</span> <span class="nav-text">二：神经网络基础</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12：python中numpy的注意点"><span class="nav-number">1.0.3.</span> <span class="nav-text">12：python中numpy的注意点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三：浅层神经网络"><span class="nav-number">1.0.4.</span> <span class="nav-text">三：浅层神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四：深层神经网络"><span class="nav-number">1.0.5.</span> <span class="nav-text">四：深层神经网络</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">鄢玉兵</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
