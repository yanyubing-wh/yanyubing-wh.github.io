<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>鄢玉兵的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yanyubing.xyz/"/>
  <updated>2020-06-17T06:45:44.272Z</updated>
  <id>https://yanyubing.xyz/</id>
  
  <author>
    <name>鄢玉兵</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>feature_matching总结</title>
    <link href="https://yanyubing.xyz/2020/06/16/feature_matching%E6%80%BB%E7%BB%93/"/>
    <id>https://yanyubing.xyz/2020/06/16/feature_matching%E6%80%BB%E7%BB%93/</id>
    <published>2020-06-15T16:42:36.218Z</published>
    <updated>2020-06-17T06:45:44.272Z</updated>
    
    <content type="html"><![CDATA[<pre><code>feature_matching总结</code></pre><p>1：一般特征提取的方法有</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">harris corner detector</span><br><span class="line">SIFT</span><br><span class="line">SURF</span><br><span class="line">FAST</span><br><span class="line">BRIEF</span><br><span class="line">ORB</span><br><span class="line">BRISK</span><br></pre></td></tr></table></figure><p>2：一般特征匹配的方法有</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Brute-Force Matcher</span><br><span class="line">FLANN(Fast Library for Approximate Nearest Neighbors) Matcher</span><br></pre></td></tr></table></figure><p>3：什么是特征点(Feature detection)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Edges：强梯度变换</span><br><span class="line">Corners / interest points：两个边缘的交点</span><br><span class="line">Blobs / regions of interest points：LoG和DoH 斑点检测器</span><br><span class="line">Ridges：从灰度图像计算的脊线描述符可以看作是中间轴的概括（一般不会使用，算法复杂，航空和医学）</span><br></pre></td></tr></table></figure><p>4：什么是特征描述符(Feature description)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">描述特征点周围的向量</span><br></pre></td></tr></table></figure><p>5：什么是特征匹配(Feature matching)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;ratio test&quot; or &quot;nearest neighbor distance ratio test&quot;</span><br><span class="line">匹配两个图片特征点之间的差异性</span><br></pre></td></tr></table></figure><p>6：harris corner detector</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">拐角是一个点，其局部邻域位于两个主要且不同的边缘方向。换句话说，一个角可以解释为两个边缘的交点，其中边缘是图像亮度的突然变化。</span><br></pre></td></tr></table></figure><p>7：SIFT</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">尺度不变特征变换，特点：</span><br><span class="line">局部性：特征是局部性的，因此对遮挡和混乱都很健壮（没有事先分割）</span><br><span class="line">独特性：单个特征可以与大型对象数据库匹配</span><br><span class="line">数量：即使是很小的物体也可以生成许多特征</span><br><span class="line">效率：接近实时性能</span><br><span class="line">可扩展性：可以轻松扩展到各种不同的功能类型，每种功能都增加了鲁棒性</span><br><span class="line"></span><br><span class="line">比例空间峰选择：查找特征的潜在位置，使得尺度不变</span><br><span class="line">比例空间分为八度，八度的数量和比例取决于原始图像的大小。因此，我们生成原始图像的几个八度。每个八度的图像大小是前一个图像的一半。</span><br><span class="line"></span><br><span class="line">模糊化：在一个八度音程中，使用高斯模糊运算符逐渐模糊图像</span><br><span class="line"></span><br><span class="line">DOG：（高斯核的差）</span><br><span class="line">我们使用那些模糊的图像来生成另一组图像，即高斯差分（DoG）；这些DoG图像非常适合找出图像中有趣的关键点。</span><br><span class="line"></span><br><span class="line">寻找关键点：</span><br><span class="line">将图像中的一个像素与其8个邻居，下一个比例的9个像素和先前比例的9个像素进行比较。这样，总共进行了26次检查。如果是局部极值，则可能是关键点。从根本上说，关键点是最好的代表。</span><br><span class="line"></span><br><span class="line">关键点本地化：准确定位功能关键点。</span><br><span class="line">他们使用了尺度空间的泰勒级数展开来获得更精确的极值位置，并且如果该极值处的强度小于阈值（根据论文为0.03），则将其拒绝。DoG对边缘的响应较高，因此也需要删除边缘。</span><br><span class="line">主要是去除边缘特性</span><br><span class="line"></span><br><span class="line">方向分配：为关键点分配方向，目的是使得旋转不变</span><br><span class="line">取360°分为36份，每份分为10°；如果该点（在“方向收集区域”中）的渐变方向为18.759度。则那么它将进入10–19度的bin（直方图）中。</span><br><span class="line">提取直方图中的最高峰，并且将其超过80％的任何峰也视为计算方向。它创建的位置和比例相同但方向不同的关键点。它有助于匹配的稳定性。</span><br><span class="line"></span><br><span class="line">关键点描述符：将关键点描述为高维向量。</span><br><span class="line">到此为止，每个店都有位置，比例和方向。接下来是为每个关键点周围的局部图像区域计算一个描述符，该描述符对于诸如视点和照明的变化之类的变化具有高度的独特性和不变性。为此，将在关键点周围使用一个16x16的窗口。它分为16个4x4大小的子块。</span><br><span class="line">对于每个子块，创建8 bin方向直方图。</span><br><span class="line">①旋转相关性特征向量使用梯度方向。显然，如果旋转图像，一切都会改变。所有的梯度方向也会改变。为了实现旋转独立性，从每个方向减去关键点的旋转。因此，每个梯度方向都相对于关键点的方向。</span><br><span class="line">②照明依赖性如果我们将较大的阈值设为阈值，则可以实现照明依赖性。因此，任何大于0.2的数（128个数）都将更改为0.2。再次将该结果特征向量归一化。现在，您有了一个与照明无关的特征向量！</span><br><span class="line"></span><br><span class="line">关键点匹配：</span><br><span class="line">通过识别两个图像之间的关键点来匹配它们之间的关键点。但是在某些情况下，第二个最接近的匹配可能非常接近第一个。它可能是由于噪音或其他原因而发生的。在那种情况下，采用最接近距离与第二最接近距离之比。如果大于0.8，将被拒绝。根据论文，它可以消除大约90％的错误匹配，而只丢弃5％的正确匹配。</span><br></pre></td></tr></table></figure><p>8：SURF</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">快</span><br></pre></td></tr></table></figure><p>9：FAST</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">快，拐角检测器：</span><br><span class="line">由于检测到的角必须在包括角的两个边缘的中心周围具有较暗或较亮的像素值环，因此清晰的图像效果不佳。</span><br></pre></td></tr></table></figure><p>10：BRIEF</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Binary Robust Independent Elementary Features：二进制独立鲁棒特征描述符</span><br></pre></td></tr></table></figure><p>11： ORB</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Oriented FAST and Rotated BRIEF</span><br><span class="line">在特征检测任务上，ORB的性能与SIFT一样好（并且比SURF更好），而速度却快了两个数量级。ORB基于著名的FAST关键点检测器和Brief描述符。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;feature_matching总结&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1：一般特征提取的方法有&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>paper-summary</title>
    <link href="https://yanyubing.xyz/2020/06/13/paper-summary/"/>
    <id>https://yanyubing.xyz/2020/06/13/paper-summary/</id>
    <published>2020-06-13T02:26:00.570Z</published>
    <updated>2020-06-19T02:08:36.678Z</updated>
    
    <content type="html"><![CDATA[<h3 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h3><p>1：<strong>Pyramid Mask Text Detector</strong> (2019)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1:pixel-level regression  代替传统Mask R-CNN的  binary text mask</span><br><span class="line">①二进制掩码定义原始图像的关注区域（ROI）。mask像素值 1表示图像像素属于ROI。mask像素值0表示图像像素是背景的一部分。</span><br><span class="line">②2D空间转换为3D空间的思想</span><br><span class="line"></span><br><span class="line">2: 传统方式没有解决的问题</span><br><span class="line">①监督简化：场景基本上基于不同的背景，但是没有特别的形状</span><br><span class="line">②错误的分割方式：会导致不属于RIO区域的背景会被识别进RIO区域</span><br><span class="line">③错误传播：二进制mask的区域基于Mask R-CNN的预测框，当预测框不准确时，mask也会错误</span><br><span class="line"></span><br><span class="line">3： “soft” semantic segmentation</span><br><span class="line">①根据距离文本框的距离编码0-1；</span><br><span class="line"></span><br><span class="line">4： plane clustering algorithm</span><br><span class="line">平面聚类算法：</span><br><span class="line">①找到四边形的中心0</span><br><span class="line">②向量计算0P=a*OM+b*OM;得到a,b</span><br><span class="line">③根据a,b的值范围，判断P属于哪个区域</span><br><span class="line"></span><br><span class="line">5：使用到的数据增强方式:</span><br><span class="line">①. Random horizon flip with a probability of 0.5.</span><br><span class="line">②. Random resize the height and width of images to 640-</span><br><span class="line">2560 individually, without keeping the original aspect</span><br><span class="line">ratio.</span><br><span class="line">③Random select one 640 × 640 crop region from the</span><br><span class="line">resized image.</span><br><span class="line"></span><br><span class="line">6：使用二进制差值上采样替代反卷积</span><br><span class="line">①因为反卷积之后会产生过多的棋盘纹，不利于后续回归</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;paper&quot;&gt;&lt;a href=&quot;#paper&quot; class=&quot;headerlink&quot; title=&quot;paper&quot;&gt;&lt;/a&gt;paper&lt;/h3&gt;&lt;p&gt;1：&lt;strong&gt;Pyramid Mask Text Detector&lt;/strong&gt; (2019)&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>PyTorch</title>
    <link href="https://yanyubing.xyz/2020/05/31/PyTorch/"/>
    <id>https://yanyubing.xyz/2020/05/31/PyTorch/</id>
    <published>2020-05-31T13:01:12.594Z</published>
    <updated>2020-05-31T13:28:51.882Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch官方书籍deep-learning-with-pytorch</p><p>1:深度学习框架对比</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Theano是最早的深度学习框架之一，已停止积极发展。</span><br><span class="line">TensorFlow：</span><br><span class="line"></span><br><span class="line">完全消耗Keras，将其升级为一流的API</span><br><span class="line">提供了立即执行的“渴望模式”</span><br><span class="line">宣布TF 2.0将默认启用eager模式</span><br><span class="line">PyTorch：</span><br><span class="line"></span><br><span class="line">消耗了Caffe2作为后端</span><br><span class="line">替换了基于Lua的Torch项目中重复使用的大多数低级代码</span><br><span class="line">增加了对ONNX的支持，这是一种与供应商无关的模型描述和交换格式</span><br><span class="line">添加了名为TorchScript的延迟执行“图形模式”运行时</span><br><span class="line">发行版本1.0</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;PyTorch官方书籍deep-learning-with-pytorch&lt;/p&gt;
&lt;p&gt;1:深度学习框架对比&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span clas
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>机器学习记录(已掌握)</title>
    <link href="https://yanyubing.xyz/2020/05/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95(%E5%B7%B2%E6%8E%8C%E6%8F%A1)/"/>
    <id>https://yanyubing.xyz/2020/05/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95(%E5%B7%B2%E6%8E%8C%E6%8F%A1)/</id>
    <published>2020-05-31T11:30:27.688Z</published>
    <updated>2020-05-31T12:36:55.738Z</updated>
    
    <content type="html"><![CDATA[<p>1：一元线性回归</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">适用于一个特征：x对应的标签y的数据集</span><br><span class="line"></span><br><span class="line">y=wx+b：找到最优的w和b使得代价函数最小</span><br></pre></td></tr></table></figure><p>2：损失函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">最小二乘法</span><br><span class="line">代价函数j=预测值h减去真实值y的平方求和，除以2倍的样本个数m</span><br><span class="line"></span><br><span class="line">常见的损失函数种类：https://zhuanlan.zhihu.com/p/47202768</span><br></pre></td></tr></table></figure><p>3： 梯度下降（优化算法）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对损失函数求导，往斜率反方向更新值，得到局部最优解。</span><br><span class="line">1：为什么不直接对损失函数求导取倒数为0的点？倒数为0只能说明斜率为0，不能说明是最小值，或者极小值</span><br><span class="line"></span><br><span class="line">2：带动量的梯度下降可以越过鞍部</span><br><span class="line"></span><br><span class="line">3:问题点是，非凸函数难以找到全局最小值</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1：一元线性回归&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>得到ROI区域总结</title>
    <link href="https://yanyubing.xyz/2020/05/25/%E5%BE%97%E5%88%B0ROI%E5%8C%BA%E5%9F%9F%E6%80%BB%E7%BB%93/"/>
    <id>https://yanyubing.xyz/2020/05/25/%E5%BE%97%E5%88%B0ROI%E5%8C%BA%E5%9F%9F%E6%80%BB%E7%BB%93/</id>
    <published>2020-05-25T09:42:37.290Z</published>
    <updated>2020-05-25T09:43:20.898Z</updated>
    
    <content type="html"><![CDATA[<p>1：基于颜色值不同的ROI区域</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"># 检测答案区域,根据颜色判断,并且保存区域</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 判断像素是不是红色</span><br><span class="line">def isRed(pixel):</span><br><span class="line">    # 纯红色</span><br><span class="line">    if pixel[2] &gt; pixel[0] + 100 and pixel[2] &gt; pixel[1] + 100 and (pixel[2] &gt; 200):</span><br><span class="line">        return &apos;t&apos;</span><br><span class="line">    else:</span><br><span class="line">        return &apos;f&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ①除了红色区域的所有区域转为白色</span><br><span class="line">def getRedPicture(path):</span><br><span class="line">    image = cv2.imread(path)</span><br><span class="line">    h, w, c = image.shape</span><br><span class="line"></span><br><span class="line">    for i in range(w):</span><br><span class="line">        for j in range(h):</span><br><span class="line">            pixel = image[j][i]</span><br><span class="line">            if isRed(pixel) == &apos;f&apos;:</span><br><span class="line">                pixel[:] = 255</span><br><span class="line">    print(path, &apos;finished---getRedPicture&apos;)</span><br><span class="line">    # 图片的下方一行需要手动去除</span><br><span class="line">    image[3100:, :] = 255</span><br><span class="line">    return image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取轮廓的最上下两个点，只需要根据上下两个点的距离过滤</span><br><span class="line">def getContours_XY_dots(cnt):</span><br><span class="line">    # 储存x坐标</span><br><span class="line">    xs = []</span><br><span class="line">    # 储存y坐标</span><br><span class="line">    ys = []</span><br><span class="line">    for c in cnt:</span><br><span class="line">        ys.append(c[0][1])</span><br><span class="line">        xs.append(c[0][0])</span><br><span class="line">    xs.sort(key=int)</span><br><span class="line">    ys.sort(key=int)</span><br><span class="line"></span><br><span class="line">    min_x = xs[0]</span><br><span class="line">    max_x = xs[-1]</span><br><span class="line">    max_y = ys[-1]</span><br><span class="line">    min_y = ys[0]</span><br><span class="line">    return min_x, max_x, min_y, max_y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取红色区域的定位</span><br><span class="line">def getRedLocation(path):</span><br><span class="line">    # 获取红色图片</span><br><span class="line">    img = getRedPicture(path)</span><br><span class="line"></span><br><span class="line">    # 转换为灰度</span><br><span class="line">    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)</span><br><span class="line"></span><br><span class="line">    # 二值化</span><br><span class="line">    _, thresh1 = cv2.threshold(img_gray, 200, 255, cv2.THRESH_BINARY)</span><br><span class="line">    # 黑色和白色对调，因为膨胀或者腐蚀的前景都是白色</span><br><span class="line">    dst = 255 - thresh1</span><br><span class="line">    # 膨胀核</span><br><span class="line">    kernel = np.ones((5, 5), np.uint8)</span><br><span class="line">    # 膨胀之后的图片</span><br><span class="line">    dilation = cv2.dilate(dst, kernel, iterations=5)  # 膨胀</span><br><span class="line">    # 查找轮廓</span><br><span class="line">    _, contours, hierarchy = cv2.findContours(dilation, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">    # 对轮廓进行判断,①轮廓的高取值范围，②轮廓的面积取值范围，最后添加到最终的轮廓中</span><br><span class="line">    res_contours = []  # 储存需要的轮廓</span><br><span class="line">    for index, cnt in enumerate(contours):</span><br><span class="line">        area = cv2.contourArea(cnt)</span><br><span class="line">        print(index, &apos;--------&apos;, area)</span><br><span class="line">        min_x, max_x, min_y, max_y = getContours_XY_dots(cnt)</span><br><span class="line">        yLength = max_y - min_y</span><br><span class="line">        print(yLength)</span><br><span class="line">        # 这里范围可调</span><br><span class="line">        if area &gt; 500 and area &lt; 20000 and yLength &gt; 10 and yLength &lt; 70:</span><br><span class="line">            res_contours.append(cnt)</span><br><span class="line"></span><br><span class="line">    res = cv2.drawContours(img, res_contours, -1, (0, 255, 0), 10)</span><br><span class="line"></span><br><span class="line">    # 遍历最终的轮廓，得到最小x,y,最大x,y的坐标</span><br><span class="line">    # 储存一张图片的所有box</span><br><span class="line">    bboxes = []</span><br><span class="line">    for res in res_contours:</span><br><span class="line">        min_x, max_x, min_y, max_y = getContours_XY_dots(res)</span><br><span class="line">        box = min_x, max_x, min_y, max_y</span><br><span class="line">        bboxes.append(box)</span><br><span class="line">    return bboxes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 图片地址</span><br><span class="line">def saveAllRedLocation(path):</span><br><span class="line">    image = cv2.imread(path)</span><br><span class="line">    bboxes = getRedLocation(path)</span><br><span class="line">    # bboxes排序，根据纵坐标</span><br><span class="line">    bboxes.sort(key=lambda x: int(x[2]))</span><br><span class="line">    index = 0</span><br><span class="line">    # 储存每个box的图片</span><br><span class="line">    for box in bboxes:</span><br><span class="line">        index += 1</span><br><span class="line">        path_out = &apos;red/&apos; + path.split(&apos;/&apos;)[1].split(&apos;.&apos;)[0] + &apos;_&apos; + str(index) + &apos;.jpg&apos;</span><br><span class="line">        print(path_out)</span><br><span class="line">        cv2.imwrite(path_out, image[box[2]:box[3], box[0]:box[1]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">path = &apos;books&apos;</span><br><span class="line">files = os.listdir(path)</span><br><span class="line"># 储存整本书的所有红色区域</span><br><span class="line">for file in files:</span><br><span class="line">    pathname = path + &apos;/&apos; + file</span><br><span class="line">    saveAllRedLocation(pathname)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1：基于颜色值不同的ROI区域&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>倾斜校正相关总结对比</title>
    <link href="https://yanyubing.xyz/2020/05/22/%E5%80%BE%E6%96%9C%E6%A0%A1%E6%AD%A3%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94/"/>
    <id>https://yanyubing.xyz/2020/05/22/%E5%80%BE%E6%96%9C%E6%A0%A1%E6%AD%A3%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94/</id>
    <published>2020-05-22T02:30:15.814Z</published>
    <updated>2020-05-23T01:36:31.810Z</updated>
    
    <content type="html"><![CDATA[<h3 id="倾斜校正相关总结对比"><a href="#倾斜校正相关总结对比" class="headerlink" title="倾斜校正相关总结对比"></a>倾斜校正相关总结对比</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以下一种方式即可：核心逻辑都是找到倾斜角度，然后校正</span><br></pre></td></tr></table></figure><p>方案1：最小外接矩形→倾斜角度→得到变换矩阵→纺射变换得到校正图</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">地址：https://www.pyimagesearch.com/2017/02/20/text-skew-correction-opencv-python/</span><br><span class="line"></span><br><span class="line">效果：书本的校正效果95分</span><br><span class="line">困难点：通过局部图片得到倾斜角度，这个局部图片的确定，如果杂点过多，很难得到角度</span><br><span class="line">困难点的解决方案：找到高度的h1,h2；h1整行为白色（往下一个像素就会有黑色）,h2整行也为白色（往上一行就会有黑色），然后截取；</span><br></pre></td></tr></table></figure><p>代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"># import the necessary packages</span><br><span class="line">import numpy as np</span><br><span class="line">import argparse</span><br><span class="line">import cv2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 得到目标区域,num为起始位置的区域得分&lt;30</span><br><span class="line">def getLocation(image, n):</span><br><span class="line">    h, w, c = image.shape</span><br><span class="line">    # 转成灰度和二值图</span><br><span class="line">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    thresh = cv2.threshold(gray, 127, 255,</span><br><span class="line">                           cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]</span><br><span class="line">    # 定义两个高度，第一个和最后一个就是高度起始和结束</span><br><span class="line">    h_temp = []</span><br><span class="line">    # 找到高度的h1,h2；h1整行为白色（往下一个像素就会有黑色）,h2整行也为白色（往上一行就会有黑色），然后截取</span><br><span class="line">    start = 0</span><br><span class="line">    # h/5开始，找到黑色区域起始位置</span><br><span class="line">    for i in range(int(n * h / 30), h, 2):</span><br><span class="line">        # 每一条初始化像素值个数</span><br><span class="line">        global num</span><br><span class="line">        num = 0</span><br><span class="line">        for j in range(w):</span><br><span class="line">            if thresh[i][j] &lt; 100:</span><br><span class="line">                # 开始有黑色点</span><br><span class="line">                # 像素值个数+1</span><br><span class="line">                num += 1</span><br><span class="line">        # 黑点个数为10</span><br><span class="line">        if 0 &lt; num &lt; 10:</span><br><span class="line">            start = i</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # 已经没有黑色区域的时候，遍历到全白</span><br><span class="line">    if num == 0:</span><br><span class="line">        start = int(n * h / 30)</span><br><span class="line"></span><br><span class="line">    # 白色区域开始遍历</span><br><span class="line">    for i in range(start, h, 2):</span><br><span class="line">        # 每一条初始化像素值个数</span><br><span class="line">        num = 0</span><br><span class="line">        for j in range(w):</span><br><span class="line">            if thresh[i][j] &lt; 100:</span><br><span class="line">                # 开始有黑色点</span><br><span class="line">                # 像素值个数+1</span><br><span class="line">                num += 1</span><br><span class="line">        if num &gt; 0:</span><br><span class="line">            h_temp.append(i)</span><br><span class="line"></span><br><span class="line">        if num == 0 and len(h_temp) &gt; 0:</span><br><span class="line">            # 证明已经经过了黑色区域，结束循环</span><br><span class="line">            break</span><br><span class="line">    if len(h_temp) &gt; 0:</span><br><span class="line">        h_temp.sort(key=int)</span><br><span class="line"></span><br><span class="line">        h1 = h_temp[0]</span><br><span class="line">        h2 = h_temp[-1]</span><br><span class="line">        # 取值</span><br><span class="line">        imagenew = image[h1:h2, :]</span><br><span class="line">        # 纵向上下拼接20个像素的白色区域</span><br><span class="line">        image_temp = np.zeros((10, imagenew.shape[1], 3), np.uint8)</span><br><span class="line">        image_temp[:] = 255</span><br><span class="line">        # 拼接结果</span><br><span class="line">        result = np.vstack([image_temp, imagenew, image_temp])</span><br><span class="line">    else:</span><br><span class="line">        result = np.zeros((10, 10, 3), np.uint8)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 传入图片地址，获取角度</span><br><span class="line">def getAngle(image, n):</span><br><span class="line">    # 获取目标区域</span><br><span class="line">    location = getLocation(image, n)</span><br><span class="line"></span><br><span class="line">    # location转成灰度，得到角度</span><br><span class="line">    gray = cv2.cvtColor(location, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    gray = cv2.bitwise_not(gray)</span><br><span class="line">    # 二值化</span><br><span class="line">    thresh = cv2.threshold(gray, 0, 255,</span><br><span class="line">                           cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]</span><br><span class="line"></span><br><span class="line">    coords = np.column_stack(np.where(thresh &gt; 0))</span><br><span class="line">    angle = cv2.minAreaRect(coords)[-1]</span><br><span class="line">    if angle &lt; -45:</span><br><span class="line">        angle = -(90 + angle)</span><br><span class="line">    else:</span><br><span class="line">        angle = -angle</span><br><span class="line">    print(&apos;angle:&apos;, angle)</span><br><span class="line"></span><br><span class="line">    return angle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取最终的角度</span><br><span class="line">def getResAngle(image):</span><br><span class="line">    angles = []</span><br><span class="line">    for n in range(30):</span><br><span class="line">        angle_temp = getAngle(image, n)</span><br><span class="line">        angles.append(angle_temp)</span><br><span class="line"></span><br><span class="line">    # 角度排序</span><br><span class="line">    angles.sort()</span><br><span class="line">    # 计算大于0和小于0的倾斜角度</span><br><span class="line">    Asum = 0</span><br><span class="line">    asum = 0</span><br><span class="line">    for a in angles:</span><br><span class="line">        # 判断角度大于0和小于0的个数:用来确定最终的偏斜角度</span><br><span class="line">        if a &gt; 0:</span><br><span class="line">            Asum += 1</span><br><span class="line">        if a &lt; 0:</span><br><span class="line">            asum += 1</span><br><span class="line">        # 去除异常值的点</span><br><span class="line">        if a &gt; 15 or a &lt; -15:</span><br><span class="line">            angles.remove(a)</span><br><span class="line">    # 获取最终角度的值</span><br><span class="line">    if Asum &gt; asum:</span><br><span class="line">        angleRes = angles[-1]</span><br><span class="line">    else:</span><br><span class="line">        angleRes = angles[0]</span><br><span class="line">    print(&apos;最后取得的角度为&apos;, angleRes)</span><br><span class="line">    return angleRes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 校正图片</span><br><span class="line">def correctSkew(image, angle):</span><br><span class="line">    # rotate the image to deskew it</span><br><span class="line">    (h, w) = image.shape[:2]</span><br><span class="line">    center = (w // 2, h // 2)</span><br><span class="line">    M = cv2.getRotationMatrix2D(center, angle, 1.0)</span><br><span class="line">    rotated = cv2.warpAffine(image, M, (w, h),</span><br><span class="line">                             flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)</span><br><span class="line"></span><br><span class="line">    return rotated</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imagepath = &apos;image/2.jpg&apos;</span><br><span class="line">image = cv2.imread(imagepath)</span><br><span class="line">angleRes = getResAngle(image)</span><br><span class="line"></span><br><span class="line"># 校正</span><br><span class="line">rotated = correctSkew(image, angleRes)</span><br><span class="line">cv2.imwrite(&apos;rotate.jpg&apos;, rotated)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;倾斜校正相关总结对比&quot;&gt;&lt;a href=&quot;#倾斜校正相关总结对比&quot; class=&quot;headerlink&quot; title=&quot;倾斜校正相关总结对比&quot;&gt;&lt;/a&gt;倾斜校正相关总结对比&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;t
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>OCR相关总结对比</title>
    <link href="https://yanyubing.xyz/2020/05/13/OCR%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94/"/>
    <id>https://yanyubing.xyz/2020/05/13/OCR%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94/</id>
    <published>2020-05-13T03:01:04.332Z</published>
    <updated>2020-06-23T08:09:23.712Z</updated>
    
    <content type="html"><![CDATA[<p>OCR总结和对比；实现书本的题干提取</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">总结：OCR对于识别的准确率基本可以满足要求，只是前提是定位的准确和一些预处理操作以及对应不同场景的不同处理方案的搭配（如中英文不同，特殊字符的处理等等）</span><br></pre></td></tr></table></figure><p>1：百度ocr</p><p>1.1:特点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1：付费</span><br><span class="line">2：偶尔报错</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;C:/Users/yanyubing/Desktop/zex/010_GUI/ocr/ocr_Topic.py&quot;, line 112, in &lt;module&gt;</span><br><span class="line">    results = client.general(image)[&quot;words_result&quot;]</span><br><span class="line">KeyError: &apos;words_result&apos;</span><br><span class="line">3：网络请求</span><br><span class="line">4：准确率基本满足要求</span><br><span class="line">5：识别数字很烂</span><br><span class="line"></span><br><span class="line">总结：可以满足生产需求</span><br></pre></td></tr></table></figure><p>1.2:代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"># 题干的ocr</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">import cv2</span><br><span class="line">from aip import AipOcr</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot; 你的 APPID AK SK &quot;&quot;&quot;</span><br><span class="line">#这里有更改</span><br><span class="line">APP_ID = &apos;198605*&apos;</span><br><span class="line">API_KEY = &apos;R7fGy5Yh900UQKXmlppPc69d&apos;</span><br><span class="line">SECRET_KEY = &apos;v2OKtKnslZq34qNQKQ4dZCGwjONxK9xY&apos;</span><br><span class="line"></span><br><span class="line">client = AipOcr(APP_ID, API_KEY, SECRET_KEY)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_file_content(filePath):</span><br><span class="line">    with open(filePath, &apos;rb&apos;) as fp:</span><br><span class="line">        return fp.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># step1：获取定位框的最左排序个数</span><br><span class="line"># num:输入获取的个数</span><br><span class="line">def getTopic(num):</span><br><span class="line">    # 没有题干的提前结束</span><br><span class="line">    if num == 0:</span><br><span class="line">        return</span><br><span class="line">    global str_temp</span><br><span class="line"></span><br><span class="line">    # 获取所有识别的集合</span><br><span class="line">    left_temp = []</span><br><span class="line"></span><br><span class="line">    for result in results:</span><br><span class="line">        # 文本</span><br><span class="line">        text = result[&quot;words&quot;]</span><br><span class="line"></span><br><span class="line">        # 定位</span><br><span class="line">        location = result[&quot;location&quot;]</span><br><span class="line"></span><br><span class="line">        # 得到字段：最左边，高度定位，和文本信息</span><br><span class="line">        strtemp = str(location[&apos;left&apos;]) + &apos;,&apos; + str(location[&apos;top&apos;]) + &apos;,&apos; + text</span><br><span class="line"></span><br><span class="line">        # 添加</span><br><span class="line">        left_temp.append(strtemp)</span><br><span class="line"></span><br><span class="line">    # 获取需要的集合,根据左边的位置排序</span><br><span class="line">    lefts = []</span><br><span class="line">    while 1:</span><br><span class="line">        # 获取最小的x值</span><br><span class="line">        min_x = 10000</span><br><span class="line">        for temp in left_temp:</span><br><span class="line">            if int(temp.split(&apos;,&apos;)[0]) &lt; min_x:</span><br><span class="line">                min_x = int(temp.split(&apos;,&apos;)[0])</span><br><span class="line">                str_temp = temp</span><br><span class="line"></span><br><span class="line">        lefts.append(str_temp)</span><br><span class="line">        # print(str_temp)</span><br><span class="line">        left_temp.remove(str_temp)</span><br><span class="line"></span><br><span class="line">        if len(lefts) == num:</span><br><span class="line">            break</span><br><span class="line">    # 左边位置排序之后再根据上下位置排序</span><br><span class="line">    result = []</span><br><span class="line"></span><br><span class="line">    while 1:</span><br><span class="line">        # 获取最小的x值</span><br><span class="line">        min_y = 10000</span><br><span class="line">        for temp in lefts:</span><br><span class="line">            if int(temp.split(&apos;,&apos;)[1]) &lt; min_y:</span><br><span class="line">                min_y = int(temp.split(&apos;,&apos;)[1])</span><br><span class="line">                str_temp = temp</span><br><span class="line"></span><br><span class="line">        result.append(str_temp)</span><br><span class="line">        # print(str_temp)</span><br><span class="line">        lefts.remove(str_temp)</span><br><span class="line"></span><br><span class="line">        if len(lefts) == 0:</span><br><span class="line">            # 过滤完全结束</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取所有书名，按照顺序排序</span><br><span class="line">def getBookNames(path):</span><br><span class="line">    booknames = []</span><br><span class="line">    filesname = os.listdir(path)</span><br><span class="line">    for i in range(len(filesname)):</span><br><span class="line">        name = path + &apos;/&apos; + str(i + 1) + &apos;.jpg&apos;</span><br><span class="line">        booknames.append(name)</span><br><span class="line">    return booknames</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 输入图片所在目录</span><br><span class="line">dir = input(&apos;输入图片所在文件夹:\n&apos;)</span><br><span class="line"></span><br><span class="line"># 获取所有的书名</span><br><span class="line">booknames = getBookNames(dir)</span><br><span class="line"></span><br><span class="line"># 输入对应要获取题干的个数，</span><br><span class="line"># nums = []</span><br><span class="line">nums = input(&apos;连续输入页码题干个数\n&apos;)</span><br><span class="line"># for bookname in booknames:</span><br><span class="line">#     num = int(input(&apos;输入需要获取页面:&apos; + bookname + &apos;的题干的个数:\n&apos;))</span><br><span class="line">#     nums.append(num)</span><br><span class="line"></span><br><span class="line"># 整本书的结果</span><br><span class="line">allResults = []</span><br><span class="line"># 遍历所有书</span><br><span class="line">for index in range(len(booknames)):</span><br><span class="line">    image = get_file_content(booknames[index])</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot; 调用通用文字识别, 图片参数为本地图片 &quot;&quot;&quot;</span><br><span class="line">    results = client.general(image)[&quot;words_result&quot;]</span><br><span class="line"></span><br><span class="line">    # 一页书的结果</span><br><span class="line">    result = getTopic(int(nums[index]))</span><br><span class="line">    print(&quot;----&quot;, index, &quot;----&quot;)</span><br><span class="line">    for re in result:</span><br><span class="line">        # 添加页码信息</span><br><span class="line">        r = re + &apos;,&apos; + booknames[index]</span><br><span class="line">        # 整本书的结果</span><br><span class="line">        allResults.append(result)</span><br><span class="line"></span><br><span class="line"># 查看整本书的结果</span><br><span class="line">for result in allResults:</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><p>2：pse+rcnn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1：免费</span><br><span class="line">2：需要编译：本人使用vs2017+python3.7编译</span><br><span class="line">3：可以识别竖向文字，斜向文字也可以</span><br><span class="line">4：识别通用文字效果不太好:</span><br><span class="line">5：对于英文识别错误率高</span><br><span class="line">对于英文：误识，漏识严重</span><br><span class="line">对于中文:也存在一定的误识和漏识</span><br><span class="line">github地址：https://github.com/ouyanghuiyu/chineseocr_lite</span><br><span class="line"></span><br><span class="line">总结：无法满足生产需求。①定位有尺寸压缩，并且定位有偏差；②识别有误识</span><br><span class="line"></span><br><span class="line">解决方案：总体而言是因为pse定位存在误差，导致识别上的错误；使用自己的mark去定位，然后识别，准确率可以达到99%</span><br><span class="line">①取mark一定100%准确</span><br><span class="line">②根据mark去location一定100%准确</span><br><span class="line">③识别准确率才能到达极限</span><br><span class="line">④识别数字可以，识别英文很烂</span><br></pre></td></tr></table></figure><p>3：ocr.space</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">特点：</span><br><span class="line">①需要翻墙</span><br><span class="line">②多种语言和特殊字符的支持</span><br><span class="line">③使用简洁：但是有限制</span><br></pre></td></tr></table></figure><p>4： CRAFT(英文字符识别)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">地址：https://github.com/clovaai/CRAFT-pytorch+https://github.com/clovaai/deep-text-recognition-benchmark</span><br><span class="line">原理：CRAFT+deep-text-recognition(检测+识别)</span><br><span class="line">特点：</span><br><span class="line">①英文识别能力强，单个单词准确率达到99%</span><br><span class="line">②定位准确：切割单个单词准确率高99%</span><br><span class="line">③需要自己糅合</span><br></pre></td></tr></table></figure><p>4.1：定位</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"># 定位和识别一体</span><br><span class="line">import argparse</span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line">import cv2</span><br><span class="line"></span><br><span class="line">from getLocations import getLocation</span><br><span class="line"></span><br><span class="line"># 存放txt的文件夹</span><br><span class="line">result_folder = &apos;txtResult/&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取txt文件</span><br><span class="line">def getTextFile(path):</span><br><span class="line">    # 存放定位之后的文件路径</span><br><span class="line"></span><br><span class="line">    # 存在结果文件夹</span><br><span class="line">    if os.path.exists(result_folder):</span><br><span class="line">        # 删除文件夹(非空)</span><br><span class="line">        shutil.rmtree(result_folder)</span><br><span class="line">    # 运行，会产生位置信息的txt文件</span><br><span class="line">    getLocation(path, result_folder)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取整个文件夹识别之后的集合</span><br><span class="line"># 格式为： &apos;&apos;&apos;文件路径：value&apos;&apos;&apos;</span><br><span class="line">def saveLocation(pathin, pathout):</span><br><span class="line">    if not os.path.exists(pathout):</span><br><span class="line">        os.mkdir(pathout)</span><br><span class="line">    # 获取原文件夹底下的文件列表</span><br><span class="line">    files = os.listdir(pathin)</span><br><span class="line">    # 遍历列表</span><br><span class="line">    for file in files:</span><br><span class="line">        # 获取不带后缀的文件名</span><br><span class="line">        filename = file.split(&apos;.&apos;)[0]</span><br><span class="line">        # 图片文件路径</span><br><span class="line">        ImgFilepath = pathin + &apos;/&apos; + file</span><br><span class="line">        # 读取图片</span><br><span class="line">        image = cv2.imread(ImgFilepath)</span><br><span class="line">        # 构建lines集合储存txt文件的lines</span><br><span class="line">        lines = []</span><br><span class="line">        # txt文件路径</span><br><span class="line">        TxtFilepath = result_folder + &apos;/&apos; + filename + &apos;.txt&apos;</span><br><span class="line">        # 读取txt文件</span><br><span class="line">        f = open(TxtFilepath)</span><br><span class="line">        line = f.readlines()</span><br><span class="line">        for li in line:</span><br><span class="line">            lines.append(li)</span><br><span class="line">        # 记录第几条数据</span><br><span class="line">        index = 0</span><br><span class="line">        # 去除多条数据中的空格</span><br><span class="line">        lines = [x.strip() for x in lines if x.strip() != &apos;&apos;]</span><br><span class="line">        # 根据横坐标位置排序，为了后面便于拼接</span><br><span class="line">        lines.sort(key=lambda x: int(x.split(&apos;,&apos;)[0]))</span><br><span class="line">        # 遍历lines，得到坐标位置</span><br><span class="line">        for line in lines:</span><br><span class="line">            # 除去每条数据中的空格</span><br><span class="line">            line = line[:-1]</span><br><span class="line">            index += 1</span><br><span class="line">            ls = line.split(&apos;,&apos;)</span><br><span class="line">            # 判断坐标位置的大小，得到左上和右下坐标的矩形框</span><br><span class="line">            # 左上</span><br><span class="line">            lt = (min(ls[0], ls[6]), min(ls[1], ls[3]))</span><br><span class="line"></span><br><span class="line">            # 右下</span><br><span class="line">            rd = (max(ls[4], ls[2]), max(ls[5], ls[7]))</span><br><span class="line">            # 得到定位之后的单个单词位置</span><br><span class="line">            imagetemp = image[int(lt[1]):int(rd[1]), int(lt[0]):int(rd[0])]</span><br><span class="line">            # 文件路径名，如1_1，则是第一个图的第一个单词</span><br><span class="line">            path_new = pathout + &apos;/&apos; + filename + &apos;_&apos; + str(index) + &apos;.jpg&apos;</span><br><span class="line">            cv2.imwrite(path_new, imagetemp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 预处理文件处理,修改path</span><br><span class="line">path = &apos;two&apos;</span><br><span class="line">pathin = path</span><br><span class="line">pathout = path + &apos;out&apos;</span><br><span class="line">getTextFile(path)</span><br><span class="line"></span><br><span class="line">saveLocation(pathin, pathout)</span><br></pre></td></tr></table></figure><p>4.2:识别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line">import string</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.backends.cudnn as cudnn</span><br><span class="line">import torch.utils.data</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">from utils import CTCLabelConverter, AttnLabelConverter</span><br><span class="line">from dataset import RawDataset, AlignCollate</span><br><span class="line">from model import Model</span><br><span class="line"></span><br><span class="line">device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def demo(opt):</span><br><span class="line">    # 储存所有的文本信息</span><br><span class="line">    # 格式：图片名称：值</span><br><span class="line">    values = []</span><br><span class="line">    # 临时存储，需要变换</span><br><span class="line">    valuetemp = []</span><br><span class="line">    &quot;&quot;&quot; model configuration &quot;&quot;&quot;</span><br><span class="line">    # CTC模型</span><br><span class="line">    if &apos;CTC&apos; in opt.Prediction:</span><br><span class="line">        converter = CTCLabelConverter(opt.character)</span><br><span class="line">    else:</span><br><span class="line">        converter = AttnLabelConverter(opt.character)</span><br><span class="line">    opt.num_class = len(converter.character)</span><br><span class="line"></span><br><span class="line">    if opt.rgb:</span><br><span class="line">        opt.input_channel = 3</span><br><span class="line">    model = Model(opt)</span><br><span class="line">    print(&apos;model input parameters&apos;, opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,</span><br><span class="line">          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,</span><br><span class="line">          opt.SequenceModeling, opt.Prediction)</span><br><span class="line">    model = torch.nn.DataParallel(model).to(device)</span><br><span class="line"></span><br><span class="line">    # load model</span><br><span class="line">    print(&apos;loading pretrained model from %s&apos; % opt.saved_model)</span><br><span class="line">    model.load_state_dict(torch.load(opt.saved_model, map_location=device))</span><br><span class="line"></span><br><span class="line">    # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo</span><br><span class="line">    AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)</span><br><span class="line">    # 加载数据目录</span><br><span class="line">    demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset</span><br><span class="line">    #</span><br><span class="line">    demo_loader = torch.utils.data.DataLoader(</span><br><span class="line">        demo_data, batch_size=opt.batch_size,</span><br><span class="line">        shuffle=False,</span><br><span class="line">        num_workers=int(opt.workers),</span><br><span class="line">        collate_fn=AlignCollate_demo, pin_memory=True)</span><br><span class="line"></span><br><span class="line">    # predict</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    with torch.no_grad():</span><br><span class="line"></span><br><span class="line">        # image_path_list为文件夹列表</span><br><span class="line">        for image_tensors, image_path_list in demo_loader:</span><br><span class="line"></span><br><span class="line">            batch_size = image_tensors.size(0)</span><br><span class="line"></span><br><span class="line">            image = image_tensors.to(device)</span><br><span class="line"></span><br><span class="line">            # For max length prediction</span><br><span class="line">            length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)</span><br><span class="line"></span><br><span class="line">            text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)</span><br><span class="line"></span><br><span class="line">            if &apos;CTC&apos; in opt.Prediction:</span><br><span class="line">                preds = model(image, text_for_pred)</span><br><span class="line"></span><br><span class="line">                # Select max probabilty (greedy decoding) then decode index to character</span><br><span class="line">                preds_size = torch.IntTensor([preds.size(1)] * batch_size)</span><br><span class="line">                _, preds_index = preds.max(2)</span><br><span class="line">                preds_index = preds_index.view(-1)</span><br><span class="line">                preds_str = converter.decode(preds_index.data, preds_size.data)</span><br><span class="line"></span><br><span class="line">            else:</span><br><span class="line">                preds = model(image, text_for_pred, is_train=False)</span><br><span class="line"></span><br><span class="line">                # select max probabilty (greedy decoding) then decode index to character</span><br><span class="line">                _, preds_index = preds.max(2)</span><br><span class="line"></span><br><span class="line">                preds_str = converter.decode(preds_index, length_for_pred)</span><br><span class="line"></span><br><span class="line">            log = open(f&apos;./log_demo_result.txt&apos;, &apos;a&apos;)</span><br><span class="line"></span><br><span class="line">            dashed_line = &apos;-&apos; * 80</span><br><span class="line"></span><br><span class="line">            head = f&apos;&#123;&quot;image_path&quot;:25s&#125;\t&#123;&quot;predicted_labels&quot;:25s&#125;\tconfidence score&apos;</span><br><span class="line"></span><br><span class="line">            print(f&apos;&#123;dashed_line&#125;\n&#123;head&#125;\n&#123;dashed_line&#125;&apos;)</span><br><span class="line"></span><br><span class="line">            log.write(f&apos;&#123;dashed_line&#125;\n&#123;head&#125;\n&#123;dashed_line&#125;\n&apos;)</span><br><span class="line"></span><br><span class="line">            preds_prob = F.softmax(preds, dim=2)</span><br><span class="line"></span><br><span class="line">            preds_max_prob, _ = preds_prob.max(dim=2)</span><br><span class="line"></span><br><span class="line">            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):</span><br><span class="line"></span><br><span class="line">                if &apos;Attn&apos; in opt.Prediction:</span><br><span class="line">                    pred_EOS = pred.find(&apos;[s]&apos;)</span><br><span class="line"></span><br><span class="line">                    pred = pred[:pred_EOS]  # prune after &quot;end of sentence&quot; token ([s])</span><br><span class="line"></span><br><span class="line">                    pred_max_prob = pred_max_prob[:pred_EOS]</span><br><span class="line"></span><br><span class="line">                # calculate confidence score (= multiply of pred_max_prob)</span><br><span class="line">                confidence_score = pred_max_prob.cumprod(dim=0)[-1]</span><br><span class="line">                #</span><br><span class="line">                # print(f&apos;&#123;img_name:25s&#125;\t&#123;pred:25s&#125;\t&#123;confidence_score:0.4f&#125;&apos;)</span><br><span class="line">                # 拼接：文件名称:值</span><br><span class="line">                value = img_name + &quot;:&quot; + pred</span><br><span class="line">                valuetemp.append(value)</span><br><span class="line">                log.write(f&apos;&#123;img_name:25s&#125;\t&#123;pred:25s&#125;\t&#123;confidence_score:0.4f&#125;\n&apos;)</span><br><span class="line"></span><br><span class="line">            log.close()</span><br><span class="line"></span><br><span class="line">    # 遍历image_path_list</span><br><span class="line">    for i in image_path_list:</span><br><span class="line">        image_path = i.split(&apos;/&apos;)[1].split(&apos;_&apos;)[0]</span><br><span class="line">        # 开始处理临时储存</span><br><span class="line">        # 用于储存同一种的文件</span><br><span class="line">        onePicture = []</span><br><span class="line">        # 遍历添加</span><br><span class="line">        for v in valuetemp:</span><br><span class="line">            if v.split(&apos;/&apos;)[1].split(&apos;_&apos;)[0] == image_path:</span><br><span class="line">                # 得到一张图片的所有信息</span><br><span class="line">                onePicture.append(v)</span><br><span class="line">        # 根据.jpg的最后一个字符排序</span><br><span class="line">        onePicture.sort(key=lambda x: int(x.split(&apos;.&apos;)[0].split(&apos;_&apos;)[1]))</span><br><span class="line">        # 拼接每张图片</span><br><span class="line">        text = i.split(&apos;/&apos;)[0][:-3] + &apos;/&apos; + i.split(&apos;/&apos;)[1].split(&apos;_&apos;)[0] + &apos;.jpg&apos; + &apos;:&apos;</span><br><span class="line">        # 遍历onePicture</span><br><span class="line">        for o in onePicture:</span><br><span class="line">            text = text + o.split(&apos;:&apos;)[1] + &apos; &apos;</span><br><span class="line">        # 除去尾部空格</span><br><span class="line">        text = text.strip()</span><br><span class="line">        # 拼接完成之后添加</span><br><span class="line">        values.append(text)</span><br><span class="line">    return values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 识别主程序</span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(&apos;--image_folder&apos;, help=&apos;path to image_folder which contains text images&apos;)</span><br><span class="line">    parser.add_argument(&apos;--workers&apos;, type=int, help=&apos;number of data loading workers&apos;, default=4)</span><br><span class="line">    parser.add_argument(&apos;--batch_size&apos;, type=int, default=192, help=&apos;input batch size&apos;)</span><br><span class="line">    parser.add_argument(&apos;--saved_model&apos;, help=&quot;path to saved_model to evaluation&quot;)</span><br><span class="line">    &quot;&quot;&quot; Data processing &quot;&quot;&quot;</span><br><span class="line">    parser.add_argument(&apos;--batch_max_length&apos;, type=int, default=25, help=&apos;maximum-label-length&apos;)</span><br><span class="line">    parser.add_argument(&apos;--imgH&apos;, type=int, default=32, help=&apos;the height of the input image&apos;)</span><br><span class="line">    parser.add_argument(&apos;--imgW&apos;, type=int, default=100, help=&apos;the width of the input image&apos;)</span><br><span class="line">    parser.add_argument(&apos;--rgb&apos;, action=&apos;store_true&apos;, help=&apos;use rgb input&apos;)</span><br><span class="line">    parser.add_argument(&apos;--character&apos;, type=str, default=&apos;0123456789abcdefghijklmnopqrstuvwxyz&apos;, help=&apos;character label&apos;)</span><br><span class="line">    parser.add_argument(&apos;--sensitive&apos;, action=&apos;store_true&apos;, help=&apos;for sensitive character mode&apos;)</span><br><span class="line">    parser.add_argument(&apos;--PAD&apos;, action=&apos;store_true&apos;, help=&apos;whether to keep ratio then pad for image resize&apos;)</span><br><span class="line">    &quot;&quot;&quot; Model Architecture &quot;&quot;&quot;</span><br><span class="line">    parser.add_argument(&apos;--Transformation&apos;, type=str, help=&apos;Transformation stage. None|TPS&apos;)</span><br><span class="line">    parser.add_argument(&apos;--FeatureExtraction&apos;, type=str, help=&apos;FeatureExtraction stage. VGG|RCNN|ResNet&apos;)</span><br><span class="line">    parser.add_argument(&apos;--SequenceModeling&apos;, type=str, help=&apos;SequenceModeling stage. None|BiLSTM&apos;)</span><br><span class="line">    parser.add_argument(&apos;--Prediction&apos;, type=str, help=&apos;Prediction stage. CTC|Attn&apos;)</span><br><span class="line">    parser.add_argument(&apos;--num_fiducial&apos;, type=int, default=20, help=&apos;number of fiducial points of TPS-STN&apos;)</span><br><span class="line">    parser.add_argument(&apos;--input_channel&apos;, type=int, default=1, help=&apos;the number of input channel of Feature extractor&apos;)</span><br><span class="line">    parser.add_argument(&apos;--output_channel&apos;, type=int, default=512,</span><br><span class="line">                        help=&apos;the number of output channel of Feature extractor&apos;)</span><br><span class="line">    parser.add_argument(&apos;--hidden_size&apos;, type=int, default=256, help=&apos;the size of the LSTM hidden state&apos;)</span><br><span class="line"></span><br><span class="line">    opt = parser.parse_args()</span><br><span class="line">    # 更改这里的位置</span><br><span class="line">    opt.image_folder = &apos;twoout/&apos;</span><br><span class="line">    &quot;&quot;&quot; vocab / character number configuration &quot;&quot;&quot;</span><br><span class="line">    if opt.sensitive:</span><br><span class="line">        opt.character = string.printable[:-6]  # same with ASTER setting (use 94 char).</span><br><span class="line"></span><br><span class="line">    cudnn.benchmark = True</span><br><span class="line">    cudnn.deterministic = True</span><br><span class="line">    opt.num_gpu = torch.cuda.device_count()</span><br><span class="line"></span><br><span class="line">    print(opt)</span><br><span class="line">    demo(opt)</span><br><span class="line"></span><br><span class="line">    values = demo(opt)</span><br><span class="line">    # 除去重复元素</span><br><span class="line">    values = list(set(values))</span><br><span class="line">    # 排序</span><br><span class="line">    values.sort(key=lambda x: int(x.split(&apos;/&apos;)[1].split(&apos;.&apos;)[0]))</span><br><span class="line">    for v in values:</span><br><span class="line">        print(v)</span><br></pre></td></tr></table></figure><p>5：paddle_ocr（微型模型，中英文混用，效果略优；通用模型也可以）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：优势在于部署的时候有微型模型，中英文混用，速度快。</span><br><span class="line">2：适用于对精度要求不是特别高的场合</span><br><span class="line">3：免费</span><br><span class="line">4：部署简单</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;OCR总结和对比；实现书本的题干提取&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td c
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>阿里云函数计算</title>
    <link href="https://yanyubing.xyz/2020/05/06/%E9%98%BF%E9%87%8C%E4%BA%91%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97/"/>
    <id>https://yanyubing.xyz/2020/05/06/%E9%98%BF%E9%87%8C%E4%BA%91%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97/</id>
    <published>2020-05-06T07:06:55.455Z</published>
    <updated>2020-05-07T03:26:05.197Z</updated>
    
    <content type="html"><![CDATA[<p>阿里云函数计算流程总结：基于flask框架的http触发器，完成模板的匹配功能</p><p>1：安装docker，设置开启自启</p><p>2：下载fun <a href="https://github.com/alibaba/funcraft/releases?spm=a2c4g.11186623.2.15.6a3a66een3mFgU" target="_blank" rel="noopener">https://github.com/alibaba/funcraft/releases?spm=a2c4g.11186623.2.15.6a3a66een3mFgU</a> </p><p>3： 在本地创建一个目录test(作为临时目录存放依赖),  然后终端进入到该目录下，把自己配置的yml文件放在该目录下</p><p>4：test目录下运行fun install init    初始化环境为python3，会出现funfile文件</p><p>5：在funfile文件中编写安装的依赖</p><p>6： 执行sudo fun install安装依赖 ，会在test目录下出现.fun文件(拉取镜像过程很慢)</p><p>7：因为torch无法使用funfile安装， 在本地重新创建一个目录，在该目录下执行fun install sbox –runtime python3  –interactive进入沙箱环境 </p><p>8： 执行pip install -t . torch 安装</p><p>9： 安装成功后，把安装的内容复制到项目的.fun/python/lib/python3.6/site-packages 目录下 )</p><p>10：更改flaskapp的入口函数，将test目录底下的所有内容复制放到项目根目录</p><p>11：执行fun deploy -y部署 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;阿里云函数计算流程总结：基于flask框架的http触发器，完成模板的匹配功能&lt;/p&gt;
&lt;p&gt;1：安装docker，设置开启自启&lt;/p&gt;
&lt;p&gt;2：下载fun &lt;a href=&quot;https://github.com/alibaba/funcraft/releases?spm
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>python_gui</title>
    <link href="https://yanyubing.xyz/2020/05/01/python_gui/"/>
    <id>https://yanyubing.xyz/2020/05/01/python_gui/</id>
    <published>2020-05-01T15:31:31.016Z</published>
    <updated>2020-05-05T07:46:43.407Z</updated>
    
    <content type="html"><![CDATA[<h3 id="PythonGUI-Tkinter"><a href="#PythonGUI-Tkinter" class="headerlink" title="PythonGUI-Tkinter"></a>PythonGUI-Tkinter</h3><p>为了做出可以提供给其他人使用的AI(CV方向)算法程序—实现切割纸张可视化（或者是制作label）</p><p>1：实现图片的切割，储存，检查，目录的创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line"># Radio Buttons:单选框,创建多个单选框</span><br><span class="line">import os</span><br><span class="line">import time</span><br><span class="line">from tkinter import *</span><br><span class="line"></span><br><span class="line"># 初始化窗口</span><br><span class="line">import cv2</span><br><span class="line">from PIL import ImageTk, Image</span><br><span class="line"></span><br><span class="line">root = Tk()</span><br><span class="line">root.title(&apos;GUI_cutPaper&apos;)</span><br><span class="line"></span><br><span class="line"># 创建目录</span><br><span class="line">Label(root, text=&apos;输入要创建的目录路径，如：‘勤学早/Unit1/第一课时/第一大题’&apos;).grid(row=1, column=0)</span><br><span class="line">e_Dir = Entry(root)</span><br><span class="line">e_Dir.grid(row=2, column=0)</span><br><span class="line"></span><br><span class="line"># 输入图片，用来切割</span><br><span class="line">Label(root, text=&apos;输入要切割的图片路径，如：‘book/1.jpg’&apos;).grid(row=3, column=0)</span><br><span class="line">e_book = Entry(root)</span><br><span class="line">e_book.grid(row=4, column=0)</span><br><span class="line"></span><br><span class="line"># 保存图片</span><br><span class="line">Label(root, text=&apos;输入图片的保存路径,如：‘勤学早/Unit1/第一课时/第一大题/1.jpg’&apos;).grid(row=5, column=0)</span><br><span class="line">e_saveImage = Entry(root)</span><br><span class="line">e_saveImage.grid(row=6, column=0)</span><br><span class="line"></span><br><span class="line"># 检测图片</span><br><span class="line">Label(root, text=&apos;输入要检测的目录路径，如：‘勤学早’&apos;).grid(row=7, column=0)</span><br><span class="line">e_check = Entry(root)</span><br><span class="line">e_check.grid(row=8, column=0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 鼠标事件，获取需要切割点的y坐标</span><br><span class="line">def on_EVENT_LBUTTONDOWN(event, x, y, flags, param):</span><br><span class="line">    if event == cv2.EVENT_LBUTTONDOWN:</span><br><span class="line">        xy = &quot;%d,%d&quot; % (x, y)</span><br><span class="line">        print(xy)</span><br><span class="line">        # cv2.circle(img, (x, y), 1, (255, 0, 0), thickness=-1)</span><br><span class="line">        # cv2.putText(img, xy, (x, y), cv2.FONT_HERSHEY_PLAIN,</span><br><span class="line">        #             1.0, (0, 0, 0), thickness=1)</span><br><span class="line">        dotsY.append(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建目录</span><br><span class="line">def createDir():</span><br><span class="line">    path = e_Dir.get()</span><br><span class="line">    if os.path.exists(path):</span><br><span class="line">        # 提示信息</span><br><span class="line">        Label(root, text=&apos;文件路径已存在，请重新输入&apos;).grid(row=0, column=3)</span><br><span class="line">    else:</span><br><span class="line">        os.makedirs(path)</span><br><span class="line">        Label(root, text=path + &apos; ：创建成功&apos;).grid(row=0, column=3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 存储切割的图片</span><br><span class="line">def saveImage():</span><br><span class="line">    # 遍历所有的file进行切割</span><br><span class="line">    global dotsY</span><br><span class="line">    dotsY = []</span><br><span class="line">    file_path_in = e_book.get()</span><br><span class="line">    global img</span><br><span class="line"></span><br><span class="line">    if not os.path.exists(file_path_in):</span><br><span class="line">        Label(root, text=file_path_in + &apos;输入的文件路径不存在，请重新输入&apos;).grid(row=0, column=3)</span><br><span class="line">    else:</span><br><span class="line">        img = cv2.imread(file_path_in)</span><br><span class="line">        cv2.namedWindow(&quot;image&quot;, 0)</span><br><span class="line">        cv2.resizeWindow(&apos;image&apos;, 600, 800)</span><br><span class="line">        cv2.imshow(&apos;image&apos;, img)</span><br><span class="line">        cv2.setMouseCallback(&quot;image&quot;, on_EVENT_LBUTTONDOWN)</span><br><span class="line">        cv2.waitKey(0)</span><br><span class="line">        cv2.destroyAllWindows()</span><br><span class="line">    print(&apos;得到的y坐标点为&apos;, dotsY)</span><br><span class="line">    file_path_out = e_saveImage.get()</span><br><span class="line">    if os.path.exists(file_path_out):</span><br><span class="line">        Label(root, text=file_path_in + &apos;输出的文件路径已经存在，请检查&apos;).grid(row=0, column=3)</span><br><span class="line">        return</span><br><span class="line">    image = img[dotsY[0]:dotsY[1], :]</span><br><span class="line">    # 处理带有中文的目录结构</span><br><span class="line">    cv2.imencode(&apos;.jpg&apos;, image)[1].tofile(file_path_out)</span><br><span class="line">    Label(root, text=file_path_out + &apos;文件保存成功&apos;).grid(row=0, column=3)</span><br><span class="line">    time.sleep(1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">index = 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取所有的文件名</span><br><span class="line">def listdir(path, list_name):</span><br><span class="line">    for file in os.listdir(path):</span><br><span class="line">        file_path = os.path.join(path, file)</span><br><span class="line">        if os.path.isdir(file_path):</span><br><span class="line">            listdir(file_path, list_name)</span><br><span class="line">        elif os.path.splitext(file_path)[1] == &apos;.jpg&apos;:</span><br><span class="line">            list_name.append(file_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 储存列表名</span><br><span class="line">listName = []</span><br><span class="line"># 储存所有转换之后的图片</span><br><span class="line">my_images = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 检查图片</span><br><span class="line">def checkImage():</span><br><span class="line">    # 前进，后退，退出按钮</span><br><span class="line">    button_back = Button(root, text=&apos;&lt;&lt;&apos;, command=back)</span><br><span class="line">    button_quit = Button(root, text=&apos;Exit&apos;, command=root.quit)</span><br><span class="line">    button_forward = Button(root, text=&apos;&gt;&gt;&apos;, command=forward)</span><br><span class="line">    button_back.grid(row=11, column=2)</span><br><span class="line">    button_quit.grid(row=11, column=3)</span><br><span class="line">    button_forward.grid(row=11, column=4)</span><br><span class="line"></span><br><span class="line">    global listName</span><br><span class="line"></span><br><span class="line">    path = e_check.get()</span><br><span class="line">    listdir(path, listName)</span><br><span class="line">    print(len(listName))</span><br><span class="line">    global my_images</span><br><span class="line"></span><br><span class="line">    for i in range(len(listName)):</span><br><span class="line">        # 打开图片，图片放入对象，对象再放入screen</span><br><span class="line">        my_img = ImageTk.PhotoImage(Image.open(listName[i]).resize((600, 300)))</span><br><span class="line">        my_images.append(my_img)</span><br><span class="line">    Label(root, image=my_images[0]).grid(row=10, column=3)</span><br><span class="line">    mainloop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def showImage(index):</span><br><span class="line">    my_label = Label(root, image=my_images[index])</span><br><span class="line">    my_label.grid(row=10, column=3)</span><br><span class="line"></span><br><span class="line">    message = &apos;路径:&apos; + listName[index] + &apos;  Image &apos; + str(index + 1) + &apos; of&apos; + str(len(listName))</span><br><span class="line">    # 添加状态栏(多少张图片，多少个)</span><br><span class="line"></span><br><span class="line">    Label(root, text=message).grid(row=9, column=3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def back():</span><br><span class="line">    # 申明全局变量</span><br><span class="line">    global index</span><br><span class="line">    index = (index - 1) % len(listName)</span><br><span class="line">    showImage(index)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def forward():</span><br><span class="line">    # 申明全局变量</span><br><span class="line">    global index</span><br><span class="line">    index = (index + 1) % len(listName)</span><br><span class="line">    showImage(index)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 三大组件位置</span><br><span class="line">button_CreateFile = Button(root, text=&quot;createDir&quot;, command=createDir)</span><br><span class="line">button_saveImage = Button(root, text=&quot;saveImage&quot;, command=saveImage)</span><br><span class="line">button_CheckImage = Button(root, text=&quot;checkImage&quot;, command=checkImage)</span><br><span class="line"></span><br><span class="line">button_CreateFile.grid(row=15, column=15, padx=2)</span><br><span class="line">button_saveImage.grid(row=16, column=15, padx=2)</span><br><span class="line">button_CheckImage.grid(row=17, column=15, padx=2)</span><br><span class="line"></span><br><span class="line">Label(root, text=&apos;使用步骤：&apos;).grid(row=12, column=0)</span><br><span class="line">Label(root, text=&apos;1、创建目录:输入需要创建的目录，点击createDir&apos;).grid(row=13, column=0)</span><br><span class="line">Label(root, text=&apos;2、切割图片:&apos;).grid(row=14, column=0)</span><br><span class="line">Label(root, text=&apos;①输入要切割的输入图片路径，以.jpg结尾&apos;).grid(row=15, column=0)</span><br><span class="line">Label(root, text=&apos;②输入需要保存的路径，以.jpg结尾&apos;).grid(row=16, column=0)</span><br><span class="line">Label(root, text=&apos;③点击saveImage进行切割操作&apos;).grid(row=17, column=0)</span><br><span class="line">Label(root, text=&apos;3、检测图片：输入需要检测图片的根目录，点击checkImage&apos;).grid(row=18, column=0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root.mainloop()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;PythonGUI-Tkinter&quot;&gt;&lt;a href=&quot;#PythonGUI-Tkinter&quot; class=&quot;headerlink&quot; title=&quot;PythonGUI-Tkinter&quot;&gt;&lt;/a&gt;PythonGUI-Tkinter&lt;/h3&gt;&lt;p&gt;为了做出可以提供给其
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>神经网络学习记录（已掌握）</title>
    <link href="https://yanyubing.xyz/2020/04/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E5%B7%B2%E6%8E%8C%E6%8F%A1%EF%BC%89/"/>
    <id>https://yanyubing.xyz/2020/04/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E5%B7%B2%E6%8E%8C%E6%8F%A1%EF%BC%89/</id>
    <published>2020-04-19T17:02:14.752Z</published>
    <updated>2020-05-15T17:20:51.864Z</updated>
    
    <content type="html"><![CDATA[<p>1：神经元</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">构建：包含初始化[w]，b；激活函数，前馈网络</span><br><span class="line"></span><br><span class="line">完成inputs→outputs的功能</span><br></pre></td></tr></table></figure><p>2：神经网络</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">构建：多个神经元构成，初始化[w],[b]；前馈网络</span><br><span class="line"></span><br><span class="line">完成inputs→outputs的功能</span><br></pre></td></tr></table></figure><p>3：神经网络的训练</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">构建：定义损失函数，随机初始化[w],[b];前向传播;训练方法（通过随机梯度下降更新权重和偏置的过程，包含学习率，偏导，和批次数）;每10个epoch打印一次准确率或者损失值等，方便可视化</span><br></pre></td></tr></table></figure><p>4：神经网络的预测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">就是神经网络的前向传播</span><br></pre></td></tr></table></figure><p>5：计算机视觉发展史</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如何让计算机理解视觉(圆柱表示法,直线表示法)→人脸识别（2000年）→SIFT（对象识别，基于特征（因为匹配整个物体非常困难（有很多干扰因素））））→图像金字塔，不同的分辨率带有同样的特征（2006）→梯度直方图（HoG，用来描述特征，表示向量机）→（2006-2012数据集的发展）开始对象识别（发展基于标准数据集的产生,PASCAL;imageNet(最大的数据集项目)）→大赛中卷积神经网络展现身手（2012年，使得错误率下降了10%，前五类识别）</span><br></pre></td></tr></table></figure><p>6：数据增强</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1：文字扭曲（贝塞尔曲线实现）</span><br><span class="line">2：背景噪声（椒盐）</span><br><span class="line">椒盐噪声 = 椒噪声 + 盐噪声 ，椒盐噪声的值为0(黑色)或者255(白色)</span><br><span class="line">3：笔画粘连（膨胀）</span><br><span class="line">4：笔画断裂（腐蚀）</span><br><span class="line">5：风格迁移</span><br><span class="line">6：ImageEnhance（增强亮度，色泽等）</span><br><span class="line">7：滤镜</span><br><span class="line">8：加下划线（适用于下划线上面是答案的情况）</span><br><span class="line">9：底色加数字（适用于答案会写在数字上的情况）</span><br><span class="line">10：左右镜像</span><br><span class="line">11：随机裁剪</span><br><span class="line">12：随机擦除</span><br><span class="line">13:旋转</span><br><span class="line">14：剪切（shearing）</span><br><span class="line">15：局部扭曲变换（Local warping）</span><br><span class="line">16：color shifting(调整rgb的值，保持失真)</span><br><span class="line">17:PCA主成分分析（颜色增强），使得RGB相对一致</span><br><span class="line"></span><br><span class="line">一般不做本地数据增强，在代码中动态增强，数据增强过程中使用的到参数，也可以作为超参数来调整</span><br></pre></td></tr></table></figure><p>7：物体识别的问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：物体相机位置不一样，像素不一样</span><br><span class="line">2：物体光照，阴影等不一样</span><br><span class="line">3：物体只有部分可视</span><br><span class="line">4：背景和物体颜色类似</span><br><span class="line">5：物体的品种不一样（猫）</span><br></pre></td></tr></table></figure><p>8：数据驱动方法(Data-driver method)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1:收集数据</span><br><span class="line">2:训练模型</span><br><span class="line">3：使用模型预测</span><br><span class="line"></span><br><span class="line">def train(images,labels):</span><br><span class="line">#meching learning</span><br><span class="line">return model</span><br><span class="line"></span><br><span class="line">def predict(test_image,model):</span><br><span class="line">return test_labels</span><br></pre></td></tr></table></figure><p>9：第一个分类器(nearest neighbor)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据集使用CIFAR10</span><br><span class="line">原理就是对图片进行对比</span><br><span class="line">两个图片的对比第一种方式：L1曼哈顿距离(围成了矩形，会随着坐标旋转改变)</span><br><span class="line">sum(|Test[:]-train[:]|)，两个对应坐标像素的绝对值的和</span><br><span class="line">两个图片的对比第二种方式：L2欧几里得距离（围成的是圆，坐标不变）</span><br><span class="line">sum((Test[:]-train[:])的平方开根号）</span><br><span class="line"></span><br><span class="line">对于knn，你需要决策的是k取值和距离的取值，距离的标识L1或者L2（超参数）</span><br></pre></td></tr></table></figure><p>10：对于超参数的选择</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对于knn：</span><br><span class="line">K=1的时候训练更好（错误，需要多调整）</span><br><span class="line">超参数在测试集上更好就选择（错误，需要均衡）</span><br><span class="line">数据分为训练集，验证集和测试集：在验证集上选择超参数，测试集上测试（？暂时不理解）</span><br><span class="line">交叉验证是一个很好的设置超参数方式(但是不适合大型数据集，时间昂贵)</span><br></pre></td></tr></table></figure><p>11:SVM损失函数（Loss function）hinge loss(铰接损失)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">在训练集上，通过得分来衡量不准确率。对于不同给定的权值w，来衡量不同w的优劣。</span><br><span class="line">label:标签，用来定义输出类型,数据类型为Interage</span><br><span class="line">score</span><br><span class="line">label   True cat  carfrog</span><br><span class="line">cat 3.21.32.2</span><br><span class="line">car5.14.92.5</span><br><span class="line">frog-1.72.0-3.1</span><br><span class="line"></span><br><span class="line">(每一类：如frog)Loss=max(2.2-(-3.1)+1,0)+max(2.5-(-3.1)+1,0)（True得分减去其他类得分&gt;1则置0，小于0取实际值）</span><br><span class="line">Li=∑max(Sj-syi+1,0)(j≠i)</span><br><span class="line">(Sj为预测的类别得分，Syi为预测为实际类别的得分):目的是让我们通过权值w得到的准确分类，真实类别的得分要比错误类别的得分大于1;继而损失函数=0的时候，刚好等于1。</span><br><span class="line">平均损失：sum/N</span><br><span class="line"></span><br><span class="line">1:如果car的得分改变一点，对损失函数是否有影响？</span><br><span class="line">没有影响，因为car的分数已经超过了其他的分数，改变一点之后，损失函数还是0</span><br><span class="line"></span><br><span class="line">2:损失函数的取值范围</span><br><span class="line">0~+∞</span><br><span class="line"></span><br><span class="line">3:如果使用的是平方损失？意义是什么</span><br><span class="line">平方损失的意义在于，我们改变不同的w，看看到底那个地方对于损失的影响更大</span><br><span class="line"></span><br><span class="line">4:这种损失函数在numpy中的表示</span><br><span class="line"></span><br><span class="line">5:得到损失函数为0的权值并不唯一</span><br><span class="line">w得到的损失为0，那么2W同样得到损失函数也为0</span><br><span class="line"></span><br><span class="line">6:我们并不关心在训练集上的损失函数，而是在测试集上面的分类准确情况</span><br><span class="line"></span><br><span class="line">7:hinge loss(铰接损失)有时候又被称为最大边界损失（max-margin loss）</span><br></pre></td></tr></table></figure><p>12:Regularization(损失函数的正则化)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">正则化是为了使得事情变得简单化:</span><br><span class="line">L1 regularization</span><br><span class="line">L2 regularization</span><br><span class="line">Dropout regularization:常用,防止过拟合，可以区别共同特征;但是会增加训练时间，因为每次会丢弃一部分的权重更新</span><br><span class="line">Max-Norm Regularization</span><br><span class="line">随机深度：不属于对损失函数的正则化，（对于很深的网络），思路是在训练的时候舍弃一些层，但是在测试的时候使用原样，效果好</span><br><span class="line">等等...</span><br><span class="line">都是通过调整损失函数来实现正则化的</span><br></pre></td></tr></table></figure><p>13:softmax classifier</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">score取值范围调整为0-1,表达式为:np.exp(a) / np.sum(np.exp(a)) </span><br><span class="line">强调了最大得分，忽略了比较小的得分（通常情况下，并非标量不变）:</span><br><span class="line">例如：</span><br><span class="line">得分为：[1、2、3、4、1、2、3]</span><br><span class="line">softmax之后为：[0.024、0.064、0.175、0.475、0.024、0.064、0.175]</span><br><span class="line">最高值的比例为4/16&lt;0.475</span><br><span class="line"></span><br><span class="line">但是有时候会出现如下情况</span><br><span class="line">得分为：[0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3] </span><br><span class="line">softmax之后为：[0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153]</span><br><span class="line">最大值0.4/1.6&gt;0.169</span><br></pre></td></tr></table></figure><p>14:softMax的lossfunction</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sofeMax的lossfunction:-logSj   </span><br><span class="line">(sj为softmax之后对应种类的概率)，</span><br><span class="line">例如样本为[0,0,0,0,0,0,1]</span><br><span class="line">softmax之后的概率为[0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153]</span><br><span class="line">则对于该次预测的loss值为-log(0.153)，Sj越大，证明预测性能更好，对应的loss越低，最低趋近于0</span><br><span class="line"></span><br><span class="line">1:lossfunction的取值</span><br><span class="line">lossfunction的取值为0-+∞</span><br><span class="line"></span><br><span class="line">2:初始化权值都很小的时候，lossfunction的值</span><br><span class="line">-log(1/c)  c为类别的个数</span><br><span class="line"></span><br><span class="line">3:所以如果你的概率是通过softmax公式得到的，那么cross entropy就是softmax loss</span><br></pre></td></tr></table></figure><p>15：SVM loss  vs  Softmax loss</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">常用损失函数</span><br><span class="line">常见的损失误差有五种：</span><br><span class="line">1. 铰链损失（Hinge Loss）：主要用于支持向量机（SVM） 中；</span><br><span class="line">2. 互熵损失 （Cross Entropy Loss，Softmax Loss ）：用于Logistic 回归与Softmax 分类中；</span><br><span class="line">3. 平方损失（Square Loss）：主要是最小二乘法（OLS）中；</span><br><span class="line">4. 指数损失（Exponential Loss） ：主要用于Adaboost 集成学习算法中；</span><br><span class="line">5. 其他损失（如0-1损失，绝对值损失）</span><br></pre></td></tr></table></figure><p>16:优化器（optimization）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">方案1:随机搜索,随机化权重，使得损失函数最小（实现困难，并且耗时巨大，准确率低）</span><br><span class="line"></span><br><span class="line">方案2:梯度（但是太过于缓慢，如果w的维度为1000万）</span><br><span class="line">十分类问题，w为1*10的矩阵，loss在固定的W为固定值，那么每一个w（其他w值固定不变）增加为W+h(first dim)，得到对应的loss，∆loss/∆w就为对应的梯度，处理10次就得到每一个的梯度。</span><br><span class="line"></span><br><span class="line">方案3:Calculus（微积分）</span><br><span class="line">直接就可以得到斜率（不需要每个增加微小值h(0.001)）</span><br><span class="line"></span><br><span class="line">总结：使用分析梯度下降（快，但是有错误情况），结合数值梯度来进行检查，称为gradient check(梯度检查,debug可以用到)</span><br><span class="line"></span><br><span class="line">4:step_size(leraning rate)作为超参数</span><br><span class="line"></span><br><span class="line">5:SGD(随机梯度下降)</span><br><span class="line">我们处理图片训练的时候，可能参数有数百万个，随机梯度下降解决的问题是不需要进行一整个循环来得到梯度，而是每个minibatch(32,64,128)就可以得到梯度;代码上多增加了一行采样训练数据</span><br><span class="line"></span><br><span class="line">6:更加优秀的梯度下降</span><br><span class="line">Adaptive Learning Algorithms(自适应学习率算法):Adagrad，Adadelta，RMSprop，Adam，优于SGD（随机梯度下降）算法</span><br><span class="line"></span><br><span class="line">7：具有动量的SGD可以越过局部最小值和鞍点，一般使用Adam</span><br></pre></td></tr></table></figure><p>17:线性分类svm，损失函数的直观了解</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/</span><br></pre></td></tr></table></figure><p>18:反向传播</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">f(x,y,z)=(x+y)*z</span><br><span class="line">e.g. x=-2 y=5 z=-4</span><br><span class="line"></span><br><span class="line">表达式分解为：</span><br><span class="line">q=x+y</span><br><span class="line">f=q*z</span><br><span class="line"></span><br><span class="line">反向传播的导数遵循链式法则：</span><br><span class="line">∂q/∂x=1</span><br><span class="line">∂q/∂y=1</span><br><span class="line"></span><br><span class="line">∂f/∂q=z</span><br><span class="line">∂f/∂z=q</span><br><span class="line"></span><br><span class="line">即∂f/∂x=∂f/∂q * ∂q/∂x=z*1=z=-4</span><br></pre></td></tr></table></figure><p>19:Patterns in backward flow(反向传播的模式)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">add gate(加法):gradient distributor(梯度分布:两个值梯度一样)</span><br><span class="line">max gate(取最大值):gradient router(梯度路由:其中一个值的梯度置0)</span><br><span class="line">mul gate(乘法):gradient switcher(梯度切换:梯度分别为另一个值)</span><br><span class="line"></span><br><span class="line">Gradient add at branches：分支的时候梯度为求分支求和</span><br><span class="line"></span><br><span class="line">向量计算的梯度：</span><br><span class="line">向量中元素的梯度总是一样的，</span><br></pre></td></tr></table></figure><p>20:Modeularized implementation(模块化)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">框架中：caffe torch TensorFlow 都是在实现正向和方向传播</span><br></pre></td></tr></table></figure><p>21:Convolutional Neural Networks(CNN)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">全连接层：input为32*32*3的图片</span><br><span class="line">那么input展开就为3072*1的矩阵；需要的w就为10*3072的矩阵，最终经过激活函数(可以是sigmod)，得到10个种类的得分</span><br><span class="line">需要的参数为30720个</span><br><span class="line"></span><br><span class="line">卷积层:input是32*32*3的图片</span><br><span class="line">保留原结构，经过5*5*3的卷积，进行运算，得到28*28的特征图;如果有6个过滤器，则可以得到6个特征图，合起来就是28*28*6；再经过10个5*5*6的过滤器，得到10个24*24的特征图</span><br><span class="line"></span><br><span class="line">输出层的大小=（N-F）/stride+1 </span><br><span class="line">N:输入层的大小</span><br><span class="line">F:过滤器的大小</span><br><span class="line">stride：步长</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">输出层的大小=（N+2p-F）/stride+1 </span><br><span class="line">pad:填充，一般使用0像素填充(为了保持输出图像的大小，再深层网络中多个卷积下来之后图片大小会急剧下降)</span><br><span class="line"></span><br><span class="line">练习：input：32*32*3  10个5*5的过滤器 步长为1  pad为2 输出是？参数的个数是？</span><br><span class="line">(32+4-5+1)/1的10个，最终为32*32*10;</span><br><span class="line">参数的个数为（5*5*3+1）*10=760个，每个过滤器都会加一个偏置项Bias</span><br><span class="line"></span><br><span class="line">过滤器的数量一般为2的次方个,32,64,128...</span><br><span class="line"></span><br><span class="line">池化层：也叫下采样层(减小特征图的大小),一般采用最大池化,池化的步长一般也和过滤器大小一致，保证不会有重叠区域被池化；目的是进行下采样</span><br><span class="line"></span><br><span class="line">直观演示地址：</span><br><span class="line">https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html</span><br></pre></td></tr></table></figure><p>22:激活函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">激活函数一般用在求出了h=w1*x1+w2*x2+b，之后f(h)</span><br><span class="line">有下面的种类f(x)：</span><br><span class="line"></span><br><span class="line">Sigmoid:取值为0~1</span><br><span class="line">问题点:</span><br><span class="line">1：当x=10,或者x=-10的时候，d(f(x))/d(x)梯度≈0；则d(f(x))/d(w)也≈0，会产生梯度消失，不利于参数更新</span><br><span class="line">2：对于向量w而言；输入x为正，梯度始终为正；或者输入x为负，梯度始终为负；这意味着我们需要使用的输入数据是均值为0</span><br><span class="line">3:Sigmoid计算昂贵（并不是很严重的问题）</span><br><span class="line"></span><br><span class="line">tanh:取值为-1~1</span><br><span class="line">问题点：</span><br><span class="line">解决了Sigmoid的第二个问题,但是还是存在第一个问题</span><br><span class="line"></span><br><span class="line">ReLU:取值0~正无穷(第一次使用是在2012 AlexNet)</span><br><span class="line">1：存在负值梯度为0的情况</span><br><span class="line">2：收敛较快（大概是sigmoid的6倍）</span><br><span class="line">3：初始化weight可能存在永远都不更新w的情况，对于所有的输入（w1*x1+w2*x2+b&lt;0），使得f(x)/d(w)=0;部分神经元死掉（不更新，不起作用）</span><br><span class="line"></span><br><span class="line">Leaky ReLu:取值-无穷~正无穷</span><br><span class="line">max(0.01x,x),解决了h的取值为负数，梯度为0的情况</span><br><span class="line"></span><br><span class="line">PRelu:max(αx,x)</span><br><span class="line">α通过超参数给定，更加灵活</span><br><span class="line"></span><br><span class="line">ELU：Rlue的另一个变种</span><br><span class="line"></span><br><span class="line">Maxout:max(w1Tx+b1,w2Tx+b2)</span><br><span class="line">1：需要两组权值</span><br><span class="line"></span><br><span class="line">使用激活函数的经验：</span><br><span class="line">1:不使用sigmoid</span><br><span class="line">2:是要ReLU,注意学习率</span><br><span class="line">3:尝试ReLU的变种</span><br><span class="line">4:尝试tanh（不做期待）</span><br></pre></td></tr></table></figure><p>23:数据预处理（data Preprocess）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1:zero-mean(0均值):</span><br><span class="line">I=D-Imean</span><br><span class="line">Imean可以是整个图片的均值（3通道）AlexNet，也可以是（每个通道的均值）vgg网络中是这样的</span><br><span class="line"></span><br><span class="line">2:Normalized data:归一化(CNN不常见)</span><br><span class="line">I = Imin + (Imax-Imin)*(D-Dmin)/(Dmax-Dmin)</span><br><span class="line"></span><br><span class="line">3:PCA和白化(CNN不常见)</span><br><span class="line"></span><br><span class="line">注意点：</span><br><span class="line">1:正确做法是计算训练数据的均值，然后分别把它从训练/验证/测试数据中减去。</span><br><span class="line">2:数据预处理不能解决sigmoid的劣势（梯度消失），因为预处理只能处理第一层的输入数据结构</span><br><span class="line">3:数据预处理可以使得数据对损失不太敏感，更好的处理;偏离中心的数据移动到中心</span><br></pre></td></tr></table></figure><p>24：（权值初始化）weight Initalization</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">问题</span><br><span class="line">1：初始化W=0会出现什么?</span><br><span class="line">大部分神经元的梯度相同，做了一样的事情，激活函数一样的时候,w=0，则得到的损失函数一致，反向梯度也一致，即会用同样的方式更新。</span><br><span class="line"></span><br><span class="line">2：W=0.01*np.random.randn(D,N)？</span><br><span class="line">均值为0,标准差为0.01;网络比较小的时候可行，深层网络不可行;网络的深入，所有的梯度都会变为0</span><br><span class="line"></span><br><span class="line">3:W=np.random.randn(fan_in.fan_out)</span><br><span class="line">标准差为1，所有神经元都将处于饱和状态，梯度消失</span><br><span class="line"></span><br><span class="line">4:Xavier初始化(最常用)</span><br><span class="line">W=np.random.randn(fan_in,fan_out)/np.sqrt(fan_in)</span><br><span class="line"></span><br><span class="line">5:note additional/2</span><br><span class="line">W=np.random.randn(fan_in,fan_out)/np.sqrt(fan_in/2)</span><br><span class="line"></span><br><span class="line">6:batch Normalization（批处理归一化）</span><br><span class="line">BN算法在Mini-batch中使用，一般在全连接层或者卷积层之后使用，或者在非线性层之前</span><br></pre></td></tr></table></figure><p>25：怎么做迁移学习</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">以分类问题而言：</span><br><span class="line">一：你的训练集小，类别少</span><br><span class="line">①你要做的是5分类问题</span><br><span class="line">②下载github上面imageNet网络和对应的权重</span><br><span class="line">③思路一：修改最后一层的softmax层(对应自己的输出类别)，同时冻结前面的权重，只需要训练最后一层的权重</span><br><span class="line">④思路二：把输入图片直到最后一层前的激活值储存到硬盘中，然后每次只需要训练一个一层网络即可(读取激活值进行softmax)</span><br><span class="line"></span><br><span class="line">经验而言：你的训练集越大，你需要冻结的层数减少。足够大的时候，可能不需要冻结任何一层</span><br></pre></td></tr></table></figure><p>26：GPU和CPU的比较</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GPU：内核多，适合做并行运算，例如矩阵</span><br><span class="line">CPU：内核少</span><br><span class="line"></span><br><span class="line">openCL使用英伟达或者AMD的GPU，或者可以在CPU上跑，但是整体而言不行</span><br></pre></td></tr></table></figure><p>17：深度学习框架</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">caffe caffe2:Facebook</span><br><span class="line">PyTorch:Facebook</span><br><span class="line">TensorFlow:Google</span><br><span class="line">Paddle:Baidu</span><br><span class="line">CNTK:Microsoft</span><br><span class="line">MXNet:(Amazon)</span><br><span class="line"></span><br><span class="line">框架的优势：</span><br><span class="line">①简单的构建大型的计算图</span><br><span class="line">②自动计算梯度</span><br><span class="line">③有效的再GPU上运行(numpy无法在GPU上运行)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1：神经元&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;b
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>神经网络相关（卷积）</title>
    <link href="https://yanyubing.xyz/2020/04/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%EF%BC%88%E5%8D%B7%E7%A7%AF%EF%BC%89/"/>
    <id>https://yanyubing.xyz/2020/04/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%EF%BC%88%E5%8D%B7%E7%A7%AF%EF%BC%89/</id>
    <published>2020-04-17T08:06:22.003Z</published>
    <updated>2020-04-17T16:06:09.756Z</updated>
    
    <content type="html"><![CDATA[<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p>1：卷积神经网络的发展</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LeNet(1998年)→AlexNet(2012年)→ZFNet(2013年)→VGG（2014年-2月）→GoogleNet(2014年-1月)→ResNet(2015年)→SENet(2017年)→DenseNet（2018年）→efficientnet（2019年）</span><br></pre></td></tr></table></figure><p>2：相关网络介绍</p><p>2.1:LeNet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1:网络结构</span><br><span class="line">LeNet-5包含七层，不包括输入，每一层都包含可训练参数（权重），当时使用的输入数据是32*32像素的图像。卷积层Cx，子采样层Sx，完全连接层Fx，其中x是层索引。输入图像大小是32*32</span><br><span class="line"></span><br><span class="line">第1层C1：具有6个5*5卷积核的卷积层，特征映射的大小为28*28（32+1-5），C1包含156个可训练参数和122304个连接。</span><br><span class="line"></span><br><span class="line">第2层S2：6个大小为14*14的特征图的子采样层（subsampling/pooling）。每个特征地图中的每个单元连接到C1中的对应特征地图中的2*2个邻域。S2中单位的四个输入相加，然后乘以可训练系数（权重），然后加到可训练偏差（bias）。结果通过S形函数传递。由于2*2个感受域不重叠，因此S2中的特征图只有C1中的特征图的一半行数和列数。S2层有12个可训练参数和5880个连接。</span><br><span class="line"></span><br><span class="line">第3层C3：具有16个5-5的卷积核的卷积层。前六个C3特征图的输入是S2中的三个特征图的每个连续子集，接下来的六个特征图的输入则来自四个连续子集的输入，接下来的三个特征图的输入来自不连续的四个子集。最后，最后一个特征图的输入来自S2所有特征图。C3层有1516个可训练参数和156000个连接。</span><br><span class="line"></span><br><span class="line">第4层S4：与S2类似，大小为2*2，输出为16个5*5的特征图。S4层有32个可训练参数和2000个连接。</span><br><span class="line"></span><br><span class="line">第5层C5：是具有120个大小为5*5的卷积核的卷积层。每个单元连接到S4的所有16个特征图上的5*5邻域。这里，因为S4的特征图大小也是5*5，所以C5的输出大小是1*1。因此S4和C5之间是完全连接的。C5被标记为卷积层，而不是完全连接的层，是因为如果LeNet-5输入变得更大而其结构保持不变，则其输出大小会大于1*1，即不是完全连接的层了。C5层有48120个可训练连接。</span><br><span class="line"></span><br><span class="line">第6层F6：完全连接到C5，输出84张特征图。它有10164个可训练参数。这里84与输出层的设计有关。</span><br><span class="line"></span><br><span class="line">2:网络结构流程总结</span><br><span class="line">输入32*32→（C1:6个5*5卷积核）→6个28*28的特征图→（S2子采样层）→6个14*14的特征图→（c3:16个5*5的卷积核）→16个10*10的特征图→（s4子采样层）→16个5*5的特征图→（C5:120个5*5的卷积核）→16*120个1*1的特征图→（F6：完全连接层）→输出84张特征图</span><br><span class="line"></span><br><span class="line">3:优缺点</span><br><span class="line">全连接层计算代价过大，而使用全部由卷积层组成的神经网络</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">名词注释：</span><br><span class="line"></span><br><span class="line">1:卷积层</span><br><span class="line">作用：特征提取，由多个卷积核构成;一般卷积核是奇数n*n</span><br><span class="line">如3*3的卷积核</span><br><span class="line">[1 0 -1</span><br><span class="line"> 1 0 -1</span><br><span class="line"> 1 0 -1]可以提取到竖向特征</span><br><span class="line"></span><br><span class="line">2：子采样层（也叫pooling层，池化层）</span><br><span class="line">作用：特征选择</span><br><span class="line">例如：经过卷积层之后得到的特征图为</span><br><span class="line">[1,9</span><br><span class="line"> 2,3]</span><br><span class="line">经过池化层之后只保留9，丢弃掉不满足条件的特征，减少参数，防止过拟合等...</span><br><span class="line">最大池化层和平均池化层：对特征图区域的保留方式不同，一种保留最大值，一种区域求平均然后保存</span><br><span class="line"></span><br><span class="line">3：完全连接层</span><br><span class="line">作用：起到分类器的作用</span><br><span class="line">1*1的特征图出来之后，每个特征经过对应的权值w和偏置b就可以得到不同种类的可能性(sigmod)</span><br><span class="line"></span><br><span class="line">4：特征图</span><br><span class="line">描述二维特征，经过卷积核（或者叫做特征滤波器）得到</span><br><span class="line"></span><br><span class="line">5：感受野</span><br><span class="line">特征对应的输入图像区域的大小</span><br></pre></td></tr></table></figure><p>2.2:AlexNet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">1：网络结构</span><br><span class="line">AlexNet网络结构共有8层，前面5层是卷积层，后面3层是全连接层，最后一个全连接层的输出传递给一个1000路的softmax层，对应1000个类标签的分布。输入图片大小为224*224*3(RGB)</span><br><span class="line"></span><br><span class="line">第一层:卷积层,训练时会把输入经过预处理变为227×227×3,使用96个11×11×3的卷积核进行卷积计算,两个GPU分别承担48个运算，每次卷积的步长为4个像素， 生成的特征图为(227-11)/4+1=55，即55×55。</span><br><span class="line">ReLU:线性单元处理,得到2组48*55*55</span><br><span class="line">池化：池化尺寸为3*3，步长为2，池化后的像素规模为27*27*96</span><br><span class="line">归一化：运算的尺寸为5*5，归一化之后的像素规模不变</span><br><span class="line"></span><br><span class="line">第二层：卷积层，对输入27*27*48的2组，进行像素填充2(上下左右)，填充之后变为(27+2+2)*(27+2+2)。256个大小为5*5的卷积核，步长为1，卷积后的大小为27*27*128*2组。</span><br><span class="line">ReLU:这些像素层经过ReLU单元的处理，生成激活像素层，尺寸仍为两组27×27×128的像素层。</span><br><span class="line">池化：池化运算的尺寸为3×3，步长为2，池化后图像的尺寸为(57-3)/2+1=13，即池化后像素的规模为2组13×13×128的像素层</span><br><span class="line">归一化:归一化运算的尺度为5×5，归一化后的像素层的规模为2组13×13×128的像素层，分别由2个GPU进行运算。</span><br><span class="line"></span><br><span class="line">第三层：卷积层，第三层输入数据为第二层输出的2组13×13×128的像素层，为便于后续处理，每幅像素层的上下左右边缘都填充1个像素，填充后变为 (13+1+1)×(13+1+1)×128，分布在两个GPU中进行运算。这一层中每个GPU都有192个卷积核，每个卷积核的尺寸是3×3×256。因此，每个GPU中的卷积核都能对2组13×13×128的像素层的所有数据进行卷积运算。两个GPU有通过交叉的虚线连接，也就是说每个GPU要处理来自前一层的所有GPU的输入。</span><br><span class="line">ReLU：卷积后的像素层经过ReLU单元的处理，生成激活像素层，尺寸仍为2组13×13×192的像素层，分配给两组GPU处理。</span><br><span class="line"></span><br><span class="line">第四层：卷积层，第四层输入数据为第三层输出的2组13×13×192的像素层，类似于第三层，为便于后续处理，每幅像素层的上下左右边缘都填充1个像素，填充后的尺寸变为 (13+1+1)×(13+1+1)×192，分布在两个GPU中进行运算。</span><br><span class="line">这一层中每个GPU都有192个卷积核，每个卷积核的尺寸是3×3×192（与第三层不同，第四层的GPU之间没有虚线连接，也即GPU之间没有通信）。卷积的移动步长是1个像素，经卷积运算后的尺寸为 (13+1+1-3)/1+1=13，每个GPU中有13×13×192个卷积核，2个GPU卷积后生成13×13×384的像素层。</span><br><span class="line">ReLU：卷积后的像素层经过ReLU单元处理，生成激活像素层，尺寸仍为2组13×13×192像素层，分配给两个GPU处理。</span><br><span class="line"></span><br><span class="line">第五层：卷积层，第五层输入数据为第四层输出的2组13×13×192的像素层，为便于后续处理，每幅像素层的上下左右边缘都填充1个像素，填充后的尺寸变为 (13+1+1)×(13+1+1) ，2组像素层数据被送至2个不同的GPU中进行运算。</span><br><span class="line">这一层中每个GPU都有128个卷积核，每个卷积核的尺寸是3×3×192，卷积的步长是1个像素，经卷积后的尺寸为 (13+1+1-3)/1+1=13，每个GPU中有13×13×128个卷积核，2个GPU卷积后生成13×13×256的像素层。</span><br><span class="line">ReLU：卷积后的像素层经过ReLU单元处理，生成激活像素层，尺寸仍为2组13×13×128像素层，由两个GPU分别处理。</span><br><span class="line">池化：2组13×13×128像素层分别在2个不同GPU中进行池化运算处理，池化运算的尺寸为3×3，步长为2，池化后图像的尺寸为 (13-3)/2+1=6，即池化后像素的规模为两组6×6×128的像素层数据，共有6×6×256的像素层数据。</span><br><span class="line"></span><br><span class="line">第六层：卷积（全连接层）,第六层输入数据是第五层的输出，尺寸为6×6×256。本层共有4096个卷积核，每个卷积核的尺寸为6×6×256，由于卷积核的尺寸刚好与待处理特征图（输入）的尺寸相同，即卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘，一一对应，因此，该层被称为全连接层。由于卷积核与特征图的尺寸相同，卷积运算后只有一个值，因此，卷积后的像素层尺寸为4096×1×1，即有4096个神经元。</span><br><span class="line">ReLU:这4096个运算结果通过ReLU激活函数生成4096个值。</span><br><span class="line">Dropout:然后再通过Dropout运算，输出4096个结果值。</span><br><span class="line"></span><br><span class="line">第七层：全连接层，第六层输出的4096个数据与第七层的4096个神经元进行全连接，然后经ReLU进行处理后生成4096个数据，再经过Dropout处理后输出4096个数据。</span><br><span class="line"></span><br><span class="line">第八层：全连接层，第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出1000个float型的值，这就是预测结果。</span><br><span class="line"></span><br><span class="line">2：优缺点</span><br><span class="line">ReLU,多个GPU：提高了训练速度</span><br><span class="line">重叠池化：提高精度</span><br><span class="line">局部归一化：提高精度</span><br><span class="line">数据扩充，dropout：减少过拟合</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">名词注释：</span><br><span class="line"></span><br><span class="line">1:ReLU（激活函数）</span><br><span class="line">线性修正单元</span><br><span class="line">使用线性的堆叠结果还是线性的，使用sigmod函数会出现梯度消失的问题（即在z很大或者很小的时候梯度接近0）</span><br><span class="line"></span><br><span class="line">2:局部归一化(最常用的min-max归一化)</span><br><span class="line">x_new=(x-x_min)/(x_max-x_min)，增强主导特征（1），提取干扰特征（0）</span><br><span class="line"></span><br><span class="line">3：重叠池化</span><br><span class="line">一般池化的步长和池化大小一致，重叠池化的大小要大于步长；例如3*3池化，步长为2;可以增加精度，如</span><br><span class="line">[9 7 8</span><br><span class="line"> 1 2 3</span><br><span class="line"> 4 5 2]</span><br><span class="line">如果是一般最大池化，则8会被舍去，但是可能8是一个有用特征，使用重叠池化后，在第二个池化结构中会被保留</span><br><span class="line"></span><br><span class="line">4：dropout</span><br><span class="line">我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。</span><br><span class="line"></span><br><span class="line">5：梯度消失和梯度爆炸</span><br><span class="line">前言：使用反向传播可以快速的提高学习效率，使用反向传播的过程需要依靠梯度来更新权值参数，那么就会出现梯度消失（梯度接近0）和梯度爆炸（梯度接近正无穷的问题）</span><br><span class="line">梯度消失会影响学习速度</span><br><span class="line">梯度爆炸则无法训练</span><br></pre></td></tr></table></figure><p>2.3:ZFNet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1：基于AlexNet进行微调，改变了AlexNet的第一层：即将滤波器的大小11*11变为了7*7，并且将步长4变为了2</span><br><span class="line">2：使用Relu激活函数和交叉熵损失函数</span><br><span class="line">3：使用反卷积，可视化feature map</span><br><span class="line">4：与AlexNet相比，前面的层使用了更小的卷积核和更小的步长，保留了更多的特征</span><br><span class="line">5：通过遮挡，找出了决定图像类别的关键部位。通过实验，说明了深度增加时，网络可以学习到更具有区分的特征。</span><br><span class="line">6：网络训练时，底层参数收敛快，越到高层，则需要越长的时间训练，才能收敛</span><br></pre></td></tr></table></figure><p>2.4:VGG</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">VGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）。对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。</span><br><span class="line"></span><br><span class="line">简单来说，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</span><br><span class="line"></span><br><span class="line">比如，3个步长为1的3x3卷积核的一层层叠加作用可看成一个大小为7的感受野（其实就表示3个3x3连续卷积相当于一个7x7卷积），其参数总量为 3x(9xC^2) ，如果直接使用7x7卷积核，其参数总量为 49xC^2 ，这里 C 指的是输入和输出的通道数。很明显，27xC^2小于49xC^2，即减少了参数；而且3x3卷积核有利于更好地保持图像性质。</span><br><span class="line"></span><br><span class="line">2个3*3可以代替一个5*5；（5-3+1-3+1=1）</span><br><span class="line">3个3*3可以代替一个7*7：（7-3+1-3+1-3+1=1）</span><br><span class="line"></span><br><span class="line">VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。</span><br><span class="line">几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）卷积层好：</span><br><span class="line">验证了通过不断加深网络结构可以提升性能。</span><br></pre></td></tr></table></figure><p>2.5:GoogleNet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">一般来说，提升网络性能最直接的办法就是增加网络深度和宽度，深度指网络层次数量、宽度指神经元数量。但这种方式存在以下问题：</span><br><span class="line">（1）参数太多，如果训练数据集有限，很容易产生过拟合；</span><br><span class="line">（2）网络越大、参数越多，计算复杂度越大，难以应用；</span><br><span class="line">（3）网络越深，容易出现梯度弥散问题（梯度越往后穿越容易消失），难以优化模型。</span><br><span class="line">因此，GoogLeNet团队提出了Inception网络结构，就是构造一种“基础神经元”结构，来搭建一个稀疏性、高计算性能的网络结构。</span><br><span class="line"></span><br><span class="line">Inception1：</span><br><span class="line">这个Inception原始版本，所有的卷积核都在上一层的所有输出上来做，而那个5x5的卷积核所需的计算量就太大了，造成了特征图的厚度很大，为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到了降低特征图厚度的作用，这也就形成了Inception v1的网络结构.</span><br><span class="line"></span><br><span class="line">1x1的卷积核有什么用呢？</span><br><span class="line">1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）。比如，上一层的输出为100x100x128，经过具有256个通道的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256，其中，卷积层的参数为128x5x5x256= 819200。而假如上一层输出先经过具有32个通道的1x1卷积层，再经过具有256个输出的5x5卷积层，那么输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256= 204800，大约减少了4倍。</span><br><span class="line"></span><br><span class="line">Inception2:</span><br><span class="line">卷积分解（Factorizing Convolutions）:和vgg一样</span><br><span class="line">n*n的卷积层分解为1*n之后接一个n*1的</span><br><span class="line">降低特征图大小</span><br><span class="line"></span><br><span class="line">Inception3:</span><br><span class="line">将7x7分解成两个一维的卷积（1x7,7x1），3x3也是一样（1x3,3x1），这样的好处，既可以加速计算，又可以将1个卷积拆成2个卷积，使得网络深度进一步增加，增加了网络的非线性（每增加一层都要进行ReLU）。</span><br><span class="line">另外，网络输入从224x224变为了299x299。</span><br><span class="line"></span><br><span class="line">Inception V4:</span><br><span class="line">Inception V4研究了Inception模块与残差连接的结合。ResNet结构大大地加深了网络深度，还极大地提升了训练速度，同时性能也有提升</span><br></pre></td></tr></table></figure><p>2.6:resnet(残差网络)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">残差跳跃式结构：</span><br><span class="line">随着网络层级的不断增加，模型精度不断得到提升，而当网络层级增加到一定的数目以后，训练精度和测试精度迅速下降，这说明当网络变得很深以后，深度网络就变得更加难以训练了。</span><br><span class="line">神经网络在反向传播过程中要不断地传播梯度，而当网络层数加深时，梯度在传播过程中会逐渐消失（假如采用Sigmoid函数，对于幅度为1的信号，每向后传递一层，梯度就衰减为原来的0.25，层数越多，衰减越厉害），导致无法对前面网络层的权重进行有效的调整。</span><br><span class="line"></span><br><span class="line">如果已经学习到较饱和的准确率（或者当发现下层的误差变大时），那么接下来的学习目标就转变为恒等映射的学习，也就是使输入x近似于输出H(x)，以保持在后面的层次中不会造成精度下降。</span><br><span class="line">通过“shortcut connections（捷径连接）”的方式，直接把输入x传到输出作为初始结果，输出结果为H(x)=F(x)+x，当F(x)=0时，那么H(x)=x，也就是上面所提到的恒等映射。于是，ResNet相当于将学习目标改变了，不再是学习一个完整的输出，而是目标值H(X)和x的差值，也就是所谓的残差F(x) := H(x)-x，因此，后面的训练目标就是要将残差结果逼近于0，使到随着网络加深，准确率不下降。</span><br></pre></td></tr></table></figure><p>2.7:SENet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Sequeeze-and-Excitation(SE) block并不是一个完整的网络结构，而是一个子结构，可以嵌到其他分类或检测模型中，作者采用SENet block和ResNeXt结合在ILSVRC 2017的分类项目中拿到第一。</span><br><span class="line">SENet的核心思想在于通过网络根据loss去学习特征权重，使得有效的feature map权重大，无效或效果小的feature map权重小的方式训练模型达到更好的结果。</span><br><span class="line">作者的动机是希望显式地建模特征通道之间的相互依赖关系。另外，作者并未引入新的空间维度来进行特征通道间的融合，而是采用了一种全新的「特征重标定」策略。具体来说，就是通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。</span><br><span class="line"></span><br><span class="line">给定一个输入 x，其特征通道数为 c_1，通过一系列卷积等一般变换后得到一个特征通道数为 c_2 的特征，传统的 CNN 不一样的是，接下来通过三个操作来重标定前面得到的特征。</span><br><span class="line"></span><br><span class="line">首先是 Squeeze 操作，顺着空间维度来进行特征压缩，将每个二维的特征通道变成一个实数，这个实数某种程度上具有全局的感受野，并且输出的维度和输入的特征通道数相匹配。它表征着在特征通道上响应的全局分布，而且使得靠近输入的层也可以获得全局的感受野，这一点在很多任务中都是非常有用的。</span><br><span class="line"></span><br><span class="line">其次是 Excitation 操作，它是一个类似于循环神经网络中门的机制。通过参数 w 来为每个特征通道生成权重，其中参数 w 被学习用来显式地建模特征通道间的相关性。</span><br><span class="line"></span><br><span class="line">最后是一个 Reweight 的操作，将 Excitation 的输出的权重看做是进过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。</span><br></pre></td></tr></table></figure><p>2.8DenseNet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DenseNet的几个优点：</span><br><span class="line">1、减轻了vanishing-gradient（梯度消失）</span><br><span class="line">2、加强了feature的传递</span><br><span class="line">3、更有效地利用了feature</span><br><span class="line">4、一定程度上较少了参数数量</span><br><span class="line"></span><br><span class="line">在深度学习网络中，随着网络深度的加深，梯度消失问题会愈加明显，目前很多论文都针对这个问题提出了解决方案，比如ResNet，Highway Networks，Stochastic depth，FractalNets等，尽管这些算法的网络结构有差别，但是核心都在于：create short paths from early layers to later layers。那么作者是怎么做呢？延续这个思路，那就是在保证网络中层与层之间最大程度的信息传输的前提下，直接将所有层连接起来！</span><br><span class="line"></span><br><span class="line">在传统的卷积神经网络中，如果你有L层，那么就会有L个连接，但是在DenseNet中，会有L(L+1)/2个连接。简单讲，就是每一层的输入来自前面所有层的输出。</span><br><span class="line"></span><br><span class="line">DenseNet的一个优点是网络更窄，参数更少，很大一部分原因得益于这种dense block的设计，后面有提到在dense block中每个卷积层的输出feature map的数量都很小（小于100），而不是像其他网络一样动不动就几百上千的宽度。同时这种连接方式使得特征和梯度的传递更加有效，网络也就更加容易训练。原文的一句话非常喜欢：Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.直接解释了为什么这个网络的效果会很好。前面提到过梯度消失问题在网络深度越深的时候越容易出现，原因就是输入信息和梯度信息在很多层之间传递导致的，而现在这种dense connection相当于每一层都直接连接input和loss，因此就可以减轻梯度消失现象，这样更深网络不是问题。另外作者还观察到这种dense connection有正则化的效果，因此对于过拟合有一定的抑制作用。</span><br><span class="line"></span><br><span class="line">该文章提出的DenseNet核心思想在于建立了不同层之间的连接关系，充分利用了feature，进一步减轻了梯度消失问题，加深网络不是问题，而且训练效果非常好。另外，利用bottleneck layer，Translation layer以及较小的growth rate使得网络变窄，参数减少，有效抑制了过拟合，同时计算量也减少了。DenseNet优点很多，而且在和ResNet的对比中优势还是非常明显的。</span><br></pre></td></tr></table></figure><p>EfficientNet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pass(调参复杂)</span><br></pre></td></tr></table></figure><h3 id="2：数据增强"><a href="#2：数据增强" class="headerlink" title="2：数据增强"></a>2：数据增强</h3><p>镜像反射和随机剪裁</p><h3 id="3：加快学习的方式"><a href="#3：加快学习的方式" class="headerlink" title="3：加快学习的方式"></a>3：加快学习的方式</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;卷积神经网络&quot;&gt;&lt;a href=&quot;#卷积神经网络&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络&quot;&gt;&lt;/a&gt;卷积神经网络&lt;/h3&gt;&lt;p&gt;1：卷积神经网络的发展&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>ComputerVision_Pytorch</title>
    <link href="https://yanyubing.xyz/2020/04/02/ComputerVision_Pytorch/"/>
    <id>https://yanyubing.xyz/2020/04/02/ComputerVision_Pytorch/</id>
    <published>2020-04-01T16:51:55.635Z</published>
    <updated>2020-04-03T04:04:07.617Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1：Image-Classification"><a href="#1：Image-Classification" class="headerlink" title="1：Image Classification"></a>1：Image Classification</h3><p>1：实现步骤</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1:包导入：导入所有必需的包和模块</span><br><span class="line">2:训练数据增强：torchvision变换用于通过随机缩放，旋转，镜像和/或裁剪来增强训练数据</span><br><span class="line">3:数据标准化：训练，验证和测试数据经过适当裁剪和标准化</span><br><span class="line">4:数据加载：每个组（训练，验证，测试）的数据都通过Torchvision的ImageFolder加载</span><br><span class="line">5:数据批处理：每个集合的数据都通过Torchvision的DataLoader加载</span><br><span class="line">6:预训练网络：从torchvision.models中加载VGG16等预训练网络，并冻结参数</span><br><span class="line">7:前馈分类器：使用功能作为输入，定义了一个新的前馈网络以用作分类器</span><br><span class="line">8:训练网络：前馈分类器的参数经过适当训练，而要素网络的参数则保持不变</span><br><span class="line">9:验证损失和准确性：在训练期间，将显示验证损失和准确性</span><br><span class="line">10:测试精度：根据测试数据测量网络的精度</span><br><span class="line">11:保存模型：将训练后的模型以及关联的超参数和class_to_idx词典保存为检查点</span><br><span class="line">12:加载检查点：有一个函数可以成功加载检查点并重建模型</span><br><span class="line">13:图像处理：process_image函数成功将PIL图像转换为可以用作训练模型输入的对象</span><br><span class="line">14:类预测：预测函数成功获取图像的路径和检查点，然后返回该图像的前K个最有可能的类</span><br><span class="line">15:使用matplotlib进行健全性检查：创建一个matplotlib图形，显示图像及其相关的前5个最可能的类，并带有实际花名</span><br></pre></td></tr></table></figure><p>2：确定包是否安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 注意使用GPU版本的pytorch</span><br><span class="line"># Imports here</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sb</span><br><span class="line">import cv2</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch import optim</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torchvision import datasets, transforms, models</span><br></pre></td></tr></table></figure><p>3：加载数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1:数据集切分成为3分，训练集需要做旋转，裁剪等操作来确保模型的适用性</span><br><span class="line">2:图片大小为224*224</span><br><span class="line">3:使用预训练网络，图片需要进行标准化和归一化处理</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># Define transforms for the training, validation, and testing sets</span><br><span class="line">training_transforms = transforms.Compose([transforms.RandomRotation(30),</span><br><span class="line">                                          transforms.RandomResizedCrop(224),</span><br><span class="line">                                          transforms.RandomHorizontalFlip(),</span><br><span class="line">                                          transforms.ToTensor(),</span><br><span class="line">                                          transforms.Normalize([0.485, 0.456, 0.406], </span><br><span class="line">                                                               [0.229, 0.224, 0.225])])</span><br><span class="line"></span><br><span class="line">validation_transforms = transforms.Compose([transforms.Resize(256),</span><br><span class="line">                                            transforms.CenterCrop(224),</span><br><span class="line">                                            transforms.ToTensor(),</span><br><span class="line">                                            transforms.Normalize([0.485, 0.456, 0.406], </span><br><span class="line">                                                                 [0.229, 0.224, 0.225])])</span><br><span class="line"></span><br><span class="line">testing_transforms = transforms.Compose([transforms.Resize(256),</span><br><span class="line">                                         transforms.CenterCrop(224),</span><br><span class="line">                                         transforms.ToTensor(),</span><br><span class="line">                                         transforms.Normalize([0.485, 0.456, 0.406], </span><br><span class="line">                                                              [0.229, 0.224, 0.225])])</span><br><span class="line"></span><br><span class="line"># TODO: Load the datasets with ImageFolder</span><br><span class="line">training_dataset = datasets.ImageFolder(train_dir, transform=training_transforms)</span><br><span class="line">validation_dataset = datasets.ImageFolder(valid_dir, transform=validation_transforms)</span><br><span class="line">testing_dataset = datasets.ImageFolder(test_dir, transform=testing_transforms)</span><br><span class="line"></span><br><span class="line"># TODO: Using the image datasets and the trainforms, define the dataloaders</span><br><span class="line">train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)</span><br><span class="line">validate_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(testing_dataset, batch_size=32)</span><br></pre></td></tr></table></figure><p>4：标签匹配</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">需要把种类标签和种类的名字匹配</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">with open(&apos;flower_to_name.json&apos;, &apos;r&apos;) as f:</span><br><span class="line">    flower_to_name = json.load(f)</span><br><span class="line">    </span><br><span class="line">print(len(flower_to_name)) </span><br><span class="line">print(flower_to_name)</span><br></pre></td></tr></table></figure><p>5：建立和训练分类器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Load a pre-trained network (If you need a starting point, the VGG networks work great and are straightforward to use)</span><br><span class="line">Define a new, untrained feed-forward network as a classifier, using ReLU activations and dropout</span><br><span class="line">Train the classifier layers using backpropagation using the pre-trained network to get the features</span><br><span class="line">Track the loss and accuracy on the validation set to determine the best hyperparameters</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Build and train your network</span><br><span class="line"># Transfer Learning</span><br><span class="line">model = models.vgg16(pretrained=True)</span><br><span class="line">#使用前移学习的时候，模型匹配和数据集匹配问题</span><br><span class="line">模型类似，数据集大小不同，需要调整超参数</span><br><span class="line">模型不同，数据集大小相同，要保存检查点</span><br><span class="line">模型类似，数据集大小类似（或者大），直接训练</span><br><span class="line">模型不同，数据集大小不同，既要保存检查点，又要调整超参数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Freeze pretrained model parameters to avoid backpropogating through them</span><br><span class="line">for parameter in model.parameters():</span><br><span class="line">    parameter.requires_grad = False</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from collections import OrderedDict</span><br><span class="line"></span><br><span class="line"># Build custom classifier</span><br><span class="line">classifier = nn.Sequential(OrderedDict([(&apos;fc1&apos;, nn.Linear(25088, 5000)),</span><br><span class="line">                                        (&apos;relu&apos;, nn.ReLU()),</span><br><span class="line">                                        (&apos;drop&apos;, nn.Dropout(p=0.5)),</span><br><span class="line">                                        (&apos;fc2&apos;, nn.Linear(5000, 102)),</span><br><span class="line">                                        (&apos;output&apos;, nn.LogSoftmax(dim=1))]))</span><br><span class="line"></span><br><span class="line">model.classifier = classifier</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Function for the validation pass</span><br><span class="line">def validation(model, validateloader, criterion):</span><br><span class="line">    </span><br><span class="line">    val_loss = 0</span><br><span class="line">    accuracy = 0</span><br><span class="line">    </span><br><span class="line">    for images, labels in iter(validateloader):</span><br><span class="line"></span><br><span class="line">        images, labels = images.to(&apos;cuda&apos;), labels.to(&apos;cuda&apos;)</span><br><span class="line"></span><br><span class="line">        output = model.forward(images)</span><br><span class="line">        val_loss += criterion(output, labels).item()</span><br><span class="line"></span><br><span class="line">        probabilities = torch.exp(output)</span><br><span class="line">        </span><br><span class="line">        equality = (labels.data == probabilities.max(dim=1)[1])</span><br><span class="line">        accuracy += equality.type(torch.FloatTensor).mean()</span><br><span class="line">    </span><br><span class="line">    return val_loss, accuracy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Loss function and gradient descent</span><br><span class="line"></span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># Train the classifier</span><br><span class="line"></span><br><span class="line">from workspace_utils import active_session</span><br><span class="line"></span><br><span class="line">def train_classifier():</span><br><span class="line"></span><br><span class="line">    with active_session():</span><br><span class="line"></span><br><span class="line">        epochs = 15</span><br><span class="line">        steps = 0</span><br><span class="line">        print_every = 40</span><br><span class="line"></span><br><span class="line">        model.to(&apos;cuda&apos;)</span><br><span class="line"></span><br><span class="line">        for e in range(epochs):</span><br><span class="line">        </span><br><span class="line">            model.train()</span><br><span class="line">    </span><br><span class="line">            running_loss = 0</span><br><span class="line">    </span><br><span class="line">            for images, labels in iter(train_loader):</span><br><span class="line">        </span><br><span class="line">                steps += 1</span><br><span class="line">        </span><br><span class="line">                images, labels = images.to(&apos;cuda&apos;), labels.to(&apos;cuda&apos;)</span><br><span class="line">        </span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">                output = model.forward(images)</span><br><span class="line">                loss = criterion(output, labels)</span><br><span class="line">                loss.backward()</span><br><span class="line">                optimizer.step()</span><br><span class="line">        </span><br><span class="line">                running_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">                if steps % print_every == 0:</span><br><span class="line">                </span><br><span class="line">                    model.eval()</span><br><span class="line">                </span><br><span class="line">                    # Turn off gradients for validation, saves memory and computations</span><br><span class="line">                    with torch.no_grad():</span><br><span class="line">                        validation_loss, accuracy = validation(model, validate_loader, criterion)</span><br><span class="line">            </span><br><span class="line">                    print(&quot;Epoch: &#123;&#125;/&#123;&#125;.. &quot;.format(e+1, epochs),</span><br><span class="line">                          &quot;Training Loss: &#123;:.3f&#125;.. &quot;.format(running_loss/print_every),</span><br><span class="line">                          &quot;Validation Loss: &#123;:.3f&#125;.. &quot;.format(validation_loss/len(validate_loader)),</span><br><span class="line">                          &quot;Validation Accuracy: &#123;:.3f&#125;&quot;.format(accuracy/len(validate_loader)))</span><br><span class="line">            </span><br><span class="line">                    running_loss = 0</span><br><span class="line">                    model.train()</span><br><span class="line">                    </span><br><span class="line">train_classifier()</span><br></pre></td></tr></table></figure><p>6：测试网络</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def test_accuracy(model, test_loader):</span><br><span class="line"></span><br><span class="line">    # Do validation on the test set</span><br><span class="line">    model.eval()</span><br><span class="line">    model.to(&apos;cuda&apos;)</span><br><span class="line"></span><br><span class="line">    with torch.no_grad():</span><br><span class="line">    </span><br><span class="line">        accuracy = 0</span><br><span class="line">    </span><br><span class="line">        for images, labels in iter(test_loader):</span><br><span class="line">    </span><br><span class="line">            images, labels = images.to(&apos;cuda&apos;), labels.to(&apos;cuda&apos;)</span><br><span class="line">    </span><br><span class="line">            output = model.forward(images)</span><br><span class="line"></span><br><span class="line">            probabilities = torch.exp(output)</span><br><span class="line">        </span><br><span class="line">            equality = (labels.data == probabilities.max(dim=1)[1])</span><br><span class="line">        </span><br><span class="line">            accuracy += equality.type(torch.FloatTensor).mean()</span><br><span class="line">        </span><br><span class="line">        print(&quot;Test Accuracy: &#123;&#125;&quot;.format(accuracy/len(test_loader)))    </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">test_accuracy(model, test_loader)</span><br></pre></td></tr></table></figure><p>7：保存检查点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Save the checkpoint</span><br><span class="line"></span><br><span class="line">def save_checkpoint(model):</span><br><span class="line"></span><br><span class="line">    model.class_to_idx = training_dataset.class_to_idx</span><br><span class="line"></span><br><span class="line">    checkpoint = &#123;&apos;arch&apos;: &quot;vgg16&quot;,</span><br><span class="line">                  &apos;class_to_idx&apos;: model.class_to_idx,</span><br><span class="line">                  &apos;model_state_dict&apos;: model.state_dict()</span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">    torch.save(checkpoint, &apos;checkpoint.pth&apos;)</span><br><span class="line">    </span><br><span class="line">save_checkpoint(model)</span><br></pre></td></tr></table></figure><p>8：加载检查点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from collections import OrderedDict</span><br><span class="line"></span><br><span class="line"># Function that loads a checkpoint and rebuilds the model</span><br><span class="line"></span><br><span class="line">def load_checkpoint(filepath):</span><br><span class="line">    </span><br><span class="line">    checkpoint = torch.load(filepath)</span><br><span class="line">    </span><br><span class="line">    if checkpoint[&apos;arch&apos;] == &apos;vgg16&apos;:</span><br><span class="line">        </span><br><span class="line">        model = models.vgg16(pretrained=True)</span><br><span class="line">        </span><br><span class="line">        for param in model.parameters():</span><br><span class="line">            param.requires_grad = False</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;Architecture not recognized.&quot;)</span><br><span class="line">    </span><br><span class="line">    model.class_to_idx = checkpoint[&apos;class_to_idx&apos;]</span><br><span class="line">    </span><br><span class="line">    classifier = nn.Sequential(OrderedDict([(&apos;fc1&apos;, nn.Linear(25088, 5000)),</span><br><span class="line">                                            (&apos;relu&apos;, nn.ReLU()),</span><br><span class="line">                                            (&apos;drop&apos;, nn.Dropout(p=0.5)),</span><br><span class="line">                                            (&apos;fc2&apos;, nn.Linear(5000, 102)),</span><br><span class="line">                                            (&apos;output&apos;, nn.LogSoftmax(dim=1))]))</span><br><span class="line"></span><br><span class="line">    model.classifier = classifier</span><br><span class="line">    </span><br><span class="line">    model.load_state_dict(checkpoint[&apos;model_state_dict&apos;])</span><br><span class="line">    </span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">#model = load_checkpoint(&apos;checkpoint.pth&apos;)</span><br><span class="line">#print(model)</span><br></pre></td></tr></table></figure><p>9：分类推测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">probs, classes = predict(image_path, model)</span><br><span class="line">print(probs)</span><br><span class="line">print(classes)</span><br><span class="line"># 可能性和种类</span><br><span class="line">&gt; [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]</span><br><span class="line">&gt; [&apos;70&apos;, &apos;3&apos;, &apos;45&apos;, &apos;62&apos;, &apos;55&apos;]</span><br></pre></td></tr></table></figure><p>10：图片处理</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">def process_image(image_path):</span><br><span class="line">    &apos;&apos;&apos; Scales, crops, and normalizes a PIL image for a PyTorch model,</span><br><span class="line">        returns an Numpy array</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    </span><br><span class="line">    # Process a PIL image for use in a PyTorch model</span><br><span class="line">    </span><br><span class="line">    pil_image = Image.open(image_path)</span><br><span class="line">    </span><br><span class="line">    # Resize</span><br><span class="line">    if pil_image.size[0] &gt; pil_image.size[1]:</span><br><span class="line">        pil_image.thumbnail((5000, 256))</span><br><span class="line">    else:</span><br><span class="line">        pil_image.thumbnail((256, 5000))</span><br><span class="line">        </span><br><span class="line">    # Crop </span><br><span class="line">    left_margin = (pil_image.width-224)/2</span><br><span class="line">    bottom_margin = (pil_image.height-224)/2</span><br><span class="line">    right_margin = left_margin + 224</span><br><span class="line">    top_margin = bottom_margin + 224</span><br><span class="line">    </span><br><span class="line">    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))</span><br><span class="line">    </span><br><span class="line">    # Normalize</span><br><span class="line">    np_image = np.array(pil_image)/255</span><br><span class="line">    mean = np.array([0.485, 0.456, 0.406])</span><br><span class="line">    std = np.array([0.229, 0.224, 0.225])</span><br><span class="line">    np_image = (np_image - mean) / std</span><br><span class="line">    </span><br><span class="line">    # PyTorch expects the color channel to be the first dimension but it&apos;s the third dimension in the PIL image and Numpy array</span><br><span class="line">    # Color channel needs to be first; retain the order of the other two dimensions.</span><br><span class="line">    np_image = np_image.transpose((2, 0, 1))</span><br><span class="line">    </span><br><span class="line">    return np_image</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def imshow(image, ax=None, title=None):</span><br><span class="line">    if ax is None:</span><br><span class="line">        fig, ax = plt.subplots()</span><br><span class="line">    </span><br><span class="line">    # PyTorch tensors assume the color channel is the first dimension</span><br><span class="line">    # but matplotlib assumes is the third dimension</span><br><span class="line">    image = image.transpose((1, 2, 0))</span><br><span class="line">    </span><br><span class="line">    # Undo preprocessing</span><br><span class="line">    mean = np.array([0.485, 0.456, 0.406])</span><br><span class="line">    std = np.array([0.229, 0.224, 0.225])</span><br><span class="line">    image = std * image + mean</span><br><span class="line">    </span><br><span class="line">    if title is not None:</span><br><span class="line">        ax.set_title(title)</span><br><span class="line">    </span><br><span class="line">    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed</span><br><span class="line">    image = np.clip(image, 0, 1)</span><br><span class="line">    </span><br><span class="line">    ax.imshow(image)</span><br><span class="line">    </span><br><span class="line">    return ax</span><br><span class="line"></span><br><span class="line">image = process_image(&apos;flowers/test/1/image_06743.jpg&apos;)</span><br><span class="line">imshow(image)</span><br></pre></td></tr></table></figure><p>11：类别预测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def predict(image_path, model, topk=5):</span><br><span class="line">    &apos;&apos;&apos; Predict the class (or classes) of an image using a trained deep learning model.</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    </span><br><span class="line">    image = process_image(image_path)</span><br><span class="line">    </span><br><span class="line">    # Convert image to PyTorch tensor first</span><br><span class="line">    image = torch.from_numpy(image).type(torch.cuda.FloatTensor)</span><br><span class="line">    #print(image.shape)</span><br><span class="line">    #print(type(image))</span><br><span class="line">    </span><br><span class="line">    # Returns a new tensor with a dimension of size one inserted at the specified position.</span><br><span class="line">    image = image.unsqueeze(0)</span><br><span class="line">    </span><br><span class="line">    output = model.forward(image)</span><br><span class="line">    </span><br><span class="line">    probabilities = torch.exp(output)</span><br><span class="line">    </span><br><span class="line">    # Probabilities and the indices of those probabilities corresponding to the classes</span><br><span class="line">    top_probabilities, top_indices = probabilities.topk(topk)</span><br><span class="line">    </span><br><span class="line">    # Convert to lists</span><br><span class="line">    top_probabilities = top_probabilities.detach().type(torch.FloatTensor).numpy().tolist()[0] </span><br><span class="line">    top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0] </span><br><span class="line">    </span><br><span class="line">    # Convert topk_indices to the actual class labels using class_to_idx</span><br><span class="line">    # Invert the dictionary so you get a mapping from index to class.</span><br><span class="line">    </span><br><span class="line">    idx_to_class = &#123;value: key for key, value in model.class_to_idx.items()&#125;</span><br><span class="line">    #print(idx_to_class)</span><br><span class="line">    </span><br><span class="line">    top_classes = [idx_to_class[index] for index in top_indices]</span><br><span class="line">    </span><br><span class="line">    return top_probabilities, top_classes</span><br><span class="line">    </span><br><span class="line">probs, classes = predict(&apos;flowers/test/15/image_06369.jpg&apos;, model)   </span><br><span class="line">print(probs)</span><br><span class="line">print(classes)</span><br></pre></td></tr></table></figure><p>12：健全性检查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Display an image along with the top 5 classes</span><br><span class="line"></span><br><span class="line"># Plot flower input image</span><br><span class="line">plt.figure(figsize = (6,10))</span><br><span class="line">plot_1 = plt.subplot(2,1,1)</span><br><span class="line"></span><br><span class="line">#image = process_image(&apos;flowers/test/1/image_06743.jpg&apos;)</span><br><span class="line">image = process_image(&apos;flowers/test/15/image_06369.jpg&apos;)</span><br><span class="line"></span><br><span class="line">flower_title = flower_to_name[&apos;15&apos;]</span><br><span class="line"></span><br><span class="line">imshow(image, plot_1, title=flower_title);</span><br><span class="line"></span><br><span class="line"># Convert from the class integer encoding to actual flower names</span><br><span class="line">flower_names = [flower_to_name[i] for i in classes]</span><br><span class="line"></span><br><span class="line"># Plot the probabilities for the top 5 classes as a bar graph</span><br><span class="line">plt.subplot(2,1,2)</span><br><span class="line"></span><br><span class="line">sb.barplot(x=probs, y=flower_names, color=sb.color_palette()[0]);</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1：Image-Classification&quot;&gt;&lt;a href=&quot;#1：Image-Classification&quot; class=&quot;headerlink&quot; title=&quot;1：Image Classification&quot;&gt;&lt;/a&gt;1：Image Classificati
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>ComputerVisionStep-By-Step(完整版)</title>
    <link href="https://yanyubing.xyz/2020/04/01/ComputerVisionStep-By-Step(%E5%AE%8C%E6%95%B4%E7%89%88)/"/>
    <id>https://yanyubing.xyz/2020/04/01/ComputerVisionStep-By-Step(%E5%AE%8C%E6%95%B4%E7%89%88)/</id>
    <published>2020-03-31T17:06:17.158Z</published>
    <updated>2020-04-01T15:09:50.495Z</updated>
    
    <content type="html"><![CDATA[<p>1： 如何使用PIL / Pillow在Python中为深度学习加载和处理图像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"># 如何使用PIL / Pillow在Python中为深度学习加载和处理图像</span><br><span class="line"># 加载图片方式有很多种：OpenCV等，主要是需要了解有加载图片的步骤</span><br><span class="line"># check Pillow version number</span><br><span class="line">import PIL</span><br><span class="line"># load and show an image with Pillow</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># 版本号</span><br><span class="line">print(&apos;Pillow Version:&apos;, PIL.__version__)</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># summarize some details about the image</span><br><span class="line"># 图片格式</span><br><span class="line">print(image.format)</span><br><span class="line"># 图片通道格式</span><br><span class="line">print(image.mode)</span><br><span class="line"># 图片大小</span><br><span class="line">print(image.size)</span><br><span class="line"># show the image</span><br><span class="line">image.show()</span><br><span class="line"></span><br><span class="line"># 如何将图像转换为NumPy数组并返回</span><br><span class="line"># load and display an image with Matplotlib</span><br><span class="line">from matplotlib import image</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line"></span><br><span class="line"># load image as pixel array</span><br><span class="line">data = image.imread(&apos;opera_house.jpg&apos;)</span><br><span class="line"># summarize shape of the pixel array</span><br><span class="line">print(data.dtype)</span><br><span class="line">print(data.shape)</span><br><span class="line"># display the array of pixels as an image</span><br><span class="line">pyplot.imshow(data)</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line"># load image and convert to and from NumPy array</span><br><span class="line"># 怎么把numpy数组转换为图片</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from PIL import Image</span><br><span class="line">from numpy import asarray</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># convert image to numpy array</span><br><span class="line">data = asarray(image)</span><br><span class="line"># summarize shape</span><br><span class="line">print(data.shape)</span><br><span class="line"># create Pillow image</span><br><span class="line">image2 = Image.fromarray(data)</span><br><span class="line"># summarize image details</span><br><span class="line">print(image2.format)</span><br><span class="line">print(image2.mode)</span><br><span class="line">print(image2.size)</span><br><span class="line"></span><br><span class="line"># 如何保存图片</span><br><span class="line"># example of saving an image in another format</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># save as PNG format</span><br><span class="line">image.save(&apos;opera_house.png&apos;, format=&apos;PNG&apos;)</span><br><span class="line"># load the image again and inspect the format</span><br><span class="line">image2 = Image.open(&apos;opera_house.png&apos;)</span><br><span class="line">print(image2.format)</span><br><span class="line"></span><br><span class="line"># 图片从三通道RGB格式转成1通道灰度图片</span><br><span class="line"># example of saving a grayscale version of a loaded image</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># convert the image to grayscale</span><br><span class="line">gs_image = image.convert(mode=&apos;L&apos;)</span><br><span class="line"># save in jpeg format</span><br><span class="line">gs_image.save(&apos;opera_house_grayscale.jpg&apos;)</span><br><span class="line"># load the image again and show it</span><br><span class="line">image2 = Image.open(&apos;opera_house_grayscale.jpg&apos;)</span><br><span class="line"># show the image</span><br><span class="line">image2.show()</span><br><span class="line"></span><br><span class="line"># 调整图片的大小,保持宽高比一样</span><br><span class="line"># create a thumbnail of an image</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># report the size of the image</span><br><span class="line">print(image.size)</span><br><span class="line"># create a thumbnail and preserve aspect ratio</span><br><span class="line"># 图片的比例不变，最大尺寸（这里是宽度）缩放为底下参数100</span><br><span class="line">image.thumbnail((100, 100))</span><br><span class="line"># report the size of the thumbnail</span><br><span class="line">print(image.size)</span><br><span class="line"></span><br><span class="line"># 调整图片大小，宽高比改变（变成新形状）</span><br><span class="line"># resize image and force a new shape</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># report the size of the image</span><br><span class="line">print(image.size)</span><br><span class="line"># resize image and ignore original aspect ratio</span><br><span class="line">img_resized = image.resize((200, 200))</span><br><span class="line"># report the size of the thumbnail</span><br><span class="line">print(img_resized.size)</span><br><span class="line"></span><br><span class="line"># 图片增强(增加样本数):翻转，旋转，裁剪</span><br><span class="line"># 1：翻转</span><br><span class="line"></span><br><span class="line">from PIL import Image</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># horizontal flip</span><br><span class="line">hoz_flip = image.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line"># vertical flip</span><br><span class="line">ver_flip = image.transpose(Image.FLIP_TOP_BOTTOM)</span><br><span class="line"># plot all three images using matplotlib</span><br><span class="line">pyplot.subplot(311)</span><br><span class="line">pyplot.imshow(image)</span><br><span class="line">pyplot.subplot(312)</span><br><span class="line">pyplot.imshow(hoz_flip)</span><br><span class="line">pyplot.subplot(313)</span><br><span class="line">pyplot.imshow(ver_flip)</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line"># 2:旋转</span><br><span class="line"># create rotated versions of an image</span><br><span class="line">from PIL import Image</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># plot original image</span><br><span class="line">pyplot.subplot(311)</span><br><span class="line">pyplot.imshow(image)</span><br><span class="line"># rotate 45 degrees</span><br><span class="line">pyplot.subplot(312)</span><br><span class="line">pyplot.imshow(image.rotate(45))</span><br><span class="line"># rotate 90 degrees</span><br><span class="line">pyplot.subplot(313)</span><br><span class="line">pyplot.imshow(image.rotate(90))</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line"># 3:裁剪</span><br><span class="line"># example of cropping an image</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;opera_house.jpg&apos;)</span><br><span class="line"># create a cropped image</span><br><span class="line">cropped = image.crop((100, 100, 200, 200))</span><br><span class="line"># show cropped image</span><br><span class="line">cropped.show()</span><br></pre></td></tr></table></figure><p>2：How to Load, Convert, and Save Images With the Keras API</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># Keras很牛B的深度学习框架之一,keras依赖于TensorFlow，版本之间需要对应</span><br><span class="line"># tensorflow1.1.4</span><br><span class="line"># example of loading an image with the Keras API</span><br><span class="line">from keras.preprocessing.image import load_img</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line"># 加载图片</span><br><span class="line">img = load_img(&apos;bondi_beach.jpg&apos;)</span><br><span class="line"># report details about the image</span><br><span class="line">print(type(img))</span><br><span class="line">print(img.format)</span><br><span class="line">print(img.mode)</span><br><span class="line">print(img.size)</span><br><span class="line"># show the image</span><br><span class="line">img.show()</span><br><span class="line"></span><br><span class="line"># 如何使用Keras转换图像</span><br><span class="line"># example of converting an image with the Keras API</span><br><span class="line">from keras.preprocessing.image import load_img</span><br><span class="line">from keras.preprocessing.image import img_to_array</span><br><span class="line">from keras.preprocessing.image import array_to_img</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">img = load_img(&apos;bondi_beach.jpg&apos;)</span><br><span class="line">print(type(img))</span><br><span class="line"># convert to numpy array</span><br><span class="line">img_array = img_to_array(img)</span><br><span class="line">print(img_array.dtype)</span><br><span class="line">print(img_array.shape)</span><br><span class="line"># convert back to image</span><br><span class="line">img_pil = array_to_img(img_array)</span><br><span class="line">print(type(img))</span><br><span class="line"></span><br><span class="line"># 使用keras保存图片</span><br><span class="line"># example of saving an image with the Keras API</span><br><span class="line">from keras.preprocessing.image import load_img</span><br><span class="line">from keras.preprocessing.image import save_img</span><br><span class="line">from keras.preprocessing.image import img_to_array</span><br><span class="line"></span><br><span class="line"># load image as as grayscale</span><br><span class="line">img = load_img(&apos;bondi_beach.jpg&apos;, grayscale=True)</span><br><span class="line"># convert image to a numpy array</span><br><span class="line">img_array = img_to_array(img)</span><br><span class="line"># save the image with a new filename</span><br><span class="line">save_img(&apos;bondi_beach_grayscale.jpg&apos;, img_array)</span><br><span class="line"># load the image to confirm it was saved correctly</span><br><span class="line">img = load_img(&apos;bondi_beach_grayscale.jpg&apos;)</span><br><span class="line">print(type(img))</span><br><span class="line">print(img.format)</span><br><span class="line">print(img.mode)</span><br><span class="line">print(img.size)</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure><p>3：A Gentle Introduction to Channels-First and Channels-Last Image Formats</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># 三通道格式有两种表示方式:</span><br><span class="line"># Channels First: [channels][rows][cols]</span><br><span class="line"># Channels Last :[rows][cols][channels]</span><br><span class="line"></span><br><span class="line"># 矩阵变换</span><br><span class="line"># example of expanding dimensions</span><br><span class="line">from numpy import expand_dims</span><br><span class="line">from numpy import asarray</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">img = Image.open(&apos;penguin_arade.jpg&apos;)</span><br><span class="line"># convert the image to grayscale</span><br><span class="line">img = img.convert(mode=&apos;L&apos;)</span><br><span class="line"># convert to numpy array</span><br><span class="line">data = asarray(img)</span><br><span class="line">print(data.shape)</span><br><span class="line"># add channels first</span><br><span class="line">data_first = expand_dims(data, axis=0)</span><br><span class="line">print(data_first.shape)</span><br><span class="line"># add channels last</span><br><span class="line">data_last = expand_dims(data, axis=2)</span><br><span class="line"># numpy的reshape也是很常见的操作</span><br><span class="line">data = data.reshape((424, 640, 1))</span><br><span class="line">print(data_last.shape)</span><br><span class="line"></span><br><span class="line"># channels优先与channels最后的转换</span><br><span class="line"># change image from channels last to channels first format</span><br><span class="line">from numpy import moveaxis</span><br><span class="line">from numpy import asarray</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load the color image</span><br><span class="line">img = Image.open(&apos;penguin_arade.jpg&apos;)</span><br><span class="line"># 此处的图片为四通道，RGBA，A表示透明度</span><br><span class="line">print(img.mode)</span><br><span class="line"># convert to numpy array</span><br><span class="line">data2 = asarray(img)</span><br><span class="line">print(data2.shape)</span><br><span class="line"># change channels last to channels first format</span><br><span class="line">data2 = moveaxis(data2, 2, 0)</span><br><span class="line">print(data2.shape)</span><br><span class="line"># change channels first to channels last format</span><br><span class="line">data2 = moveaxis(data2, 0, 2)</span><br><span class="line">print(data2.shape)</span><br></pre></td></tr></table></figure><p>4：How to Load Large Datasets From Directories for Deep Learning in Keras</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># 怎么从目录中加载大型数据集</span><br><span class="line"></span><br><span class="line"># 数据集目录结构</span><br><span class="line"># data/</span><br><span class="line"># data/train/</span><br><span class="line"># data/test/</span><br><span class="line"># data/validation/</span><br><span class="line"></span><br><span class="line"># 例如，如果我们有一个将汽车的照片分类为红色汽车或蓝色汽车的二进制分类任务，我们将有两个类别，“ 红色 ”和“ 蓝色 ”，因此在每个数据集目录下都有两个类别目录。</span><br><span class="line"># data/</span><br><span class="line"># data/train/</span><br><span class="line"># data/train/red/</span><br><span class="line"># data/train/blue/</span><br><span class="line"># data/test/</span><br><span class="line"># data/test/red/</span><br><span class="line"># data/test/blue/</span><br><span class="line"># data/validation/</span><br><span class="line"># data/validation/red/</span><br><span class="line"># data/validation/blue/</span><br><span class="line"></span><br><span class="line"># 分别放置该类别的图片</span><br><span class="line"># data/train/red/car01.jpg</span><br><span class="line"># data/train/red/car02.jpg</span><br><span class="line"># data/train/red/car03.jpg</span><br><span class="line"># ...</span><br><span class="line"># data/train/blue/car01.jpg</span><br><span class="line"># data/train/blue/car02.jpg</span><br><span class="line"># data/train/blue/car03.jpg</span><br><span class="line"># ...</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># create a data generator</span><br><span class="line">from keras.preprocessing.image import ImageDataGenerator</span><br><span class="line"></span><br><span class="line">datagen = ImageDataGenerator()</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"># load and iterate training dataset</span><br><span class="line">train_it = datagen.flow_from_directory(&apos;data/train/&apos;, class_mode=&apos;binary&apos;, batch_size=64)</span><br><span class="line"># load and iterate validation dataset</span><br><span class="line">val_it = datagen.flow_from_directory(&apos;data/validation/&apos;, class_mode=&apos;binary&apos;, batch_size=64)</span><br><span class="line"># load and iterate test dataset</span><br><span class="line">test_it = datagen.flow_from_directory(&apos;data/test/&apos;, class_mode=&apos;binary&apos;, batch_size=64)</span><br><span class="line"></span><br><span class="line"># confirm the iterator works</span><br><span class="line">batchX, batchy = train_it.next()</span><br><span class="line">print(&apos;Batch shape=%s, min=%.3f, max=%.3f&apos; % (batchX.shape, batchX.min(), batchX.max()))</span><br></pre></td></tr></table></figure><p>5：How to Configure Image Data Augmentation in Keras</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># 使用keras进行图片增强,增加数据集个数</span><br><span class="line"></span><br><span class="line"># example of horizontal shift image augmentation</span><br><span class="line">from numpy import expand_dims</span><br><span class="line">from keras.preprocessing.image import load_img</span><br><span class="line">from keras.preprocessing.image import img_to_array</span><br><span class="line">from keras.preprocessing.image import ImageDataGenerator</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">img = load_img(&apos;bird.jpg&apos;)</span><br><span class="line"># convert to numpy array</span><br><span class="line">data = img_to_array(img)</span><br><span class="line"># expand dimension to one sample</span><br><span class="line">samples = expand_dims(data, 0)</span><br><span class="line"># create image data augmentation generator,</span><br><span class="line"># 水平增强像素值</span><br><span class="line"># datagen = ImageDataGenerator(width_shift_range=[-200, 200])</span><br><span class="line"># 高度增强百分比</span><br><span class="line"># datagen = ImageDataGenerator(height_shift_range=0.5)</span><br><span class="line"># 水平翻转增强</span><br><span class="line"># datagen = ImageDataGenerator(horizontal_flip=True)</span><br><span class="line"># 随机选择增强，指定角度范围为90°</span><br><span class="line"># datagen = ImageDataGenerator(rotation_range=90)</span><br><span class="line"># 随机亮度增强，浮点数范围小于1会变暗，大于1会变亮，这里使用0.2-1随机范围变暗方式</span><br><span class="line"># datagen = ImageDataGenerator(brightness_range=[0.2, 1.0])</span><br><span class="line"># 随机变焦增强</span><br><span class="line">datagen = ImageDataGenerator(zoom_range=[0.5, 1.0])</span><br><span class="line"></span><br><span class="line"># prepare iterator</span><br><span class="line">it = datagen.flow(samples, batch_size=1)</span><br><span class="line"># generate samples and plot</span><br><span class="line">for i in range(9):</span><br><span class="line">    # define subplot</span><br><span class="line">    pyplot.subplot(330 + 1 + i)</span><br><span class="line">    # generate batch of images</span><br><span class="line">    batch = it.next()</span><br><span class="line">    # convert to unsigned integers for viewing</span><br><span class="line">    image = batch[0].astype(&apos;uint8&apos;)</span><br><span class="line">    # plot raw pixel data</span><br><span class="line">    pyplot.imshow(image)</span><br><span class="line"># show the figure</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>6：How to Use Test-Time Augmentation to Make Better Predictions</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>10：How to Manually Scale Image Pixel Data for Deep Learning</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"># 如何手动缩放图片数据来适合深度学习</span><br><span class="line"># load and show an image with Pillow</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">image = Image.open(&apos;sydney_bridge.jpg&apos;)</span><br><span class="line"># summarize some details about the image</span><br><span class="line">print(image.format)</span><br><span class="line">print(image.mode)</span><br><span class="line">print(image.size)</span><br><span class="line"># show the image</span><br><span class="line">image.show()</span><br><span class="line"></span><br><span class="line"># 归一化像素值,使像素值在0-1之间，提高训练模型的速度</span><br><span class="line"># example of pixel normalization</span><br><span class="line">from numpy import asarray</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;sydney_bridge.jpg&apos;)</span><br><span class="line">pixels = asarray(image)</span><br><span class="line"># confirm pixel range is 0-255</span><br><span class="line">print(&apos;Data Type: %s&apos; % pixels.dtype)</span><br><span class="line">print(&apos;Min: %.3f, Max: %.3f&apos; % (pixels.min(), pixels.max()))</span><br><span class="line"># convert from integers to floats</span><br><span class="line">pixels = pixels.astype(&apos;float32&apos;)</span><br><span class="line"># normalize to the range 0-1</span><br><span class="line">pixels /= 255.0</span><br><span class="line"># confirm the normalization</span><br><span class="line">print(&apos;Min: %.3f, Max: %.3f&apos; % (pixels.min(), pixels.max()))</span><br><span class="line"></span><br><span class="line"># 中心化像素值，将像素值减去平均值</span><br><span class="line"># 1:中心化之后归一化处理，均值在0.5范围左右，整体范围在0-1内，这种方式最常见</span><br><span class="line"># 2：归一化之后居中，会出现负数情况</span><br><span class="line"># 全局居中：计算和减去各个颜色通道的平均像素值。</span><br><span class="line"># 局部居中：计算并减去每个颜色通道的平均像素值。</span><br><span class="line"></span><br><span class="line"># example of global centering (subtract mean)</span><br><span class="line"># 全局居中</span><br><span class="line">from numpy import asarray</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;sydney_bridge.jpg&apos;)</span><br><span class="line">pixels = asarray(image)</span><br><span class="line"># convert from integers to floats</span><br><span class="line">pixels = pixels.astype(&apos;float32&apos;)</span><br><span class="line"># calculate global mean</span><br><span class="line">mean = pixels.mean()</span><br><span class="line">print(&apos;Mean: %.3f&apos; % mean)</span><br><span class="line">print(&apos;Min: %.3f, Max: %.3f&apos; % (pixels.min(), pixels.max()))</span><br><span class="line"># global centering of pixels</span><br><span class="line">pixels = pixels - mean</span><br><span class="line"># confirm it had the desired effect</span><br><span class="line">mean = pixels.mean()</span><br><span class="line">print(&apos;Mean: %.3f&apos; % mean)</span><br><span class="line">print(&apos;Min: %.3f, Max: %.3f&apos; % (pixels.min(), pixels.max()))</span><br><span class="line"></span><br><span class="line"># example of per-channel centering (subtract mean)</span><br><span class="line"># 局部居中</span><br><span class="line">from numpy import asarray</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;sydney_bridge.jpg&apos;)</span><br><span class="line">pixels = asarray(image)</span><br><span class="line"># convert from integers to floats</span><br><span class="line">pixels = pixels.astype(&apos;float32&apos;)</span><br><span class="line"># calculate per-channel means and standard deviations</span><br><span class="line">means = pixels.mean(axis=(0, 1), dtype=&apos;float64&apos;)</span><br><span class="line">print(&apos;Means: %s&apos; % means)</span><br><span class="line">print(&apos;Mins: %s, Maxs: %s&apos; % (pixels.min(axis=(0, 1)), pixels.max(axis=(0, 1))))</span><br><span class="line"># per-channel centering of pixels</span><br><span class="line">pixels -= means</span><br><span class="line"># confirm it had the desired effect</span><br><span class="line">means = pixels.mean(axis=(0, 1), dtype=&apos;float64&apos;)</span><br><span class="line">print(&apos;Means: %s&apos; % means)</span><br><span class="line">print(&apos;Mins: %s, Maxs: %s&apos; % (pixels.min(axis=(0, 1)), pixels.max(axis=(0, 1))))</span><br><span class="line"></span><br><span class="line"># 标准化像素值,进行高斯分布</span><br><span class="line"># 将像素值的分布转换为标准高斯可能会有所好处：既将像素值定为零，又通过标准偏差将值归一化。</span><br><span class="line"># 结果是像素值的标准高斯分布，平均值为0.0，标准差为1.0。</span><br><span class="line"></span><br><span class="line"># 问题是此时无法可视化，部分像素值为负数</span><br><span class="line"># example of global pixel standardization</span><br><span class="line">from numpy import asarray</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;sydney_bridge.jpg&apos;)</span><br><span class="line">pixels = asarray(image)</span><br><span class="line"># convert from integers to floats</span><br><span class="line">pixels = pixels.astype(&apos;float32&apos;)</span><br><span class="line"># calculate global mean and standard deviation</span><br><span class="line">mean, std = pixels.mean(), pixels.std()</span><br><span class="line">print(&apos;Mean: %.3f, Standard Deviation: %.3f&apos; % (mean, std))</span><br><span class="line"># global standardization of pixels</span><br><span class="line">pixels = (pixels - mean) / std</span><br><span class="line"># confirm it had the desired effect</span><br><span class="line">mean, std = pixels.mean(), pixels.std()</span><br><span class="line">print(&apos;Mean: %.3f, Standard Deviation: %.3f&apos; % (mean, std))</span><br><span class="line"></span><br><span class="line"># 积极的全球标准化</span><br><span class="line"># 可能希望将像素值保持在正域中，因此可能可以使图像可视化，或者可能是为了在模型中选择激活函数。</span><br><span class="line"># 实现此目的的一种流行方法是将标准化像素值裁剪到[-1，1]范围，然后将值从[-1,1]重新缩放为[0,1]。</span><br><span class="line"># example of global pixel standardization shifted to positive domain</span><br><span class="line">from numpy import asarray</span><br><span class="line">from numpy import clip</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;sydney_bridge.jpg&apos;)</span><br><span class="line">pixels = asarray(image)</span><br><span class="line"># convert from integers to floats</span><br><span class="line">pixels = pixels.astype(&apos;float32&apos;)</span><br><span class="line"># calculate global mean and standard deviation</span><br><span class="line">mean, std = pixels.mean(), pixels.std()</span><br><span class="line">print(&apos;Mean: %.3f, Standard Deviation: %.3f&apos; % (mean, std))</span><br><span class="line"># global standardization of pixels</span><br><span class="line">pixels = (pixels - mean) / std</span><br><span class="line"># clip pixel values to [-1,1]</span><br><span class="line">pixels = clip(pixels, -1.0, 1.0)</span><br><span class="line"># shift from [-1,1] to [0,1] with 0.5 mean</span><br><span class="line">pixels = (pixels + 1.0) / 2.0</span><br><span class="line"># confirm it had the desired effect</span><br><span class="line">mean, std = pixels.mean(), pixels.std()</span><br><span class="line">print(&apos;Mean: %.3f, Standard Deviation: %.3f&apos; % (mean, std))</span><br><span class="line">print(&apos;Min: %.3f, Max: %.3f&apos; % (pixels.min(), pixels.max()))</span><br><span class="line"></span><br><span class="line"># 本地标准化,每个通道各自处理</span><br><span class="line"># Local Standardization</span><br><span class="line"># 下面的示例计算每个通道加载的图像的平均值和标准偏差，然后使用这些统计信息分别标准化每个通道中的像素。</span><br><span class="line"># example of per-channel pixel standardization</span><br><span class="line">from numpy import asarray</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;sydney_bridge.jpg&apos;)</span><br><span class="line">pixels = asarray(image)</span><br><span class="line"># convert from integers to floats</span><br><span class="line">pixels = pixels.astype(&apos;float32&apos;)</span><br><span class="line"># calculate per-channel means and standard deviations</span><br><span class="line">means = pixels.mean(axis=(0, 1), dtype=&apos;float64&apos;)</span><br><span class="line">stds = pixels.std(axis=(0, 1), dtype=&apos;float64&apos;)</span><br><span class="line">print(&apos;Means: %s, Stds: %s&apos; % (means, stds))</span><br><span class="line"># per-channel standardization of pixels</span><br><span class="line">pixels = (pixels - means) / stds</span><br><span class="line"># confirm it had the desired effect</span><br><span class="line">means = pixels.mean(axis=(0, 1), dtype=&apos;float64&apos;)</span><br><span class="line">stds = pixels.std(axis=(0, 1), dtype=&apos;float64&apos;)</span><br><span class="line">print(&apos;Means: %s, Stds: %s&apos; % (means, stds))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1： 如何使用PIL / Pillow在Python中为深度学习加载和处理图像&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MachineLearning_ComputerVision</title>
    <link href="https://yanyubing.xyz/2020/03/27/MachineLearning_ComputerVision/"/>
    <id>https://yanyubing.xyz/2020/03/27/MachineLearning_ComputerVision/</id>
    <published>2020-03-27T15:56:04.557Z</published>
    <updated>2020-03-27T18:46:57.950Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Deep-Learning-for-Computer-Vision"><a href="#Deep-Learning-for-Computer-Vision" class="headerlink" title="Deep Learning for  Computer Vision"></a><strong>Deep Learning for</strong>  <strong>Computer Vision</strong></h3><p>1：<strong>Deep Learning and</strong>  <strong>Computer Vision</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Computer Vision:</span><br><span class="line">Computer Vision, or CV for short, is broadly defined as helping computers to see or extract meaning from digital images such as photographs and videos. Researchers have been working on the problem of helping computers see for more than 50 years, and some great successes have been achieved, such as the face detection available in modern cameras and smartphones. The problem of understanding images is not solved, and may never be. This is primarily because the world is complex and messy. There are few rules. And yet we can easily and effortlessly recognize objects, people, and context.</span><br><span class="line"></span><br><span class="line">Deep Learning:</span><br><span class="line">Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. A property of deep learning is that the performance of this type of model improves by training it with more examples and by increasing its depth or representational capacity. In addition to scalability, another often-cited benefit of deep learning models is their ability to perform automatic feature extraction from raw data, also called feature learning.</span><br><span class="line"></span><br><span class="line">Deep Promise of Deep Learning for Computer vision:</span><br><span class="line">Deep learning methods are popular for computer vision, primarily because they are delivering on their promise. Some of the first large demonstrations of the power of deep learning were in computer vision, specifically image classification. More recently in object detection and face recognition. The three key promises of deep learning for computer vision are as follows:</span><br><span class="line">1:The Promise of Feature Learning(特征学习)</span><br><span class="line">2:The Promise of Continued Improvement(持续改进)</span><br><span class="line">3:The Promise of End-to-End Models(端到端原则，不需要第三方，自己可以从头到尾解决)</span><br></pre></td></tr></table></figure><p>2: <strong>Preparing Image Data</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># example of pixel normalization</span><br><span class="line">from numpy import asarray</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># load image</span><br><span class="line">image = Image.open(&apos;bondi_beach.jpg&apos;)</span><br><span class="line">pixels = asarray(image)</span><br><span class="line"># confirm pixel range is 0-255</span><br><span class="line">print(&apos;Data Type: %s&apos; % pixels.dtype)</span><br><span class="line">print(&apos;Min: %.3f, Max: %.3f&apos; % (pixels.min(), pixels.max()))</span><br><span class="line"># convert from integers to floats</span><br><span class="line"># 归一化</span><br><span class="line">pixels = pixels.astype(&apos;float32&apos;)  # normalize to the range 0-1</span><br><span class="line">pixels /= 255.0</span><br><span class="line"># confirm the normalization</span><br><span class="line">print(&apos;Min: %.3f, Max: %.3f&apos; % (pixels.min(), pixels.max()))</span><br></pre></td></tr></table></figure><p>3:Convolutional Neural Networks(卷积神经网络CNN)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In this lesson, you will discover how to construct a convolutional neural network using a convolutional layer, pooling layer, and fully connected output layer.</span><br><span class="line">在本课程中，您将发现如何使用卷积层，池化层和完全连接的输出层来构建卷积神经网络。</span><br><span class="line">我的理解是：</span><br><span class="line">1：卷积层的作用的提取特征。</span><br><span class="line">2：池化层的作用是降低卷积层对边缘的敏感性。因为卷积层发现边缘过于精确。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Convolutional Layers:</span><br><span class="line">A convolution is the simple application of a filter to an input that results in an activation.Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image. A convolutional layer can be created by specifying both the number of filters to learn and the fixed size of each filter, often called the kernel shape.</span><br><span class="line">卷积是将过滤器简单地应用到输入上以导致激活的过程。将同一过滤器重复应用到输入上会导致激活图称为特征图，该图表示特征在输入中的位置和强度 ，例如图片。 可以通过指定要学习的过滤器数量和每个过滤器的固定大小（通常称为核形状）来创建卷积层。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Pooling Layers:</span><br><span class="line">Pooling layers provide an approach to downsampling feature maps by summarizing the presence of features in patches of the feature map. Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map.</span><br><span class="line">池化层通过汇总特征图的补丁中特征的存在，提供了一种对特征图进行下采样的方法。 最大池化或最大池化是一种池化操作，用于计算每个功能图的每个面片中的最大值或最大值</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Classifier Layer:</span><br><span class="line">Once the features have been extracted, they can be interpreted and used to make a prediction,such as classifying the type of object in a photograph. This can be achieved by first flattening the two-dimensional feature maps, and then adding a fully connected output layer. For a binary classification problem, the output layer would have one node that would predict a value between 0 and 1 for the two classes.</span><br><span class="line">提取特征后，就可以将其解释并用于进行预测，例如对照片中的对象类型进行分类。 这可以通过首先展平二维特征图，然后添加完全连接的输出层来实现。 对于二进制分类问题，输出层将具有一个节点，该节点将为两个类别预测0到1之间的值</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># cnn with single convolutional, pooling and output layer</span><br><span class="line"># The example below creates a convolutional neural network that expects grayscale images with</span><br><span class="line"># the square size of 256 × 256 pixels, with one convolutional layer with 32 filters, each with the</span><br><span class="line"># size of 3 × 3 pixels, a max pooling layer, and a binary classification output layer.</span><br><span class="line"></span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers import Conv2D</span><br><span class="line">from keras.layers import MaxPooling2D</span><br><span class="line">from keras.layers import Flatten</span><br><span class="line">from keras.layers import Dense</span><br><span class="line"></span><br><span class="line"># create model</span><br><span class="line">model = Sequential()</span><br><span class="line"># add convolutional layer</span><br><span class="line">model.add(Conv2D(32, (3, 3), input_shape=(256, 256, 1)))</span><br><span class="line">model.add(MaxPooling2D())</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(1, activation=&apos;sigmoid&apos;))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>4:<strong>Image Classifification</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># In this lesson, you will discover how to use a pre-trained model to classify photographs of objects.</span><br><span class="line"># Deep convolutional neural network models may take days, or even weeks, to train on very</span><br><span class="line"># large datasets. A way to short-cut this process is to re-use the model weights from pre-trained</span><br><span class="line"># models that were developed for standard computer vision benchmark datasets, such as the</span><br><span class="line"># ImageNet image recognition tasks. The example below uses the VGG-16 pre-trained model to</span><br><span class="line"># classify photographs of objects into one of 1,000 known classes. Download this photograph of a</span><br><span class="line"># dog taken by Justin Morgan4 and released under a permissive license. Save it in your current</span><br><span class="line"># working directory with the filename dog.jpg</span><br><span class="line"># 上面表达的意思d是训练一份分类,网络需要的时间过长，我们使用预训练模型</span><br><span class="line"></span><br><span class="line"># example of using a pre-trained model as a classifier</span><br><span class="line"># example of using a pre-trained model as a classifier</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from keras.preprocessing.image import load_img</span><br><span class="line">from keras.preprocessing.image import img_to_array</span><br><span class="line">from keras.applications.vgg16 import preprocess_input</span><br><span class="line">from keras.applications.vgg16 import decode_predictions</span><br><span class="line">from keras.applications.vgg16 import VGG16</span><br><span class="line"></span><br><span class="line"># load an image from file</span><br><span class="line">image = load_img(&apos;dog.jpg&apos;, target_size=(224, 224))</span><br><span class="line"># convert the image pixels to a numpy array</span><br><span class="line">image = img_to_array(image)</span><br><span class="line"># reshape data for the model</span><br><span class="line">image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))</span><br><span class="line"># prepare the image for the VGG model</span><br><span class="line">image = preprocess_input(image)</span><br><span class="line"># load the model</span><br><span class="line"># The example below uses the VGG-16 pre-trained model to</span><br><span class="line"># classify photographs of objects into one of 1,000 known classes</span><br><span class="line">model = VGG16()</span><br><span class="line"># predict the probability across all output classes</span><br><span class="line">yhat = model.predict(image)</span><br><span class="line"># convert the probabilities to class labels</span><br><span class="line">label = decode_predictions(yhat)</span><br><span class="line"># retrieve the most likely result, e.g. highest probability</span><br><span class="line">label = label[0][0]</span><br><span class="line"># print the classification</span><br><span class="line">print(&apos;%s (%.2f%%)&apos; % (label[1], label[2] * 100))</span><br></pre></td></tr></table></figure><p>5: <strong>Train Image Classifification</strong> <strong>Model</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"># In this lesson, you will discover how to train and evaluate a convolutional neural network for</span><br><span class="line"># image classification. The Fashion-MNIST clothing classification problem is a new standard</span><br><span class="line"># dataset used in computer vision and deep learning. It is a dataset comprised of 60,000 small</span><br><span class="line"># square 28 × 28 pixel grayscale images of items of 10 types of clothing, such as shoes, t-shirts,</span><br><span class="line"># dresses, and more. The example below loads the dataset, scales the pixel values, then fits a</span><br><span class="line"># convolutional neural network on the training dataset and evaluates the performance of the</span><br><span class="line"># network on the test dataset. The example will run in just a few minutes on a modern CPU; no</span><br><span class="line"># GPU is required.</span><br><span class="line"></span><br><span class="line"># 在本课程中，您将发现如何训练和评估卷积神经网络进行图像分类。</span><br><span class="line"># Fashion-MNIST服装分类问题是用于计算机视觉和深度学习的新标准数据集。</span><br><span class="line"># 它是一个数据集，由60,000张28×28像素的小方块灰度图像组成，</span><br><span class="line"># 其中包含10种衣服的商品，例如鞋子，T恤,礼服等等。</span><br><span class="line"># 下面的示例加载数据集，缩放像素值，</span><br><span class="line"># 然后在训练数据集上拟合卷积神经网络，并在测试数据集上评估网络的性能。</span><br><span class="line"># 该示例仅需几分钟即可在现代CPU上运行。 不需要GPU。</span><br><span class="line"># fit a cnn on the fashion mnist dataset</span><br><span class="line">from keras.datasets import fashion_mnist</span><br><span class="line">from keras.utils import to_categorical</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers import Conv2D</span><br><span class="line">from keras.layers import MaxPooling2D</span><br><span class="line">from keras.layers import Dense</span><br><span class="line">from keras.layers import Flatten</span><br><span class="line"></span><br><span class="line"># load dataset</span><br><span class="line">(trainX, trainY), (testX, testY) = fashion_mnist.load_data()</span><br><span class="line"># reshape dataset to have a single channel</span><br><span class="line">trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))</span><br><span class="line">testX = testX.reshape((testX.shape[0], 28, 28, 1))</span><br><span class="line"># convert from integers to floats</span><br><span class="line">trainX, testX = trainX.astype(&apos;float32&apos;), testX.astype(&apos;float32&apos;)  # normalize to range 0-1</span><br><span class="line">trainX, testX = trainX / 255.0, testX / 255.0</span><br><span class="line"># one hot encode target values</span><br><span class="line">trainY, testY = to_categorical(trainY), to_categorical(testY)</span><br><span class="line"># define model</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(32, (3, 3), activation=&apos;relu&apos;, kernel_initializer=&apos;he_uniform&apos;,</span><br><span class="line">                 input_shape=(28, 28, 1)))</span><br><span class="line">model.add(MaxPooling2D())</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(100, activation=&apos;relu&apos;, kernel_initializer=&apos;he_uniform&apos;))</span><br><span class="line">model.add(Dense(10, activation=&apos;softmax&apos;))</span><br><span class="line">model.compile(optimizer=&apos;adam&apos;, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;])</span><br><span class="line"># fit model</span><br><span class="line">model.fit(trainX, trainY, epochs=10, batch_size=32, verbose=2)</span><br><span class="line"># evaluate model</span><br><span class="line">loss, acc = model.evaluate(testX, testY, verbose=0)</span><br><span class="line">print(loss, acc)</span><br><span class="line">model.save(&apos;myModel.model&apos;)</span><br></pre></td></tr></table></figure><p>6:<strong>Image Augmentation</strong> (图片扩充)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In this lesson, you will discover how to use image augmentation. Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images. The Keras deep learning neural network library provides the capability to fit models using image data augmentation via the ImageDataGenerator class.</span><br><span class="line">图像数据扩充是一种可通过在数据集中创建图像的修改版本来人工扩展训练数据集大小的技术。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># example using image augmentation</span><br><span class="line">from numpy import expand_dims</span><br><span class="line">from keras.preprocessing.image import load_img</span><br><span class="line">from keras.preprocessing.image import img_to_array</span><br><span class="line">from keras.preprocessing.image import ImageDataGenerator</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line"></span><br><span class="line"># load the image</span><br><span class="line">img = load_img(&apos;bird.jpg&apos;)  # convert to numpy array</span><br><span class="line">data = img_to_array(img)</span><br><span class="line"># expand dimension to one sample</span><br><span class="line">samples = expand_dims(data, 0)</span><br><span class="line"># create image data augmentation generator</span><br><span class="line">datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)</span><br><span class="line"># prepare iterator</span><br><span class="line">it = datagen.flow(samples, batch_size=1)</span><br><span class="line"># generate samples and plot</span><br><span class="line">for i in range(9):</span><br><span class="line">    # define subplot</span><br><span class="line">    pyplot.subplot(330 + 1 + i)</span><br><span class="line">    # generate batch of images</span><br><span class="line">    batch = it.next()</span><br><span class="line">    # convert to unsigned integers for viewing</span><br><span class="line">    image = batch[0].astype(&apos;uint8&apos;)</span><br><span class="line">    # plot raw pixel data</span><br><span class="line">    pyplot.imshow(image)</span><br><span class="line"># show the figure</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>7:<strong>Face Detection</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In this lesson, you will discover how to use a convolutional neural network for face detection. Face detection is a trivial problem for humans to solve and has been solved reasonably well by classical feature-based techniques, such as the cascade classifier. More recently, deep learning methods have achieved state-of-the-art results on standard face detection datasets. One example is the Multi-task Cascade Convolutional Neural Network,or MTCNN for short. The ipazc/MTCNN project8 provides an open source implementation of the MTCNN that can be installed easily as follows:</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># face detection with mtcnn on a photograph</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line">from matplotlib.patches import Rectangle</span><br><span class="line">from mtcnn.mtcnn import MTCNN</span><br><span class="line"></span><br><span class="line"># load image from file</span><br><span class="line">pixels = pyplot.imread(&apos;yan.jpg&apos;)  # create the detector, using default weights</span><br><span class="line">detector = MTCNN()</span><br><span class="line"># detect faces in the image</span><br><span class="line">faces = detector.detect_faces(pixels)</span><br><span class="line"># plot the image</span><br><span class="line">pyplot.imshow(pixels)</span><br><span class="line"># get the context for drawing boxes</span><br><span class="line">ax = pyplot.gca()</span><br><span class="line"># get coordinates from the first face</span><br><span class="line">x, y, width, height = faces[0][&apos;box&apos;]  # create the shape</span><br><span class="line">rect = Rectangle((x, y), width, height, fill=False, color=&apos;red&apos;)  # draw the box</span><br><span class="line">ax.add_patch(rect)</span><br><span class="line"># show the plot</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Deep-Learning-for-Computer-Vision&quot;&gt;&lt;a href=&quot;#Deep-Learning-for-Computer-Vision&quot; class=&quot;headerlink&quot; title=&quot;Deep Learning for  Compute
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MachineLearningAlgorithms_Python</title>
    <link href="https://yanyubing.xyz/2020/03/26/MachineLearningAlgorithms_Python/"/>
    <id>https://yanyubing.xyz/2020/03/26/MachineLearningAlgorithms_Python/</id>
    <published>2020-03-25T16:28:52.178Z</published>
    <updated>2020-03-27T05:32:31.856Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从头开始实现算法——Python"><a href="#从头开始实现算法——Python" class="headerlink" title="从头开始实现算法——Python"></a>从头开始实现算法——Python</h1><p>1:从头开始加载数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 加载数据，这样加载的问题是可能出现空行</span><br><span class="line"></span><br><span class="line">from csv import reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    file = open(filename, &quot;r&quot;)</span><br><span class="line">    lines = reader(file)</span><br><span class="line">    dataset = list(lines)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># load dataset</span><br><span class="line">filename = &apos;pima-indians-diabetes.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line"></span><br><span class="line">print(&apos;列：&apos;, len(dataset))</span><br><span class="line">print(&apos;行：&apos;, len(dataset[0]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Example of loading Pima Indians CSV dataset</span><br><span class="line">#防止空行</span><br><span class="line">from csv import reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    dataset = list()</span><br><span class="line">    with open(filename, &apos;r&apos;) as file:</span><br><span class="line">        csv_reader = reader(file)</span><br><span class="line">        for row in csv_reader:</span><br><span class="line">            if not row:</span><br><span class="line">                continue</span><br><span class="line">            dataset.append(row)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load dataset</span><br><span class="line">filename = &apos;pima-indians-diabetes.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line">print(&apos;列：&apos;, len(dataset))</span><br><span class="line">print(&apos;行：&apos;, len(dataset[0]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 机器学习中数据一般为数字，浮点型或者整数型,所以需要转换</span><br><span class="line"></span><br><span class="line">from csv import reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_csv(filename):</span><br><span class="line">    # 这里加rb报错</span><br><span class="line">    file = open(filename, &quot;r&quot;)</span><br><span class="line">    lines = reader(file)</span><br><span class="line">    dataset = list(lines)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Convert string column to float</span><br><span class="line">def str_column_to_float(dataset, column):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = float(row[column].strip())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load pima-indians-diabetes dataset</span><br><span class="line">filename = &apos;pima-indians-diabetes.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line">print(dataset[0])</span><br><span class="line"># convert string columns to float</span><br><span class="line">for i in range(len(dataset[0])):</span><br><span class="line">    str_column_to_float(dataset, i)</span><br><span class="line">print(dataset[0])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># 机器学习中，有时候需要结果数据为0,1,2类型，而不是字符串,所以需要转换</span><br><span class="line">from csv import reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    file = open(filename, &quot;r&quot;)</span><br><span class="line">    lines = reader(file)</span><br><span class="line">    dataset = list(lines)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Convert string column to float</span><br><span class="line">def str_column_to_float(dataset, column):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = float(row[column].strip())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Convert string column to integer</span><br><span class="line">def str_column_to_int(dataset, column):</span><br><span class="line">    class_values = [row[column] for row in dataset]</span><br><span class="line">    unique = set(class_values)</span><br><span class="line">    lookup = dict()</span><br><span class="line">    # 这里进行了转换</span><br><span class="line">    for i, value in enumerate(unique):</span><br><span class="line">        lookup[value] = i</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = lookup[row[column]]</span><br><span class="line">    return lookup</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load iris dataset</span><br><span class="line">filename = &apos;iris.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line">print(dataset[0])</span><br><span class="line"># convert string columns to float</span><br><span class="line">for i in range(4):</span><br><span class="line">    # 这里直接使用的函数，因为前面已经实现过</span><br><span class="line">    str_column_to_float(dataset, i)</span><br><span class="line"># convert class column to int</span><br><span class="line">lookup = str_column_to_int(dataset, 4)</span><br><span class="line">print(dataset[0])</span><br><span class="line">print(lookup)</span><br></pre></td></tr></table></figure><p>2:从头开始缩放数据</p><p>规范化数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># 使数据分在0-1之间</span><br><span class="line">from csv import reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    file = open(filename, &quot;r&quot;)</span><br><span class="line">    lines = reader(file)</span><br><span class="line">    dataset = list(lines)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Convert string column to float</span><br><span class="line">def str_column_to_float(dataset, column):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = float(row[column].strip())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Find the min and max values for each column</span><br><span class="line">def dataset_minmax(dataset):</span><br><span class="line">    minmax = list()</span><br><span class="line">    for i in range(len(dataset[0])):</span><br><span class="line">        col_values = [row[i] for row in dataset]</span><br><span class="line">        value_min = min(col_values)</span><br><span class="line">        value_max = max(col_values)</span><br><span class="line">        minmax.append([value_min, value_max])</span><br><span class="line">    return minmax</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Rescale dataset columns to the range 0-1</span><br><span class="line">def normalize_dataset(dataset, minmax):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        for i in range(len(row)):</span><br><span class="line">            # 核心转换步骤</span><br><span class="line">            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load pima-indians-diabetes dataset</span><br><span class="line">filename = &apos;pima-indians-diabetes.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line"># convert string columns to float</span><br><span class="line">for i in range(len(dataset[0])):</span><br><span class="line">    str_column_to_float(dataset, i)</span><br><span class="line">print(dataset[0])</span><br><span class="line"># Calculate min and max for each column</span><br><span class="line">minmax = dataset_minmax(dataset)</span><br><span class="line"># Normalize columns</span><br><span class="line">normalize_dataset(dataset, minmax)</span><br><span class="line">print(dataset[0])</span><br></pre></td></tr></table></figure><p>标准化数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"># 标准化是一种重新缩放技术，是指将数据的分布集中在值0上，标准偏差集中在值1上。</span><br><span class="line">from csv import reader</span><br><span class="line">from math import sqrt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    file = open(filename, &quot;r&quot;)</span><br><span class="line">    lines = reader(file)</span><br><span class="line">    dataset = list(lines)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Convert string column to float</span><br><span class="line">def str_column_to_float(dataset, column):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = float(row[column].strip())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># calculate column means</span><br><span class="line">def column_means(dataset):</span><br><span class="line">    means = [0 for i in range(len(dataset[0]))]</span><br><span class="line">    for i in range(len(dataset[0])):</span><br><span class="line">        col_values = [row[i] for row in dataset]</span><br><span class="line">        means[i] = sum(col_values) / float(len(dataset))</span><br><span class="line">    return means</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># calculate column standard deviations</span><br><span class="line">def column_stdevs(dataset, means):</span><br><span class="line">    stdevs = [0 for i in range(len(dataset[0]))]</span><br><span class="line">    for i in range(len(dataset[0])):</span><br><span class="line">        variance = [pow(row[i] - means[i], 2) for row in dataset]</span><br><span class="line">        stdevs[i] = sum(variance)</span><br><span class="line">    stdevs = [sqrt(x / (float(len(dataset) - 1))) for x in stdevs]</span><br><span class="line">    return stdevs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># standardize dataset</span><br><span class="line">def standardize_dataset(dataset, means, stdevs):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        for i in range(len(row)):</span><br><span class="line">            row[i] = (row[i] - means[i]) / stdevs[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load pima-indians-diabetes dataset</span><br><span class="line">filename = &apos;pima-indians-diabetes.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line"># convert string columns to float</span><br><span class="line">for i in range(len(dataset[0])):</span><br><span class="line">    str_column_to_float(dataset, i)</span><br><span class="line">print(dataset[0])</span><br><span class="line"># Estimate mean and standard deviation</span><br><span class="line">means = column_means(dataset)</span><br><span class="line">stdevs = column_stdevs(dataset, means)</span><br><span class="line"># standardize dataset</span><br><span class="line">standardize_dataset(dataset, means, stdevs)</span><br><span class="line">print(dataset[0])</span><br></pre></td></tr></table></figure><p>何时标准化或者规范化数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Standardization is a scaling technique that assumes your data conforms to a normal distribution.</span><br><span class="line"></span><br><span class="line">If a given data attribute is normal or close to normal, this is probably the scaling method to use.</span><br><span class="line"></span><br><span class="line">It is good practice to record the summary statistics used in the standardization process, so that you can apply them when standardizing data in the future that you may want to use with your model.</span><br><span class="line"></span><br><span class="line">Normalization is a scaling technique that does not assume any specific distribution.</span><br><span class="line"></span><br><span class="line">If your data is not normally distributed, consider normalizing it prior to applying your machine learning algorithm.</span><br><span class="line"></span><br><span class="line">It is good practice to record the minimum and maximum values for each column used in the normalization process, again, in case you need to normalize new data in the future to be used with your model.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Extensions</span><br><span class="line">There are many other data transforms you could apply.</span><br><span class="line"></span><br><span class="line">The idea of data transforms is to best expose the structure of your problem in your data to the learning algorithm.</span><br><span class="line"></span><br><span class="line">It may not be clear what transforms are required upfront. A combination of trial and error and exploratory data analysis (plots and stats) can help tease out what may work.</span><br><span class="line"></span><br><span class="line">Below are some additional transforms you may want to consider researching and implementing:</span><br><span class="line"></span><br><span class="line">Normalization that permits a configurable range, such as -1 to 1 and more.</span><br><span class="line">Standardization that permits a configurable spread, such as 1, 2 or more standard deviations from the mean.</span><br><span class="line">Exponential transforms such as logarithm, square root and exponents.</span><br><span class="line">Power transforms such as box-cox for fixing the skew in normally distributed data.</span><br></pre></td></tr></table></figure><p>3:从头开始重采样技术</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The goal of predictive modeling is to create models that make good predictions on new data.</span><br><span class="line"></span><br><span class="line">We don’t have access to this new data at the time of training, so we must use statistical methods to estimate the performance of a model on new data.</span><br></pre></td></tr></table></figure><p>训练集和测试集的拆分</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Split a dataset into a train and test set</span><br><span class="line">def train_test_split(dataset, split=0.60):</span><br><span class="line">    train = list()</span><br><span class="line">    train_size = split * len(dataset)</span><br><span class="line">    dataset_copy = list(dataset)</span><br><span class="line">    while len(train) &lt; train_size:</span><br><span class="line">        index = randrange(len(dataset_copy))</span><br><span class="line">        train.append(dataset_copy.pop(index))</span><br><span class="line">    return train, dataset_copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># test train/test split</span><br><span class="line"># 随机种子，随机种子相同时，结果相同</span><br><span class="line">seed(1)</span><br><span class="line">dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]</span><br><span class="line">train, test = train_test_split(dataset)</span><br><span class="line">print(train)</span><br><span class="line">print(test)</span><br></pre></td></tr></table></figure><p>k倍交叉验证拆分</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 直接切分可能参数噪声干扰偏差，所有采用K倍交叉验证的方式切分数据集</span><br><span class="line"># 1:将数据集切分成k份，训练集为k-1的部分，测试集为余下的第k份；</span><br><span class="line"># 重复此过程，保证每一组都可以成为验证集</span><br><span class="line"># 数据集大的时候采用10倍交叉验证，数据集小采用3倍交叉验证</span><br><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Split a dataset into k folds</span><br><span class="line">def cross_validation_split(dataset, folds=3):</span><br><span class="line">    dataset_split = list()</span><br><span class="line">    dataset_copy = list(dataset)</span><br><span class="line">    fold_size = int(len(dataset) / folds)</span><br><span class="line">    for i in range(folds):</span><br><span class="line">        fold = list()</span><br><span class="line">        while len(fold) &lt; fold_size:</span><br><span class="line">            index = randrange(len(dataset_copy))</span><br><span class="line">            fold.append(dataset_copy.pop(index))</span><br><span class="line">        dataset_split.append(fold)</span><br><span class="line">    return dataset_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># test cross validation split</span><br><span class="line">seed(1)</span><br><span class="line">dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]</span><br><span class="line">folds = cross_validation_split(dataset, 4)</span><br><span class="line">print(folds)</span><br></pre></td></tr></table></figure><p>怎么选择重采样模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据小，执行速度快就采用切分方式；数据集大，需要精度高，则采用K倍交叉验证</span><br></pre></td></tr></table></figure><p>4：从头开始学习指标</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用于评估模型的好坏</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：准确度</span><br><span class="line">2：混淆矩阵</span><br><span class="line">3：平均绝对误差</span><br><span class="line">4：均方误差</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 1:准确度</span><br><span class="line"># 准确的结果值与所有预测的比值</span><br><span class="line"># Calculate accuracy percentage between two lists</span><br><span class="line"></span><br><span class="line">def accuracy_metric(actual, predicted):</span><br><span class="line">    correct = 0</span><br><span class="line">    for i in range(len(actual)):</span><br><span class="line">        if actual[i] == predicted[i]:</span><br><span class="line">            correct += 1</span><br><span class="line">    return correct / float(len(actual)) * 100.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Test accuracy</span><br><span class="line">actual = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]</span><br><span class="line">predicted = [0, 1, 0, 0, 0, 1, 0, 1, 1, 1]</span><br><span class="line">accuracy = accuracy_metric(actual, predicted)</span><br><span class="line">print(accuracy)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">2:混淆矩阵</span><br><span class="line"># Example of Calculating and Displaying a Pretty Confusion Matrix</span><br><span class="line"></span><br><span class="line"># calculate a confusion matrix</span><br><span class="line">def confusion_matrix(actual, predicted):</span><br><span class="line">    unique = set(actual)</span><br><span class="line">    matrix = [list() for x in range(len(unique))]</span><br><span class="line">    for i in range(len(unique)):</span><br><span class="line">        matrix[i] = [0 for x in range(len(unique))]</span><br><span class="line">    lookup = dict()</span><br><span class="line">    for i, value in enumerate(unique):</span><br><span class="line">        lookup[value] = i</span><br><span class="line">    for i in range(len(actual)):</span><br><span class="line">        x = lookup[actual[i]]</span><br><span class="line">        y = lookup[predicted[i]]</span><br><span class="line">        matrix[y][x] += 1</span><br><span class="line">    return unique, matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># pretty print a confusion matrix</span><br><span class="line">def print_confusion_matrix(unique, matrix):</span><br><span class="line">    print(&apos;(A)&apos; + &apos; &apos;.join(str(x) for x in unique))</span><br><span class="line">    print(&apos;(P)---&apos;)</span><br><span class="line">    for i, x in enumerate(unique):</span><br><span class="line">        print(&quot;%s| %s&quot; % (x, &apos; &apos;.join(str(x) for x in matrix[i])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Test confusion matrix with integers</span><br><span class="line">actual = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]</span><br><span class="line">predicted = [0, 1, 1, 0, 0, 1, 0, 1, 1, 1]</span><br><span class="line">unique, matrix = confusion_matrix(actual, predicted)</span><br><span class="line">print_confusion_matrix(unique, matrix)</span><br><span class="line"></span><br><span class="line">输出结果为：</span><br><span class="line">(A)0 1</span><br><span class="line">(P)---</span><br><span class="line">0| 3 1</span><br><span class="line">1| 2 4</span><br><span class="line"></span><br><span class="line">A表示准确值，p表示预测值</span><br><span class="line">上面矩阵表达的是：实际为0，预测为0的个数为3，实际为1，预测为1的个数为4；实际为0，但是预测为0的个数是1，实际为0，但是预测为1的个数为2。即预测准确的为7个，预测错误的为3个。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 平均绝对误差</span><br><span class="line"># MAE = sum( abs(predicted_i - actual_i) ) / total predictions</span><br><span class="line"></span><br><span class="line"># Calculate mean absolute error</span><br><span class="line">def mae_metric(actual, predicted):</span><br><span class="line">    sum_error = 0.0</span><br><span class="line">    for i in range(len(actual)):</span><br><span class="line">        sum_error += abs(predicted[i] - actual[i])</span><br><span class="line">    return sum_error / float(len(actual))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Test RMSE</span><br><span class="line">actual = [0.1, 0.2, 0.3, 0.4, 0.5]</span><br><span class="line">predicted = [0.11, 0.19, 0.29, 0.41, 0.5]</span><br><span class="line">mae = mae_metric(actual, predicted)</span><br><span class="line">print(mae)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 均方根误差</span><br><span class="line"># RMSE = sqrt( sum( (predicted_i - actual_i)^2 ) / total predictions)</span><br><span class="line"></span><br><span class="line">from math import sqrt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Calculate root mean squared error</span><br><span class="line">def rmse_metric(actual, predicted):</span><br><span class="line">    sum_error = 0.0</span><br><span class="line">    for i in range(len(actual)):</span><br><span class="line">        prediction_error = predicted[i] - actual[i]</span><br><span class="line">        sum_error += (prediction_error ** 2)</span><br><span class="line">    mean_error = sum_error / float(len(actual))</span><br><span class="line">    return sqrt(mean_error)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Test RMSE</span><br><span class="line">actual = [0.1, 0.2, 0.3, 0.4, 0.5]</span><br><span class="line">predicted = [0.11, 0.19, 0.29, 0.41, 0.5]</span><br><span class="line">rmse = rmse_metric(actual, predicted)</span><br><span class="line">print(rmse)</span><br></pre></td></tr></table></figure><p>5：基线算法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基线算法：假设基线算法得到的误差值为20%，那么你用到的算法误差值则需要比20%小，才说明算法有用！</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Random Prediction Algorithm</span><br><span class="line">2. Zero Rule Algorithm</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">These steps will provide the foundations you need to handle implementing and calculating baseline performance for your machine learning algorithms.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1. Random Prediction Algorithm</span><br><span class="line"></span><br><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Generate random predictions</span><br><span class="line">def random_algorithm(train, test):</span><br><span class="line">    output_values = [row[-1] for row in train]</span><br><span class="line">    unique = list(set(output_values))</span><br><span class="line">    predicted = list()</span><br><span class="line">    for row in test:</span><br><span class="line">        index = randrange(len(unique))</span><br><span class="line">        predicted.append(unique[index])</span><br><span class="line">    return predicted</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">seed(1)</span><br><span class="line">train = [[0], [1], [0], [1], [0], [1]]</span><br><span class="line">test = [[None], [None], [None], [None]]</span><br><span class="line">predictions = random_algorithm(train, test)</span><br><span class="line">print(predictions)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># The Zero Rule Algorithm is a better baseline than the random algorithm.</span><br><span class="line"># It uses more information about a given problem to create one rule in order to make predictions. This rule is different depending on the problem type.</span><br><span class="line"></span><br><span class="line">2. Zero Rule Algorithm  classification</span><br><span class="line"></span><br><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># zero rule algorithm for classification</span><br><span class="line">def zero_rule_algorithm_classification(train, test):</span><br><span class="line">    output_values = [row[-1] for row in train]</span><br><span class="line">    prediction = max(set(output_values), key=output_values.count)</span><br><span class="line">    predicted = [prediction for i in range(len(train))]</span><br><span class="line">    return predicted</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">seed(1)</span><br><span class="line">train = [[&apos;0&apos;], [&apos;0&apos;], [&apos;0&apos;], [&apos;0&apos;], [&apos;1&apos;], [&apos;1&apos;]]</span><br><span class="line">test = [[None], [None], [None], [None]]</span><br><span class="line">predictions = zero_rule_algorithm_classification(train, test)</span><br><span class="line">print(predictions)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line"># 回归问题，预测平均值</span><br><span class="line"></span><br><span class="line"># zero rule algorithm for regression</span><br><span class="line">def zero_rule_algorithm_regression(train, test):</span><br><span class="line">    output_values = [row[-1] for row in train]</span><br><span class="line">    prediction = sum(output_values) / float(len(output_values))</span><br><span class="line">    predicted = [prediction for i in range(len(test))]</span><br><span class="line">    return predicted</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">seed(1)</span><br><span class="line">train = [[10], [15], [12], [15], [18], [20]]</span><br><span class="line">test = [[None], [None], [None], [None]]</span><br><span class="line">predictions = zero_rule_algorithm_regression(train, test)</span><br><span class="line">print(predictions)</span><br></pre></td></tr></table></figure><p>6：从头开始编写测试工具</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">目的：为了检测算法的性能</span><br><span class="line">1:The resampling method to split-up the dataset.</span><br><span class="line">重采样切分数据集</span><br><span class="line">2:The machine learning algorithm to evaluate.</span><br><span class="line">算法的计算</span><br><span class="line">3:The performance measure by which to evaluate predictions.</span><br><span class="line">用于评估预测的绩效指标</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">测试工具必须允许对不同的机器学习算法进行评估，同时数据集，重采样方法和性能指标应保持不变。</span><br><span class="line">测试工具分为以下两类，对应不同的重采样方法!</span><br><span class="line">区别是：第一种得到的是准确度，而第二种得到的是平均精度（因为会进行多次验证，保证所有的第k份都为测试集）</span><br><span class="line">1:Train-Test Algorithm Test Harness.</span><br><span class="line">2:Cross-Validation Algorithm Test Harness.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">#1: Train-Test Test Harness</span><br><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line">from csv import reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line"># 加载文件</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    file = open(filename, &quot;r&quot;)</span><br><span class="line">    lines = reader(file)</span><br><span class="line">    dataset = list(lines)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 字符串转换</span><br><span class="line"># Convert string column to float</span><br><span class="line">def str_column_to_float(dataset, column):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = float(row[column].strip())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据集的切割</span><br><span class="line"># Split a dataset into a train and test set</span><br><span class="line">def train_test_split(dataset, split):</span><br><span class="line">    train = list()</span><br><span class="line">    train_size = split * len(dataset)</span><br><span class="line">    dataset_copy = list(dataset)</span><br><span class="line">    while len(train) &lt; train_size:</span><br><span class="line">        index = randrange(len(dataset_copy))</span><br><span class="line">        train.append(dataset_copy.pop(index))</span><br><span class="line">    return train, dataset_copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 计算准确度</span><br><span class="line"># Calculate accuracy percentage</span><br><span class="line">def accuracy_metric(actual, predicted):</span><br><span class="line">    correct = 0</span><br><span class="line">    for i in range(len(actual)):</span><br><span class="line">        if actual[i] == predicted[i]:</span><br><span class="line">            correct += 1</span><br><span class="line">    return correct / float(len(actual)) * 100.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用train/split切割评估算法</span><br><span class="line"># Evaluate an algorithm using a train/test split</span><br><span class="line"></span><br><span class="line"># 使用到的函数有切割(),算法函数,准确度评估函数</span><br><span class="line">def evaluate_algorithm(dataset, algorithm, split, *args):</span><br><span class="line">    train, test = train_test_split(dataset, split)</span><br><span class="line">    test_set = list()</span><br><span class="line">    for row in test:</span><br><span class="line">        row_copy = list(row)</span><br><span class="line">        row_copy[-1] = None</span><br><span class="line">        test_set.append(row_copy)</span><br><span class="line">    predicted = algorithm(train, test_set, *args)</span><br><span class="line">    actual = [row[-1] for row in test]</span><br><span class="line">    accuracy = accuracy_metric(actual, predicted)</span><br><span class="line">    return accuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分类问题的0规则算法</span><br><span class="line"># zero rule algorithm for classification</span><br><span class="line">def zero_rule_algorithm_classification(train, test):</span><br><span class="line">    output_values = [row[-1] for row in train]</span><br><span class="line">    prediction = max(set(output_values), key=output_values.count)</span><br><span class="line">    predicted = [prediction for i in range(len(test))]</span><br><span class="line">    return predicted</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Test the zero rule algorithm on the diabetes dataset</span><br><span class="line">seed(1)</span><br><span class="line"># load and prepare data</span><br><span class="line">filename = &apos;pima-indians-diabetes.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line">for i in range(len(dataset[0])):</span><br><span class="line">    str_column_to_float(dataset, i)</span><br><span class="line"># evaluate algorithm</span><br><span class="line">split = 0.6</span><br><span class="line"># 评估0规则的分类算法</span><br><span class="line">accuracy = evaluate_algorithm(dataset, zero_rule_algorithm_classification, split)</span><br><span class="line">print(&apos;Accuracy: %.3f%%&apos; % (accuracy))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"># 2：Cross-Validation Algorithm Test Harness</span><br><span class="line"></span><br><span class="line"># Cross Validation Test Harness</span><br><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line">from csv import reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    file = open(filename, &quot;r&quot;)</span><br><span class="line">    lines = reader(file)</span><br><span class="line">    dataset = list(lines)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Convert string column to float</span><br><span class="line">def str_column_to_float(dataset, column):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = float(row[column].strip())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Split a dataset into k folds，使用K被交叉方式切割</span><br><span class="line">def cross_validation_split(dataset, n_folds):</span><br><span class="line">    dataset_split = list()</span><br><span class="line">    dataset_copy = list(dataset)</span><br><span class="line">    fold_size = int(len(dataset) / n_folds)</span><br><span class="line">    for i in range(n_folds):</span><br><span class="line">        fold = list()</span><br><span class="line">        while len(fold) &lt; fold_size:</span><br><span class="line">            index = randrange(len(dataset_copy))</span><br><span class="line">            fold.append(dataset_copy.pop(index))</span><br><span class="line">        dataset_split.append(fold)</span><br><span class="line">    return dataset_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Calculate accuracy percentage</span><br><span class="line">def accuracy_metric(actual, predicted):</span><br><span class="line">    correct = 0</span><br><span class="line">    for i in range(len(actual)):</span><br><span class="line">        if actual[i] == predicted[i]:</span><br><span class="line">            correct += 1</span><br><span class="line">    return correct / float(len(actual)) * 100.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Evaluate an algorithm using a cross validation split</span><br><span class="line">def evaluate_algorithm(dataset, algorithm, n_folds, *args):</span><br><span class="line">    folds = cross_validation_split(dataset, n_folds)</span><br><span class="line">    scores = list()</span><br><span class="line">    for fold in folds:</span><br><span class="line">        train_set = list(folds)</span><br><span class="line">        train_set.remove(fold)</span><br><span class="line">        train_set = sum(train_set, [])</span><br><span class="line">        test_set = list()</span><br><span class="line">        for row in fold:</span><br><span class="line">            row_copy = list(row)</span><br><span class="line">            test_set.append(row_copy)</span><br><span class="line">            row_copy[-1] = None</span><br><span class="line">        predicted = algorithm(train_set, test_set, *args)</span><br><span class="line">        actual = [row[-1] for row in fold]</span><br><span class="line">        accuracy = accuracy_metric(actual, predicted)</span><br><span class="line">        scores.append(accuracy)</span><br><span class="line">    return scores</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># zero rule algorithm for classification</span><br><span class="line">def zero_rule_algorithm_classification(train, test):</span><br><span class="line">    output_values = [row[-1] for row in train]</span><br><span class="line">    prediction = max(set(output_values), key=output_values.count)</span><br><span class="line">    predicted = [prediction for i in range(len(test))]</span><br><span class="line">    return predicted</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Test the zero rule algorithm on the diabetes dataset</span><br><span class="line">seed(1)</span><br><span class="line"># load and prepare data</span><br><span class="line">filename = &apos;pima-indians-diabetes.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line">for i in range(len(dataset[0])):</span><br><span class="line">    str_column_to_float(dataset, i)</span><br><span class="line"># evaluate algorithm</span><br><span class="line">n_folds = 5</span><br><span class="line">scores = evaluate_algorithm(dataset, zero_rule_algorithm_classification, n_folds)</span><br><span class="line">print(&apos;Scores: %s&apos; % scores)</span><br><span class="line">print(&apos;Mean Accuracy: %.3f%%&apos; % (sum(scores) / len(scores)))</span><br></pre></td></tr></table></figure><p>7：从头开始实现线性回归</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 简单线性回归,目标就是计算B0和B1的值</span><br><span class="line"># y = b0 + b1 * x</span><br><span class="line"># B1 = sum((x(i) - mean(x)) * (y(i) - mean(y))) / sum( (x(i) - mean(x))^2 )</span><br><span class="line"># B0 = mean(y) - B1 * mean(x)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">步骤为：</span><br><span class="line">1:Calculate Mean and Variance.</span><br><span class="line">2:Calculate Covariance.</span><br><span class="line">3:Estimate Coefficients.</span><br><span class="line">4:Make Predictions.</span><br><span class="line">5:Predict Insurance.</span><br><span class="line">计算均值和方差。</span><br><span class="line">计算协方差。</span><br><span class="line">估计系数。</span><br><span class="line">作出预测。</span><br><span class="line">预测保险。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"># 简单线性回归，目标就是求出B1和B0</span><br><span class="line"># y = b0 + b1 * x</span><br><span class="line"># B1 = sum((x(i) - mean(x)) * (y(i) - mean(y))) / sum( (x(i) - mean(x))^2 )</span><br><span class="line"># B0 = mean(y) - B1 * mean(x)</span><br><span class="line"></span><br><span class="line"># 计算均值和方差。</span><br><span class="line"># mean(x) = sum(x) / count(x)</span><br><span class="line"># variance = sum( (x - mean(x))^2 )</span><br><span class="line"></span><br><span class="line"># 计算协方差。</span><br><span class="line"># covariance = sum((x(i) - mean(x)) * (y(i) - mean(y)))</span><br><span class="line"></span><br><span class="line"># 估计系数。</span><br><span class="line"># B1 = sum((x(i) - mean(x)) * (y(i) - mean(y))) / sum( (x(i) - mean(x))^2 )</span><br><span class="line"># B1 = covariance(x, y) / variance(x)</span><br><span class="line"># B0 = mean(y) - B1 * mean(x)</span><br><span class="line"></span><br><span class="line"># 作出预测。</span><br><span class="line"># y = b0 + b1 * x</span><br><span class="line"></span><br><span class="line"># 预测保险。</span><br><span class="line"># RMSE 均方根误差</span><br><span class="line"></span><br><span class="line"># Simple Linear Regression on the Swedish Insurance Dataset</span><br><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line">from csv import reader</span><br><span class="line">from math import sqrt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    dataset = list()</span><br><span class="line">    with open(filename, &apos;r&apos;) as file:</span><br><span class="line">        csv_reader = reader(file)</span><br><span class="line">        for row in csv_reader:</span><br><span class="line">            if not row:</span><br><span class="line">                continue</span><br><span class="line">            dataset.append(row)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Convert string column to float</span><br><span class="line">def str_column_to_float(dataset, column):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = float(row[column].strip())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Split a dataset into a train and test set</span><br><span class="line">def train_test_split(dataset, split):</span><br><span class="line">    train = list()</span><br><span class="line">    train_size = split * len(dataset)</span><br><span class="line">    dataset_copy = list(dataset)</span><br><span class="line">    while len(train) &lt; train_size:</span><br><span class="line">        index = randrange(len(dataset_copy))</span><br><span class="line">        train.append(dataset_copy.pop(index))</span><br><span class="line">    return train, dataset_copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Calculate root mean squared error</span><br><span class="line">def rmse_metric(actual, predicted):</span><br><span class="line">    sum_error = 0.0</span><br><span class="line">    for i in range(len(actual)):</span><br><span class="line">        prediction_error = predicted[i] - actual[i]</span><br><span class="line">        sum_error += (prediction_error ** 2)</span><br><span class="line">    mean_error = sum_error / float(len(actual))</span><br><span class="line">    return sqrt(mean_error)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Evaluate an algorithm using a train/test split</span><br><span class="line">def evaluate_algorithm(dataset, algorithm, split, *args):</span><br><span class="line">    train, test = train_test_split(dataset, split)</span><br><span class="line">    test_set = list()</span><br><span class="line">    for row in test:</span><br><span class="line">        row_copy = list(row)</span><br><span class="line">        row_copy[-1] = None</span><br><span class="line">        test_set.append(row_copy)</span><br><span class="line">    predicted = algorithm(train, test_set, *args)</span><br><span class="line">    actual = [row[-1] for row in test]</span><br><span class="line">    rmse = rmse_metric(actual, predicted)</span><br><span class="line">    return rmse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Calculate the mean value of a list of numbers</span><br><span class="line">def mean(values):</span><br><span class="line">    return sum(values) / float(len(values))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Calculate covariance between x and y</span><br><span class="line">def covariance(x, mean_x, y, mean_y):</span><br><span class="line">    covar = 0.0</span><br><span class="line">    for i in range(len(x)):</span><br><span class="line">        covar += (x[i] - mean_x) * (y[i] - mean_y)</span><br><span class="line">    return covar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Calculate the variance of a list of numbers</span><br><span class="line">def variance(values, mean):</span><br><span class="line">    return sum([(x - mean) ** 2 for x in values])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Calculate coefficients，返回B0和B1</span><br><span class="line">def coefficients(dataset):</span><br><span class="line">    x = [row[0] for row in dataset]</span><br><span class="line">    y = [row[1] for row in dataset]</span><br><span class="line">    x_mean, y_mean = mean(x), mean(y)</span><br><span class="line">    b1 = covariance(x, x_mean, y, y_mean) / variance(x, x_mean)</span><br><span class="line">    b0 = y_mean - b1 * x_mean</span><br><span class="line">    return [b0, b1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Simple linear regression algorithm</span><br><span class="line"># 训练集最终得到参数B0和B1，得到方程，并且进行预测，返回值为预测值</span><br><span class="line">def simple_linear_regression(train, test):</span><br><span class="line">    predictions = list()</span><br><span class="line">    b0, b1 = coefficients(train)</span><br><span class="line">    for row in test:</span><br><span class="line">        yhat = b0 + b1 * row[0]</span><br><span class="line">        predictions.append(yhat)</span><br><span class="line">    return predictions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Simple linear regression on insurance dataset</span><br><span class="line">seed(1)</span><br><span class="line"># load and prepare data</span><br><span class="line">filename = &apos;insurance.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line">for i in range(len(dataset[0])):</span><br><span class="line">    str_column_to_float(dataset, i)</span><br><span class="line"># evaluate algorithm</span><br><span class="line">split = 0.6</span><br><span class="line">rmse = evaluate_algorithm(dataset, simple_linear_regression, split)</span><br><span class="line">print(&apos;RMSE: %.3f&apos; % (rmse))</span><br></pre></td></tr></table></figure><p>8：从头开始实现逻辑回归</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">How to make predictions with a logistic regression model.</span><br><span class="line">How to estimate coefficients using stochastic gradient descent.</span><br><span class="line">How to apply logistic regression to a real prediction problem.</span><br><span class="line">如何使用逻辑回归模型进行预测。</span><br><span class="line">如何使用随机梯度下降法估算系数。</span><br><span class="line">如何将逻辑回归应用于实际的预测问题。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"># A key difference from linear regression is that the output value being modeled is a binary value (0 or 1) rather than a numeric value.</span><br><span class="line"># 逻辑回归的输出值是0-1之间的数，和线性输出的整数值不一样</span><br><span class="line"># yhat = e^(b0 + b1 * x1) / (1 + e^(b0 + b1 * x1))</span><br><span class="line"># yhat = 1.0 / (1.0 + e^(-(b0 + b1 * x1)))</span><br><span class="line"></span><br><span class="line"># Stochastic Gradient Descent（随机梯度下降）</span><br><span class="line"># Gradient Descent is the process of minimizing a function by following the gradients of the cost function.</span><br><span class="line"># This involves knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction, e.g. downhill towards the minimum value.</span><br><span class="line"># In machine learning, we can use a technique that evaluates and updates the coefficients every iteration called stochastic gradient descent to minimize the error of a model on our training data.</span><br><span class="line"># The way this optimization algorithm works is that each training instance is shown to the model one at a time. The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction.</span><br><span class="line"># This procedure can be used to find the set of coefficients in a model that result in the smallest error for the model on the training data. Each iteration, the coefficients (b) in machine learning language are updated using the equation:</span><br><span class="line"></span><br><span class="line"># 最终:b = b + learning_rate * (y - yhat) * yhat * (1 - yhat) * x</span><br><span class="line"></span><br><span class="line"># 1:Making Predictions.（做出预测）</span><br><span class="line"># 2:Estimating Coefficients.（求出参数）</span><br><span class="line"># 3:Diabetes Prediction.（预测）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Logistic Regression on Diabetes Dataset</span><br><span class="line">from random import seed</span><br><span class="line">from random import randrange</span><br><span class="line">from csv import reader</span><br><span class="line">from math import exp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load a CSV file</span><br><span class="line"># 读取文件</span><br><span class="line">def load_csv(filename):</span><br><span class="line">    dataset = list()</span><br><span class="line">    with open(filename, &apos;r&apos;) as file:</span><br><span class="line">        csv_reader = reader(file)</span><br><span class="line">        for row in csv_reader:</span><br><span class="line">            if not row:</span><br><span class="line">                continue</span><br><span class="line">            dataset.append(row)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 字符串转换</span><br><span class="line"># Convert string column to float</span><br><span class="line">def str_column_to_float(dataset, column):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        row[column] = float(row[column].strip())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 找到最大值和最小值，为了进行转换</span><br><span class="line"># Find the min and max values for each column</span><br><span class="line">def dataset_minmax(dataset):</span><br><span class="line">    minmax = list()</span><br><span class="line">    for i in range(len(dataset[0])):</span><br><span class="line">        col_values = [row[i] for row in dataset]</span><br><span class="line">        value_min = min(col_values)</span><br><span class="line">        value_max = max(col_values)</span><br><span class="line">        minmax.append([value_min, value_max])</span><br><span class="line">    return minmax</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据转换</span><br><span class="line"># Rescale dataset columns to the range 0-1</span><br><span class="line">def normalize_dataset(dataset, minmax):</span><br><span class="line">    for row in dataset:</span><br><span class="line">        for i in range(len(row)):</span><br><span class="line">            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用K倍交叉验证</span><br><span class="line"># Split a dataset into k folds</span><br><span class="line">def cross_validation_split(dataset, n_folds):</span><br><span class="line">    dataset_split = list()</span><br><span class="line">    dataset_copy = list(dataset)</span><br><span class="line">    fold_size = int(len(dataset) / n_folds)</span><br><span class="line">    for i in range(n_folds):</span><br><span class="line">        fold = list()</span><br><span class="line">        while len(fold) &lt; fold_size:</span><br><span class="line">            index = randrange(len(dataset_copy))</span><br><span class="line">            fold.append(dataset_copy.pop(index))</span><br><span class="line">        dataset_split.append(fold)</span><br><span class="line">    return dataset_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 计算模型准确度</span><br><span class="line"># Calculate accuracy percentage</span><br><span class="line">def accuracy_metric(actual, predicted):</span><br><span class="line">    correct = 0</span><br><span class="line">    for i in range(len(actual)):</span><br><span class="line">        if actual[i] == predicted[i]:</span><br><span class="line">            correct += 1</span><br><span class="line">    return correct / float(len(actual)) * 100.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 计算算法的平均准确度</span><br><span class="line"># Evaluate an algorithm using a cross validation split</span><br><span class="line">def evaluate_algorithm(dataset, algorithm, n_folds, *args):</span><br><span class="line">    folds = cross_validation_split(dataset, n_folds)</span><br><span class="line">    scores = list()</span><br><span class="line">    for fold in folds:</span><br><span class="line">        train_set = list(folds)</span><br><span class="line">        train_set.remove(fold)</span><br><span class="line">        train_set = sum(train_set, [])</span><br><span class="line">        test_set = list()</span><br><span class="line">        for row in fold:</span><br><span class="line">            row_copy = list(row)</span><br><span class="line">            test_set.append(row_copy)</span><br><span class="line">            row_copy[-1] = None</span><br><span class="line">        predicted = algorithm(train_set, test_set, *args)</span><br><span class="line">        actual = [row[-1] for row in fold]</span><br><span class="line">        accuracy = accuracy_metric(actual, predicted)</span><br><span class="line">        scores.append(accuracy)</span><br><span class="line">    return scores</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 通过参数进行预测</span><br><span class="line"># Make a prediction with coefficients</span><br><span class="line">def predict(row, coefficients):</span><br><span class="line">    yhat = coefficients[0]</span><br><span class="line">    for i in range(len(row) - 1):</span><br><span class="line">        yhat += coefficients[i + 1] * row[i]</span><br><span class="line">    return 1.0 / (1.0 + exp(-yhat))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用随机梯度下降，估算模型的参数</span><br><span class="line"># Estimate logistic regression coefficients using stochastic gradient descent</span><br><span class="line">def coefficients_sgd(train, l_rate, n_epoch):</span><br><span class="line">    coef = [0.0 for i in range(len(train[0]))]</span><br><span class="line">    for epoch in range(n_epoch):</span><br><span class="line">        for row in train:</span><br><span class="line">            yhat = predict(row, coef)</span><br><span class="line">            error = row[-1] - yhat</span><br><span class="line">            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)</span><br><span class="line">            for i in range(len(row) - 1):</span><br><span class="line">                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]</span><br><span class="line">    return coef</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 线性回归使用随机梯度下降</span><br><span class="line"># Linear Regression Algorithm With Stochastic Gradient Descent</span><br><span class="line">def logistic_regression(train, test, l_rate, n_epoch):</span><br><span class="line">    predictions = list()</span><br><span class="line">    coef = coefficients_sgd(train, l_rate, n_epoch)</span><br><span class="line">    for row in test:</span><br><span class="line">        yhat = predict(row, coef)</span><br><span class="line">        yhat = round(yhat)</span><br><span class="line">        predictions.append(yhat)</span><br><span class="line">    return (predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Test the logistic regression algorithm on the diabetes dataset</span><br><span class="line">seed(1)</span><br><span class="line"># load and prepare data</span><br><span class="line">filename = &apos;pima-indians-diabetes.csv&apos;</span><br><span class="line">dataset = load_csv(filename)</span><br><span class="line">for i in range(len(dataset[0])):</span><br><span class="line">    str_column_to_float(dataset, i)</span><br><span class="line"># normalize</span><br><span class="line">minmax = dataset_minmax(dataset)</span><br><span class="line">normalize_dataset(dataset, minmax)</span><br><span class="line"># evaluate algorithm</span><br><span class="line">n_folds = 5</span><br><span class="line">l_rate = 0.1</span><br><span class="line">n_epoch = 100</span><br><span class="line">scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)</span><br><span class="line">print(&apos;Scores: %s&apos; % scores)</span><br><span class="line">print(&apos;Mean Accuracy: %.3f%%&apos; % (sum(scores) / float(len(scores))))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从头开始实现算法——Python&quot;&gt;&lt;a href=&quot;#从头开始实现算法——Python&quot; class=&quot;headerlink&quot; title=&quot;从头开始实现算法——Python&quot;&gt;&lt;/a&gt;从头开始实现算法——Python&lt;/h1&gt;&lt;p&gt;1:从头开始加载数据&lt;/p&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MachineLearningAlgorithms</title>
    <link href="https://yanyubing.xyz/2020/03/25/MachineLearningAlgorithms/"/>
    <id>https://yanyubing.xyz/2020/03/25/MachineLearningAlgorithms/</id>
    <published>2020-03-24T18:37:28.530Z</published>
    <updated>2020-03-25T16:28:38.675Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Machine-Learning-Algorithms"><a href="#Machine-Learning-Algorithms" class="headerlink" title="Machine Learning Algorithms"></a><strong>Machine Learning Algorithms</strong></h3><h4 id="1：A-Tour-of-Machine-Learning-Algorithms-机器学习算法简介"><a href="#1：A-Tour-of-Machine-Learning-Algorithms-机器学习算法简介" class="headerlink" title="1：A Tour of Machine Learning Algorithms(机器学习算法简介)"></a>1：A Tour of Machine Learning Algorithms(机器学习算法简介)</h4><p>1.1:Algorithms Grouped by Learning Style</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. Supervised Learning</span><br><span class="line">2. Unsupervised Learning</span><br><span class="line">3. Semi-Supervised Learning</span><br></pre></td></tr></table></figure><p>1.2:Algorithms Grouped By Similarity</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Regression Algorithms:回归算法</span><br><span class="line"></span><br><span class="line">Ordinary Least Squares Regression (OLSR)</span><br><span class="line">Linear Regression</span><br><span class="line">Logistic Regression</span><br><span class="line">Stepwise Regression</span><br><span class="line">Multivariate Adaptive Regression Splines (MARS)</span><br><span class="line">Locally Estimated Scatterplot Smoothing (LOESS)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Instance-based Algorithms:</span><br><span class="line"></span><br><span class="line">k-Nearest Neighbor (kNN)</span><br><span class="line">Learning Vector Quantization (LVQ)</span><br><span class="line">Self-Organizing Map (SOM)</span><br><span class="line">Locally Weighted Learning (LWL)</span><br><span class="line">Support Vector Machines (SVM)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Regularization Algorithms:(正则化)</span><br><span class="line"></span><br><span class="line">Ridge Regression</span><br><span class="line">Least Absolute Shrinkage and Selection Operator (LASSO)</span><br><span class="line">Elastic Net</span><br><span class="line">Least-Angle Regression (LARS)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Decision Tree Algorithms:(决策树算法)</span><br><span class="line"></span><br><span class="line">Classification and Regression Tree (CART)</span><br><span class="line">Iterative Dichotomiser 3 (ID3)</span><br><span class="line">C4.5 and C5.0 (different versions of a powerful approach)</span><br><span class="line">Chi-squared Automatic Interaction Detection (CHAID)</span><br><span class="line">Decision Stump</span><br><span class="line">M5</span><br><span class="line">Conditional Decision Trees</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Bayesian Algorithms:(贝叶斯算法)</span><br><span class="line"></span><br><span class="line">Naive Bayes</span><br><span class="line">Gaussian Naive Bayes</span><br><span class="line">Multinomial Naive Bayes</span><br><span class="line">Averaged One-Dependence Estimators (AODE)</span><br><span class="line">Bayesian Belief Network (BBN)</span><br><span class="line">Bayesian Network (BN)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Clustering Algorithms:</span><br><span class="line"></span><br><span class="line">k-Means</span><br><span class="line">k-Medians</span><br><span class="line">Expectation Maximisation (EM)</span><br><span class="line">Hierarchical Clustering</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Association Rule Learning Algorithms:</span><br><span class="line"></span><br><span class="line">Apriori algorithm</span><br><span class="line">Eclat algorithm</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Artificial Neural Network Algorithms:(自然神经网络算法)</span><br><span class="line"></span><br><span class="line">Perceptron</span><br><span class="line">Multilayer Perceptrons (MLP)</span><br><span class="line">Back-Propagation</span><br><span class="line">Stochastic Gradient Descent</span><br><span class="line">Hopfield Network</span><br><span class="line">Radial Basis Function Network (RBFN)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Deep Learning Algorithms:(深度学习算法)</span><br><span class="line"></span><br><span class="line">Convolutional Neural Network (CNN)</span><br><span class="line">Recurrent Neural Networks (RNNs)</span><br><span class="line">Long Short-Term Memory Networks (LSTMs)</span><br><span class="line">Stacked Auto-Encoders</span><br><span class="line">Deep Boltzmann Machine (DBM)</span><br><span class="line">Deep Belief Networks (DBN)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Dimensionality Reduction Algorithms:(降维算法)</span><br><span class="line"></span><br><span class="line">Principal Component Analysis (PCA)</span><br><span class="line">Principal Component Regression (PCR)</span><br><span class="line">Partial Least Squares Regression (PLSR)</span><br><span class="line">Sammon Mapping</span><br><span class="line">Multidimensional Scaling (MDS)</span><br><span class="line">Projection Pursuit</span><br><span class="line">Linear Discriminant Analysis (LDA)</span><br><span class="line">Mixture Discriminant Analysis (MDA)</span><br><span class="line">Quadratic Discriminant Analysis (QDA)</span><br><span class="line">Flexible Discriminant Analysis (FDA)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Ensemble Algorithms:(算法集，合并的算法)</span><br><span class="line"></span><br><span class="line">Boosting</span><br><span class="line">Bootstrapped Aggregation (Bagging)</span><br><span class="line">AdaBoost</span><br><span class="line">Weighted Average (Blending)</span><br><span class="line">Stacked Generalization (Stacking)</span><br><span class="line">Gradient Boosting Machines (GBM)</span><br><span class="line">Gradient Boosted Regression Trees (GBRT)</span><br><span class="line">Random Forest</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Other Machine Learning Algorithms:其他机器学习算法</span><br><span class="line"></span><br><span class="line">Feature selection algorithms</span><br><span class="line">Algorithm accuracy evaluation</span><br><span class="line">Performance measures</span><br><span class="line">Optimization algorithms</span><br><span class="line"></span><br><span class="line">Computational intelligence (evolutionary algorithms, etc.)</span><br><span class="line">Computer Vision (CV)</span><br><span class="line">Natural Language Processing (NLP)</span><br><span class="line">Recommender Systems</span><br><span class="line">Reinforcement Learning</span><br><span class="line">Graphical Models</span><br><span class="line">And more…</span><br></pre></td></tr></table></figure><h4 id="2-How-Machine-Learning-Algorithms-Work-they-learn-a-mapping-of-input-to-output"><a href="#2-How-Machine-Learning-Algorithms-Work-they-learn-a-mapping-of-input-to-output" class="headerlink" title="2:How Machine Learning Algorithms Work (they learn a mapping of input to output)"></a>2:How Machine Learning Algorithms Work (they learn a mapping of input to output)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我的理解就是，机器学习算法的目的就是:得到最优的函数关系（最小误差，或者最快效率等等）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Learning a Function</span><br><span class="line">Machine learning algorithms are described as learning a target function (f) that best maps input variables (X) to an output variable (Y).</span><br><span class="line"></span><br><span class="line">Y = f(X)</span><br><span class="line"></span><br><span class="line">This is a general learning task where we would like to make predictions in the future (Y) given new examples of input variables (X).</span><br><span class="line"></span><br><span class="line">We don’t know what the function (f) looks like or it’s form. If we did, we would use it directly and we would not need to learn it from data using machine learning algorithms.</span><br><span class="line"></span><br><span class="line">It is harder than you think. There is also error (e) that is independent of the input data (X).</span><br><span class="line"></span><br><span class="line">Y = f(X) + e</span><br><span class="line"></span><br><span class="line">This error might be error such as not having enough attributes to sufficiently characterize the best mapping from X to Y. This error is called irreducible error because no matter how good we get at estimating the target function (f), we cannot reduce this error.</span><br><span class="line"></span><br><span class="line">This is to say, that the problem of learning a function from data is a difficult problem and this is the reason why the field of machine learning and machine learning algorithms exist.</span><br></pre></td></tr></table></figure><p>3:Parametric and Nonparametric Machine Learning Algorithms(参数和非参数学习)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我的理解是：</span><br><span class="line">参数学习更加适合小的网络，速度快，固定模型的参数;</span><br><span class="line">非参数学习精准度更高，但是可能出现过拟合（噪声使用）。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parametric Machine Learning Algorithms</span><br><span class="line"></span><br><span class="line">Assumptions can greatly simplify the learning process, but can also limit what can be learned. Algorithms that simplify the function to a known form are called parametric machine learning algorithms.</span><br><span class="line"></span><br><span class="line">Logistic Regression</span><br><span class="line">Linear Discriminant Analysis</span><br><span class="line">Perceptron</span><br><span class="line">Naive Bayes</span><br><span class="line">Simple Neural Networks</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Nonparametric Machine Learning Algorithms</span><br><span class="line"></span><br><span class="line">k-Nearest Neighbors</span><br><span class="line">Decision Trees like CART and C4.5</span><br><span class="line">Support Vector Machines</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Machine-Learning-Algorithms&quot;&gt;&lt;a href=&quot;#Machine-Learning-Algorithms&quot; class=&quot;headerlink&quot; title=&quot;Machine Learning Algorithms&quot;&gt;&lt;/a&gt;&lt;stro
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MachineLearning_math</title>
    <link href="https://yanyubing.xyz/2020/03/22/MachineLearning_math/"/>
    <id>https://yanyubing.xyz/2020/03/22/MachineLearning_math/</id>
    <published>2020-03-22T06:42:37.975Z</published>
    <updated>2020-03-24T18:29:34.818Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习数学基础"><a href="#机器学习数学基础" class="headerlink" title="机器学习数学基础"></a>机器学习数学基础</h1><h3 id="1：概率论"><a href="#1：概率论" class="headerlink" title="1：概率论"></a>1：概率论</h3><p>1:<strong>Probability and Machine</strong> <strong>Learning</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❼ Noise in observations, e.g. measurement errors and random noise.</span><br><span class="line">❼ Incomplete coverage of the domain, e.g. you can never observe all data.</span><br><span class="line">❼ Imperfect model of the problem, e.g. all models have errors, some are useful.Uncertainty in applied machine learning is managed using probability.</span><br><span class="line">❼ Probability and statistics help us to understand and quantify the expected value and variability of variables in our observations from the domain.</span><br><span class="line">❼ Probability helps to understand and quantify the expected distribution and density of observations in the domain.</span><br><span class="line">❼ Probability helps to understand and quantify the expected capability and variance in performance of our predictive models when applied to new data.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This is the bedrock of machine learning. On top of that, we may need models to predict a probability, we may use probability to develop predictive models (e.g. Naive Bayes), and we may use probabilistic frameworks to train predictive models (e.g. maximum likelihood estimation).</span><br></pre></td></tr></table></figure><p>2:<strong>Three Types of Probability</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">joint:联合概率 A且B一起发生的概率</span><br><span class="line">marginal：边缘概率 在边缘分布中，我们得到只关于一个变量的概率分布，而不再考虑另一变量的影响，实际上进行了降维操作。在实际应用中，例如人工神经网络的神经元互相关联，在计算它们各自的参数的时候，就会使用边缘分布计算得到某一特定神经元（变量）的值。</span><br><span class="line">conditional probability：条件概率  A发生的情况下B发生的概率</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">For this lesson, you must practice calculating joint, marginal, and conditional probabilities. For</span><br><span class="line">example, if a family has two children and the oldest is a boy, what is the probability of this</span><br><span class="line">family having two sons? This is called the Boy or Girl Problem and is one of many common toy</span><br><span class="line">problems for practicing probability.</span><br><span class="line"></span><br><span class="line">解答：两个孩子的性别可能的顺序是，男女，男男，女男，女女。</span><br><span class="line">已知第一个是男孩子，则条件概率的结果为1/4（两个男孩子的概率）除以1/2(第一个是男孩子的概率)=1/2</span><br></pre></td></tr></table></figure><p>3:<strong>Probability Distributions</strong> （概率分布）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">随机分布：</span><br><span class="line">❼ Discrete Random Variable. 离散随机分布</span><br><span class="line">❼ Continuous Random Variable.连续随机分布</span><br><span class="line"></span><br><span class="line">Discrete Probability Distributions（离散概率分布）</span><br><span class="line">❼ Poisson distribution.（伯努利分布）</span><br><span class="line">❼ Bernoulli and binomial distributions.（伯努利和二项分布）</span><br><span class="line">❼ Multinoulli and multinomial distributions.（多元分布和多项式分布）</span><br><span class="line"></span><br><span class="line">Continuous Probability Distributions(连续概率分布)</span><br><span class="line">❼ Normal or Gaussian distribution.(高斯分布）</span><br><span class="line">❼ Exponential distribution.</span><br><span class="line">❼ Pareto distribution.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># sample a normal distribution</span><br><span class="line"># Randomly Sample Gaussian Distribution（随机抽样高斯分布）</span><br><span class="line">from numpy.random import normal</span><br><span class="line"></span><br><span class="line"># define the distribution</span><br><span class="line"># 定义结构</span><br><span class="line">mu = 50</span><br><span class="line">sigma = 5</span><br><span class="line">n = 10</span><br><span class="line"># generate the sample</span><br><span class="line">sample = normal(mu, sigma, n)</span><br><span class="line">print(sample)</span><br></pre></td></tr></table></figure><p>4：<strong>Naive Bayes Classififier</strong>（朴素贝叶斯分类器）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Naive Bayes Classififier（朴素贝叶斯分类器）</span><br><span class="line"></span><br><span class="line"># example of gaussian naive bayes</span><br><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line"></span><br><span class="line"># generate 2d classification dataset</span><br><span class="line"># Generate isotropic Gaussian blobs for clustering.</span><br><span class="line">X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)</span><br><span class="line"></span><br><span class="line"># define the model</span><br><span class="line">model = GaussianNB()</span><br><span class="line"># fit the model，训练模型</span><br><span class="line">model.fit(X, y)</span><br><span class="line"># select a single sample()选择一个样本</span><br><span class="line">Xsample, ysample = [X[0]], y[0]</span><br><span class="line"># make a probabilistic prediction，概率预测</span><br><span class="line">yhat_prob = model.predict_proba(Xsample)</span><br><span class="line">print(&apos;Predicted Probabilities: &apos;, yhat_prob)</span><br><span class="line"># make a classification prediction</span><br><span class="line">yhat_class = model.predict(Xsample)</span><br><span class="line">print(&apos;Predicted Class: &apos;, yhat_class)</span><br><span class="line">print(&apos;Truth: y=%d&apos; % ysample)</span><br></pre></td></tr></table></figure><p>5：<strong>Entropy and Cross-Entropy</strong>（熵和互熵）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Low Probability Event: High Information (surprising).</span><br><span class="line">❼ High Probability Event: Low Information (unsurprising).</span><br><span class="line">低概率事件更具有价值(高信息值)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># example of calculating cross-entropy</span><br><span class="line">from math import log2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># calculate cross-entropy</span><br><span class="line">def cross_entropy(p, q):</span><br><span class="line">    return -sum([p[i] * log2(q[i]) for i in range(len(p))])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># define data</span><br><span class="line">p = [0.10, 0.40, 0.50]</span><br><span class="line">q = [0.80, 0.15, 0.05]</span><br><span class="line"># calculate cross-entropy H(P, Q)</span><br><span class="line">ce_pq = cross_entropy(p, q)</span><br><span class="line">print(&apos;H(P, Q): %.3f bits&apos; % ce_pq)</span><br><span class="line"># calculate cross-entropy H(Q, P)</span><br><span class="line">ce_qp = cross_entropy(q, p)</span><br><span class="line">print(&apos;H(Q, P): %.3f bits&apos; % ce_qp)</span><br></pre></td></tr></table></figure><p>6： <strong>Naive Classififiers</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 朴素分类器，一个简单概率验证</span><br><span class="line"></span><br><span class="line"># example of the majority class naive classifier in scikit-learn</span><br><span class="line">from numpy import asarray</span><br><span class="line">from sklearn.dummy import DummyClassifier</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line"># define dataset</span><br><span class="line">X = asarray([0 for _ in range(100)])</span><br><span class="line">class0 = [0 for _ in range(25)]</span><br><span class="line">class1 = [1 for _ in range(75)]</span><br><span class="line">y = asarray(class0 + class1)</span><br><span class="line"># reshape data for sklearn</span><br><span class="line">X = X.reshape((len(X), 1))</span><br><span class="line"># define model</span><br><span class="line">model = DummyClassifier(strategy=&apos;most_frequent&apos;)  # fit model</span><br><span class="line">model.fit(X, y)</span><br><span class="line"># print(X)</span><br><span class="line"># print(y)</span><br><span class="line"># make predictions</span><br><span class="line">yhat = model.predict(X)</span><br><span class="line"># print(yhat)</span><br><span class="line"># calculate accuracy</span><br><span class="line">accuracy = accuracy_score(y, yhat)</span><br><span class="line">print(&apos;Accuracy: %.3f&apos; % accuracy)</span><br></pre></td></tr></table></figure><p>7:<strong>Probability Scores</strong> (用于比较预测值和实际值的偏差的两种方式)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1:Log Loss Score</span><br><span class="line">2:Brier Score</span><br><span class="line"># A model with perfect skill has a log loss score of 0.0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># example of log loss,评价预测值和实际值的差距</span><br><span class="line">from numpy import asarray</span><br><span class="line">from sklearn.metrics import log_loss</span><br><span class="line"># define data</span><br><span class="line">y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</span><br><span class="line">y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]</span><br><span class="line"># define data as expected, e.g. probability for each event &#123;0, 1&#125;</span><br><span class="line">y_true = asarray([[v, 1-v] for v in y_true])</span><br><span class="line">y_pred = asarray([[v, 1-v] for v in y_pred])</span><br><span class="line"># calculate log loss</span><br><span class="line">loss = log_loss(y_true, y_pred)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># example of brier loss</span><br><span class="line">from sklearn.metrics import brier_score_loss</span><br><span class="line"># define data</span><br><span class="line">y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</span><br><span class="line">y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]</span><br><span class="line"># calculate brier score</span><br><span class="line">score = brier_score_loss(y_true, y_pred, pos_label=1)</span><br><span class="line">print(score)</span><br></pre></td></tr></table></figure><h3 id="2：统计学"><a href="#2：统计学" class="headerlink" title="2：统计学"></a>2：统计学</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">统计学是应用数学的一个分支，主要通过利用概率论建立数学模型，收集所观察系统的数据，进行量化的分析、总结，并进而进行推断和预测，为相关决策提供依据和参考。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我的理解是：</span><br><span class="line">①为了更好的描述一些指标，让数据看起来更加直观！</span><br><span class="line">②更多的是在对数据的处理环节使用到的一些方法</span><br></pre></td></tr></table></figure><p>1：<strong>Statistics and Machine</strong> <strong>Learning</strong></p><p>1.1：数据准备过程需要的统计方法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❼ Outlier detection.</span><br><span class="line">❼ Missing value imputation.</span><br><span class="line">❼ Data sampling.</span><br><span class="line">❼ Data scaling.</span><br><span class="line">❼ Variable encoding.</span><br><span class="line">❼ And much more.</span><br></pre></td></tr></table></figure><p>1.2：模型评估中的统计</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Data sampling.</span><br><span class="line">❼ Data resampling.重采样技术进行模型评估，例如k-fold cross-validation(k倍交叉集验证)</span><br><span class="line">❼ Experimental design.</span><br></pre></td></tr></table></figure><p>1.3：模型选择中的统计</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Checking for a significant difference between results.</span><br><span class="line">❼ Quantifying the size of the difference between results.</span><br><span class="line">This might include the use of statistical hypothesis tests.</span><br></pre></td></tr></table></figure><p>1.4：模型总结中的统计</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❼ Summarizing the expected skill of the model on average.</span><br><span class="line">❼ Quantifying the expected variability of the skill of the model in practice.</span><br></pre></td></tr></table></figure><p>1.5：预测值中的统计</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">❼ Quantifying the expected variability for the prediction.</span><br></pre></td></tr></table></figure><p>2： <strong>Introduction to Statistics</strong>（统计概论）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❼ Descriptive Statistics: Descriptive statistics refer to methods for summarizing raw observations into information that we can understand and share.</span><br><span class="line">❼ Inferential Statistics: Inferential statistics is a fancy name for methods that aid in quantifying properties of the domain or population from a smaller set of obtained observations called a sample.</span><br></pre></td></tr></table></figure><p>3：<strong>Gaussian Distribution and</strong>  <strong>Descriptive Stats</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># calculate summary stats</span><br><span class="line">from numpy.random import seed</span><br><span class="line">from numpy.random import randn</span><br><span class="line">from numpy import mean</span><br><span class="line">from numpy import var</span><br><span class="line">from numpy import std</span><br><span class="line"></span><br><span class="line"># seed the random number generator</span><br><span class="line">seed(1)</span><br><span class="line"># generate univariate observations</span><br><span class="line">data = 5 * randn(10000) + 50</span><br><span class="line"># calculate statistics</span><br><span class="line">print(&apos;Mean: %.3f&apos; % mean(data))</span><br><span class="line">print(&apos;Variance: %.3f&apos; % var(data))</span><br><span class="line">print(&apos;Standard Deviation: %.3f&apos; % std(data))</span><br></pre></td></tr></table></figure><p>4:<strong>Correlation Between</strong> <strong>Variables</strong>(变量之间的相关性)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❼ Positive Correlation: Both variables change in the same direction.(正相关)</span><br><span class="line">❼ Neutral Correlation: No relationship in the change of the variables.（无关）</span><br><span class="line">❼ Negative Correlation: Variables change in opposite directions.（负相关）</span><br><span class="line">在数据中如果有两个变量相关性很强，则可以考虑删除其中一个变量，来提高算法的性能(执行效率)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># calculate correlation coefficient</span><br><span class="line">from numpy.random import seed</span><br><span class="line">from numpy.random import randn</span><br><span class="line">from scipy.stats import pearsonr</span><br><span class="line"># seed random number generator</span><br><span class="line">seed(1)</span><br><span class="line"># prepare data</span><br><span class="line">data1 = 20 * randn(1000) + 100</span><br><span class="line">data2 = data1 + (10 * randn(1000) + 50)</span><br><span class="line"># calculate Pearson&apos;s correlation</span><br><span class="line">corr, p = pearsonr(data1, data2)</span><br><span class="line"># display the correlation</span><br><span class="line">print(&apos;Pearsons correlation: %.3f&apos; % corr)</span><br></pre></td></tr></table></figure><p>5:<strong>Statistical Hypothesis Tests</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是用来判断样本与样本、样本与总体的差异是由抽样误差引起还是本质差别造成的统计推断方法。显著性检验是假设检验中最常用的一种方法，也是一种最基本的统计推断形式，其基本原理是先对总体的特征做出某种假设，然后通过抽样研究的统计推理，对此假设应该被拒绝还是接受做出推断。常用的假设检验方法有Z检验、t检验、卡方检验、F检验等 [1]  。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 统计假设性原则</span><br><span class="line"># student&apos;s t-test</span><br><span class="line">from numpy.random import seed</span><br><span class="line">from numpy.random import randn</span><br><span class="line">from scipy.stats import ttest_ind</span><br><span class="line"></span><br><span class="line"># seed the random number generator</span><br><span class="line">seed(1)</span><br><span class="line"># generate two independent samples</span><br><span class="line">data1 = 5 * randn(100) + 52</span><br><span class="line">data2 = 5 * randn(100) + 51</span><br><span class="line"># compare samples</span><br><span class="line">stat, p = ttest_ind(data1, data2)</span><br><span class="line">print(&apos;Statistics=%.3f, p=%.3f&apos; % (stat, p))</span><br><span class="line"># interpret</span><br><span class="line">alpha = 0.05</span><br><span class="line">if p &gt; alpha:</span><br><span class="line">    print(&apos;Same distributions (fail to reject H0)&apos;)</span><br><span class="line">else:</span><br><span class="line">    print(&apos;Different distributions (reject H0)&apos;)</span><br></pre></td></tr></table></figure><p>6:<strong>Estimation Statistics</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># calculate the confidence interval</span><br><span class="line">from statsmodels.stats.proportion import proportion_confint</span><br><span class="line"></span><br><span class="line"># calculate the interval</span><br><span class="line">lower, upper = proportion_confint(88, 100, 0.05)</span><br><span class="line">print(&apos;lower=%.3f, upper=%.3f&apos; % (lower, upper))</span><br></pre></td></tr></table></figure><p>7: <strong>Nonparametric Statistics</strong>(非参数统计)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># example of the mann-whitney u test</span><br><span class="line"># 同样用来验证两个样本的差异性</span><br><span class="line">from numpy.random import seed</span><br><span class="line">from numpy.random import rand</span><br><span class="line">from scipy.stats import mannwhitneyu</span><br><span class="line"></span><br><span class="line"># seed the random number generator</span><br><span class="line">seed(1)</span><br><span class="line"># generate two independent samples</span><br><span class="line">data1 = 50 + (rand(100) * 10)</span><br><span class="line">data2 = 51 + (rand(100) * 10)</span><br><span class="line"># compare samples</span><br><span class="line">stat, p = mannwhitneyu(data1, data2)</span><br><span class="line">print(&apos;Statistics=%.3f, p=%.3f&apos; % (stat, p))</span><br><span class="line"># interpret</span><br><span class="line">alpha = 0.05</span><br><span class="line">if p &gt; alpha:</span><br><span class="line">    print(&apos;Same distribution (fail to reject H0)&apos;)</span><br><span class="line">else:</span><br><span class="line">    print(&apos;Different distribution (reject H0)&apos;)</span><br></pre></td></tr></table></figure><h3 id="3：线性代数"><a href="#3：线性代数" class="headerlink" title="3：线性代数"></a>3：线性代数</h3><p>1：Linear Algebra for Machine Learning</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">You Need to Learn Linear Algebra Notation</span><br><span class="line">You Need to Learn Linear Algebra Arithmetic</span><br><span class="line">You Need to Learn Linear Algebra for Statistics</span><br><span class="line">You Need to Learn Matrix Factorization</span><br><span class="line">You Need to Learn Linear Least Squares</span><br><span class="line"></span><br><span class="line">您需要学习线性代数符号</span><br><span class="line">您需要学习线性代数算法</span><br><span class="line">您需要学习用于统计的线性代数</span><br><span class="line">您需要学习矩阵分解</span><br><span class="line">您需要学习线性最小二乘</span><br></pre></td></tr></table></figure><p>2： <strong>Linear Algebra</strong></p><p> <strong>Linear Algebra</strong>(线性代数)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">线性代数是数学的一个分支，但事实是线性代数是数据的数学。 矩阵和向量是数据的语言。 线性代数是关于线性组合的。 也就是说，对称为向量的数字列和称为矩阵的数字2D数组进行算术运算，以创建新的数字列和数组。</span><br></pre></td></tr></table></figure><p><strong>Numerical Linear Algebra</strong> （数值线性代数）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">线性代数在计算机中的应用通常称为数值线性代数。 它不仅仅是在代码库中实现线性代数运算； 它还包括认真处理应用数学的问题，例如使用数字计算机有限的浮点精度进行处理。</span><br></pre></td></tr></table></figure><p><strong>Applications of Linear Algebra</strong>（应用线性代数）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">❼ Matrices in Engineering, such as a line of springs.</span><br><span class="line">❼ Graphs and Networks, such as analyzing networks.</span><br><span class="line">❼ Markov Matrices, Population, and Economics, such as population growth.</span><br><span class="line">❼ Linear Programming, the simplex optimization method.</span><br><span class="line">❼ Fourier Series, Linear Algebra for functions, used widely in signal processing.</span><br><span class="line">❼ Linear Algebra for statistics and probability, such as least squares for regression.</span><br><span class="line">❼ Computer Graphics, such as the various translation, scaling and rotation of images.</span><br></pre></td></tr></table></figure><p>3:<strong>Vectors</strong>(向量) </p><p><strong>Defifining a Vector</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">We can represent a vector in Python as a NumPy array. A NumPy array can be created from a list of numbers. For example, below we defifine a vector with the length of 3 and the integer values 1, 2 and 3.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create a vector</span><br><span class="line">from numpy import array</span><br><span class="line">v = array([1, 2, 3])</span><br><span class="line">print(v)</span><br></pre></td></tr></table></figure><p><strong>Vector Multiplication</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># multiply vectors</span><br><span class="line">from numpy import array</span><br><span class="line">a = array([1, 2, 3])</span><br><span class="line">print(a)</span><br><span class="line">b = array([1, 2, 3])</span><br><span class="line">print(b)</span><br><span class="line">c = a * b</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure><p><strong>Defifining a Matrix</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create matrix</span><br><span class="line">from numpy import array</span><br><span class="line">A = array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">print(A)</span><br></pre></td></tr></table></figure><p><strong>Matrix Addition</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># add matrices</span><br><span class="line">from numpy import array</span><br><span class="line">A = array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">print(A)</span><br><span class="line">B = array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">print(B)</span><br><span class="line">C = A + B</span><br><span class="line">print(C)</span><br></pre></td></tr></table></figure><p><strong>Matrix Dot Product</strong> (矩阵点集,乘法)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">C(m, k) = A(m, n) × B(n, k)</span><br><span class="line"></span><br><span class="line"># matrix dot product</span><br><span class="line">from numpy import array</span><br><span class="line">A = array([[1, 2], [3, 4], [5, 6]])</span><br><span class="line">print(A)</span><br><span class="line">B = array([[1, 2], [3, 4]])</span><br><span class="line">print(B)</span><br><span class="line">C = A.dot(B)</span><br><span class="line">print(C)</span><br></pre></td></tr></table></figure><p> 4:Matrix Types and Operations</p><p><strong>Transpose</strong>(转置)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># transpose matrix</span><br><span class="line">from numpy import array</span><br><span class="line">A = array([[1, 2], [3, 4], [5, 6]])</span><br><span class="line">print(A)</span><br><span class="line">C = A.T</span><br><span class="line">print(C)</span><br></pre></td></tr></table></figure><p><strong>Inversion</strong>(逆)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># invert matrix</span><br><span class="line">from numpy import array</span><br><span class="line">from numpy.linalg import inv</span><br><span class="line"># define matrix</span><br><span class="line">A = array([[1.0, 2.0], [3.0, 4.0]])</span><br><span class="line">print(A)</span><br><span class="line"># invert matrix</span><br><span class="line">B = inv(A)</span><br><span class="line">print(B)</span><br></pre></td></tr></table></figure><p><strong>Square Matrix</strong> (方阵)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A square matrix is a matrix where the number of rows (n) equals the number of columns (m).</span><br></pre></td></tr></table></figure><p><strong>Symmetric Matrix</strong>(对称矩阵)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A symmetric matrix is a type of square matrix where the top-right triangle is the same as the bottom-left triangle. To be symmetric, the axis of symmetry is always the main diagonal of the matrix, from the top left to the bottom right. A symmetric matrix is always square and equal to its own transpose.</span><br></pre></td></tr></table></figure><p><strong>Triangular Matrix</strong>(三角矩阵)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A triangular matrix is a type of square matrix that has all values in the upper-right or lower-left of the matrix with the remaining elements filled with zero values. A triangular matrix with values only above the main diagonal is called an upper triangular matrix. Whereas, a triangular matrix with values only below the main diagonal is called a lower triangular matrix.</span><br></pre></td></tr></table></figure><p><strong>Diagonal Matrix</strong>(对角矩阵)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A diagonal matrix is one where values outside of the main diagonal have a zero value, where the main diagonal is taken from the top left of the matrix to the bottom right. A diagonal matrix is often denoted with the variable D and may be represented as a full matrix or as a vector of values on the main diagonal.</span><br></pre></td></tr></table></figure><p>5:<strong>Matrix Factorization</strong> (矩阵分解)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">矩阵分解是将矩阵还原成其组成部分的一种方式。 这是一种可以简化更复杂的矩阵运算的方法，该运算可以在分解矩阵上执行，而不能在原始矩阵本身上执行。 矩阵分解的一个常见类比是数的分解，例如将25分解为5×5。因此,像分解实数值一样，有许多方法可以分解矩阵，因此存在多种不同的矩阵分解技术。</span><br></pre></td></tr></table></figure><p><strong>LU Matrix Decomposition</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">The LU decomposition is for square matrices and decomposes a matrix into L and U components.</span><br><span class="line"></span><br><span class="line">A = L · U (12)</span><br><span class="line"></span><br><span class="line">Where A is the square matrix that we wish to decompose, L is the lower triangle matrix and U is the upper triangle matrix. A variation of this decomposition that is numerically more stable to solve in practice is called the LUP decomposition, or the LU decomposition with partial pivoting.</span><br><span class="line"></span><br><span class="line">A = P · L · U (13)</span><br><span class="line"></span><br><span class="line">The rows of the parent matrix are re-ordered to simplify the decomposition process and the additional P matrix specifies a way to permute the result or return the result to the original order. There are also other variations of the LU. The LU decomposition is often used to simplify the solving of systems of linear equations, such as finding the coefficients in a linear regression.</span><br><span class="line">The LU decomposition can be implemented in Python with the lu() function. More specifically,this function calculates an LPU decomposition.</span><br></pre></td></tr></table></figure><p>Singular-Value Decomposition(奇异值分解)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler.</span><br><span class="line"></span><br><span class="line">A = U · Σ · V T (14)</span><br><span class="line"></span><br><span class="line">Where A is the real m × n matrix that we wish to decompose, U is an m × m matrix, Σ</span><br><span class="line">(sigma) is an m × n diagonal matrix, and V T is the transpose of an n × n matrix where T is a superscript.</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习数学基础&quot;&gt;&lt;a href=&quot;#机器学习数学基础&quot; class=&quot;headerlink&quot; title=&quot;机器学习数学基础&quot;&gt;&lt;/a&gt;机器学习数学基础&lt;/h1&gt;&lt;h3 id=&quot;1：概率论&quot;&gt;&lt;a href=&quot;#1：概率论&quot; class=&quot;headerlink
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MachineLearning_Test</title>
    <link href="https://yanyubing.xyz/2020/03/22/MachineLearning_Test/"/>
    <id>https://yanyubing.xyz/2020/03/22/MachineLearning_Test/</id>
    <published>2020-03-22T01:26:15.213Z</published>
    <updated>2020-03-22T06:42:19.324Z</updated>
    
    <content type="html"><![CDATA[<h3 id="练习机器学习"><a href="#练习机器学习" class="headerlink" title="练习机器学习"></a>练习机器学习</h3><p>1：机器学习步骤</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：定义问题</span><br><span class="line">2：准备数据</span><br><span class="line">3：评估算法</span><br><span class="line">4：改善结果</span><br><span class="line">5：写出结果</span><br></pre></td></tr></table></figure><p>2：数据哪里来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">机器学习库：http://archive.ics.uci.edu/ml/index.php</span><br></pre></td></tr></table></figure><p>3： Regression: <a href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality" target="_blank" rel="noopener">Wine Quality Data Set</a> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># 手写代码，Regression: Wine Quality Data Set</span><br><span class="line"># 用于读取数据集</span><br><span class="line">import pandas</span><br><span class="line"># 使用逻辑回归模型</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"># 绘图，用于查看数据结构</span><br><span class="line">import matplotlib as plt</span><br><span class="line"># 用于保存模型</span><br><span class="line">import pickle</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split</span><br><span class="line"></span><br><span class="line"># 1：定义问题：白酒种类和哪些因素有关系</span><br><span class="line"># 2：准备数据</span><br><span class="line">from sklearn.utils import column_or_1d</span><br><span class="line"></span><br><span class="line">url_data = &apos;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&apos;</span><br><span class="line"></span><br><span class="line">data = pandas.read_csv(url_data)</span><br><span class="line"></span><br><span class="line"># print(data)</span><br><span class="line"># 3:评估模型</span><br><span class="line">array = data.values</span><br><span class="line"></span><br><span class="line">X = array[:, 1:]</span><br><span class="line">y = array[:, 0:1]</span><br><span class="line"></span><br><span class="line"># print(X),需要把y改为y.ravel()</span><br><span class="line">X_train, X_validation, Y_train, Y_validation = train_test_split(X, y.ravel(), test_size=0.20, random_state=1)</span><br><span class="line">model = LogisticRegression(solver=&apos;liblinear&apos;, multi_class=&apos;ovr&apos;)</span><br><span class="line"></span><br><span class="line">kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)</span><br><span class="line">cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=&apos;accuracy&apos;)</span><br><span class="line">print(&apos; %f (%f)&apos; % (cv_results.mean(), cv_results.std()))</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">model.fit(X_train, Y_train)</span><br><span class="line"># 保存模型</span><br><span class="line">filename = &apos;Wine.model&apos;</span><br><span class="line">pickle.dump(model, open(filename, &apos;wb&apos;))</span><br><span class="line"># 预测模型</span><br><span class="line">predictions = model.predict(X_validation)</span><br><span class="line">print(&apos;真实值:&apos;, Y_validation)</span><br><span class="line">print(&apos;------------&apos;)</span><br><span class="line">print(&apos;预测值:&apos;, predictions)</span><br><span class="line">print(&apos;------------&apos;)</span><br><span class="line">array = predictions - Y_validation</span><br><span class="line">print(&apos;对比:&apos;, array)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;练习机器学习&quot;&gt;&lt;a href=&quot;#练习机器学习&quot; class=&quot;headerlink&quot; title=&quot;练习机器学习&quot;&gt;&lt;/a&gt;练习机器学习&lt;/h3&gt;&lt;p&gt;1：机器学习步骤&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;t
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Mini_Python_MachineLearning</title>
    <link href="https://yanyubing.xyz/2020/03/20/Mini_Python_MachineLearning/"/>
    <id>https://yanyubing.xyz/2020/03/20/Mini_Python_MachineLearning/</id>
    <published>2020-03-19T22:36:53.798Z</published>
    <updated>2020-03-21T03:09:40.335Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Machine-Learning-Mastery-With-Python-Mini-Course"><a href="#Machine-Learning-Mastery-With-Python-Mini-Course" class="headerlink" title="Machine Learning Mastery With Python Mini-Course"></a><strong>Machine Learning Mastery With</strong> <strong>Python Mini-Course</strong></h3><p>1：<strong>Download and Install Python</strong> <strong>and SciPy Ecosystem</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 环境准备</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">import scipy</span><br><span class="line">import numpy</span><br><span class="line">import matplotlib</span><br><span class="line">import pandas</span><br><span class="line">import sklearn</span><br><span class="line"></span><br><span class="line">print(&apos;Python:&#123;&#125;&apos;.format(sys.version))</span><br><span class="line">print(&apos;scipy:&#123;&#125;&apos;.format(scipy.__version__))</span><br><span class="line">print(&apos;numpy:&#123;&#125;&apos;.format(numpy.__version__))</span><br><span class="line">print(&apos;matplotlib:&#123;&#125;:&apos;.format(matplotlib.__version__))</span><br><span class="line">print(&apos;pandas:&#123;&#125;&apos;.format(pandas.__version__))</span><br><span class="line">print(&apos;sklearn:&#123;&#125;&apos;.format(sklearn.__version__))</span><br></pre></td></tr></table></figure><p>2： <strong>Get Around In Python,</strong> <strong>NumPy, Matplotlib and Pandas</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. Practice assignment, working with lists and flow control in Python.</span><br><span class="line">2. Practice working with NumPy arrays.</span><br><span class="line">3. Practice creating simple plots in Matplotlib.</span><br><span class="line">4. Practice working with Pandas Series and DataFrame.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy</span><br><span class="line">import pandas</span><br><span class="line"></span><br><span class="line">myArray = numpy.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">rowNames = [&apos;a&apos;, &apos;b&apos;]</span><br><span class="line">colNames = [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;]</span><br><span class="line">myDataFrame = pandas.DataFrame(myArray, index=rowNames, columns=colNames)</span><br><span class="line">print(myDataFrame)</span><br></pre></td></tr></table></figure><p>3：<strong>Load Data From CSV</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据网站http://archive.ics.uci.edu/ml/index.php</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">❼ Practice loading CSV files into Python using the CSV.reader()6</span><br><span class="line">function in the standard</span><br><span class="line">library.</span><br><span class="line">❼ Practice loading CSV files using NumPy and the numpy.loadtxt()7</span><br><span class="line">function.</span><br><span class="line">❼ Practice loading CSV files using Pandas and the pandas.read csv()8</span><br><span class="line">function.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 加载数据</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from io import StringIO</span><br><span class="line"></span><br><span class="line"># 1:numpy加载数据</span><br><span class="line">c = StringIO(&apos;0 1 \n2 3&apos;)</span><br><span class="line">d = np.loadtxt(c)</span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line"># 2:CSV加载数据https://docs.python.org/2/library/csv.html</span><br><span class="line"></span><br><span class="line"># 3: pandas Load a CSV dataset from a URL.</span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">data = read_csv(url, names=names)</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure><p>4：<strong>Understand Data with</strong> <strong>Descriptive Statistics</strong> （了解数据简单结构，内容）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Once you have loaded your data into Python you need to be able to understand it. The better you can understand your data, the better and more accurate the models that you can build.</span><br><span class="line">The first step to understanding your data is to use descriptive statistics. Today your lesson is to learn how to use descriptive statistics to understand your data. I recommend using the helper functions provided on the Pandas DataFrame.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">❼ Understand your data using the head() function to look at the first few rows.</span><br><span class="line">❼ Review the dimensions of your data with the shape property.</span><br><span class="line">❼ Look at the data types for each attribute with the dtypes property.</span><br><span class="line">❼ Review the distribution of your data with the describe() function.</span><br><span class="line">❼ Calculate pairwise correlation between your variables using the corr() function</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 查看数据</span><br><span class="line">from pandas import read_csv</span><br><span class="line"></span><br><span class="line">#  pandas Load a CSV dataset from a URL.</span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">data = read_csv(url, names=names)</span><br><span class="line">shape = data.shape</span><br><span class="line">head = data.head(20)</span><br><span class="line">dtype = data.dtypes</span><br><span class="line">description = data.describe()</span><br><span class="line">print(shape)</span><br><span class="line">print(head)</span><br><span class="line">print(dtype)</span><br><span class="line">print(&apos;----------&apos;)</span><br><span class="line">print(description)</span><br></pre></td></tr></table></figure><p>5：<strong>Understand Data with</strong>  <strong>Visualization</strong>（数据可视化）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from pandas import read_csv</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from pandas.plotting import scatter_matrix</span><br><span class="line"></span><br><span class="line"># 数据可视化</span><br><span class="line">#  pandas Load a CSV dataset from a URL.</span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">data = read_csv(url, names=names)</span><br><span class="line"></span><br><span class="line"># 直方图</span><br><span class="line">data.hist()</span><br><span class="line"># pairwise scatter plots of all attributes.</span><br><span class="line">scatter_matrix(data)</span><br><span class="line"># pairwise scatter plots</span><br><span class="line"># data.plot.box()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>6：<strong>Prepare For Modeling by</strong>  <strong>Pre-Processing Data</strong> （预处理数据）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The scikit-learn library provides two standard idioms for transforming data. Each are useful in different circumstances: Fit and Multiple Transform and Combined Fit-And-Transform.</span><br><span class="line">There are many techniques that you can use to prepare your data for modeling, for example try out some of the following</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 下面的代码段加载了皮马印第安人发病的糖尿病数据集，标准化数据所需的参数，然后创建输入的标准化副本数据</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from pandas import read_csv</span><br><span class="line">import numpy</span><br><span class="line"></span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">dataFrame = read_csv(url, names=names)</span><br><span class="line">arrray = dataFrame.values</span><br><span class="line"># 切割array得到输入集和输出集</span><br><span class="line">X = arrray[:, 0:8]</span><br><span class="line">Y = arrray[:, 8]</span><br><span class="line"># 转换数据，归一化处理</span><br><span class="line">scala = StandardScaler().fit(X)</span><br><span class="line">rescaledX = scala.transform(X)</span><br><span class="line"># summarize transformed data</span><br><span class="line"># precision:小数点的位数</span><br><span class="line">numpy.set_printoptions(precision=4)</span><br><span class="line">print(X)</span><br><span class="line">print(&apos;---------&apos;)</span><br><span class="line">print(rescaledX)</span><br></pre></td></tr></table></figure><p>7：<strong>Algorithm Evaluation With</strong>  <strong>Resampling Methods</strong>（重采样方法进行算法评估）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Split a dataset into training and test sets.</span><br><span class="line">❼ Estimate the accuracy of an algorithm using k-fold cross-validation.</span><br><span class="line">❼ Estimate the accuracy of an algorithm using leave one out cross-validation.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Evaluate using Cross-Validation</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class &apos;]</span><br><span class="line">dataframe = read_csv(url, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:, 0: 8]</span><br><span class="line">Y = array[:, 8]</span><br><span class="line">kfold = KFold(n_splits=10, random_state=7)</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">results = cross_val_score(model, X, Y, cv=kfold)</span><br><span class="line">print(&quot;Accuracy: %.3f%% (%.3f%%)&quot; % (results.mean() * 100.0, results.std() * 100.0))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输出准确率:Accuracy: 76.951% (4.841%)</span><br></pre></td></tr></table></figure><p>8：<strong>Algorithm Evaluation</strong> <strong>Metrics</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">There are many different metrics that you can use to evaluate the skill of a machine learning algorithm on a dataset.</span><br><span class="line">You can specify the metric used for your test harness in scikit-learn via the cross val score() function and defaults can be used for regression and classification problems. Your goal with todays lesson is to practice using the different algorithm performance metrics available in the scikit-learn package.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Practice using the Accuracy and LogLoss metrics on a classification problem.</span><br><span class="line">❼ Practice generating a confusion matrix and a classification report.</span><br><span class="line">❼ Practice using RMSE and RSquared metrics on a regression problem.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Cross-Validation Classification LogLoss</span><br><span class="line"># 评估指标</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">dataframe = read_csv(url, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:, 0:8]</span><br><span class="line">Y = array[:, 8]</span><br><span class="line">kfold = KFold(n_splits=10, random_state=7)</span><br><span class="line">model = LogisticRegression(solver=&apos;liblinear&apos;)</span><br><span class="line">scoring = &apos;neg_log_loss&apos;</span><br><span class="line">results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)</span><br><span class="line">print(&quot;Logloss: %.3f (%.3f)&quot; % (results.mean(), results.std()))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输出:Logloss: -0.493 (0.047)</span><br></pre></td></tr></table></figure><p>9：<strong>Spot-Check Algorithms</strong>（选择合适的算法）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">You cannot possibly know which algorithm will perform best on your data beforehand. You have to discover it using a process of trial and error. I call this spot-checking algorithms. The scikitlearn library provides an interface to many machine learning algorithms and tools to compare the estimated accuracy of those algorithms. In this lesson you must practice spot-checking different machine learning algorithms.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Spot-check linear algorithms on a dataset (e.g. linear regression, logistic regression and linear discriminate analysis).(选择线性算法)</span><br><span class="line">❼ Spot-check some nonlinear algorithms on a dataset (e.g. KNN, SVM and CART).(选择非线性算法)</span><br><span class="line">❼ Spot-check some sophisticated ensemble algorithms on a dataset (e.g. random forest and stochastic gradient boosting)(选择组合算法)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># KNN Regression</span><br><span class="line"># 多种算法尝试，这里举例</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.neighbors import KNeighborsRegressor</span><br><span class="line"></span><br><span class="line">url = &apos;https://goo.gl/FmJUSM&apos;</span><br><span class="line">names = [&apos;CRIM&apos;, &apos;ZN&apos;, &apos;INDUS&apos;, &apos;CHAS&apos;, &apos;NOX&apos;, &apos;RM&apos;, &apos;AGE&apos;, &apos;DIS&apos;, &apos;RAD&apos;, &apos;TAX&apos;, &apos;PTRATIO&apos;, &apos;B&apos;, &apos;LSTAT&apos;, &apos;MEDV&apos;]</span><br><span class="line">dataframe = read_csv(url, delim_whitespace=True, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:, 0:13]</span><br><span class="line">Y = array[:, 13]</span><br><span class="line">kfold = KFold(n_splits=10, random_state=7)</span><br><span class="line">model = KNeighborsRegressor()</span><br><span class="line">scoring = &apos;neg_mean_squared_error&apos;</span><br><span class="line">results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)</span><br><span class="line">print(results.mean())</span><br></pre></td></tr></table></figure><p>10： <strong>Model Comparison and</strong> <strong>Selection</strong> （模型的比较和选择）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Now that you know how to spot-check machine learning algorithms on your dataset, you need to know how to compare the estimated performance of different algorithms and select the best model. In todays lesson you will practice comparing the accuracy of machine learning algorithms in Python with scikit-learn.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Compare linear algorithms to each other on a dataset.</span><br><span class="line">❼ Compare nonlinear algorithms to each other on a dataset.</span><br><span class="line">❼ Create plots of the results comparing algorithms</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># Compare Algorithms</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line"># load dataset</span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">dataframe = read_csv(url, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:, 0:8]</span><br><span class="line">Y = array[:, 8]</span><br><span class="line"># prepare models</span><br><span class="line">models = []</span><br><span class="line">models.append((&apos;LR&apos;, LogisticRegression(solver=&apos;liblinear&apos;)))</span><br><span class="line">models.append((&apos;LDA&apos;, LinearDiscriminantAnalysis()))</span><br><span class="line"># evaluate each model in turn</span><br><span class="line">results = []</span><br><span class="line">names = []</span><br><span class="line">scoring = &apos;accuracy&apos;</span><br><span class="line">for name, model in models:</span><br><span class="line">    kfold = KFold(n_splits=10, random_state=7)</span><br><span class="line">    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)</span><br><span class="line">    results.append(cv_results)</span><br><span class="line">    names.append(name)</span><br><span class="line">    print(&apos;% s: % f( % f)&apos; % (name, cv_results.mean(), cv_results.std()))</span><br></pre></td></tr></table></figure><p>11：<strong>Improve Accuracy with</strong> <strong>Algorithm Tuning</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Once you have found one or two algorithms that perform well on your dataset, you may want to improve the performance of those models. One way to increase the performance of an algorithm is to tune it’s parameters to your specific dataset. The scikit-learn library provides two ways to</span><br><span class="line">search for combinations of parameters for a machine learning algorithm:</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❼ Tune the parameters of an algorithm using a grid search that you specify.</span><br><span class="line">❼ Tune the parameters of an algorithm using a random search</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Grid Search for Algorithm Tuning</span><br><span class="line"></span><br><span class="line">from pandas import read_csv</span><br><span class="line">import numpy</span><br><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">dataframe = read_csv(url, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:, 0:8]</span><br><span class="line">Y = array[:, 8]</span><br><span class="line">alphas = numpy.array([1, 0.1, 0.01, 0.001, 0.0001, 0])</span><br><span class="line">param_grid = dict(alpha=alphas)</span><br><span class="line">model = Ridge()</span><br><span class="line">grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)</span><br><span class="line">grid.fit(X, Y)</span><br><span class="line">print(grid.best_score_)</span><br><span class="line">print(grid.best_estimator_.alpha)</span><br></pre></td></tr></table></figure><p>12： <strong>Improve Accuracy with</strong> <strong>Ensemble Predictions</strong>（组合算法提高准确度）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Another way that you can improve the performance of your models is to combine the predictions from multiple models. Some models provide this capability built-in such as random forest for bagging and stochastic gradient boosting for boosting. Another type of ensembling called voting can be used to combine the predictions from multiple different models together. In todays lesson you will practice using ensemble methods.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❼Practice bagging ensembles with the Random Forest and Extra Trees algorithms.</span><br><span class="line">❼ Practice boosting ensembles with the Gradient Boosting Machine and AdaBoost algorithms.</span><br><span class="line">❼ Practice voting ensembles using by combining the predictions from multiple models</span><br><span class="line">together.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Random Forest Classification</span><br><span class="line"></span><br><span class="line">from pandas import read_csv</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">dataframe = read_csv(url, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:, 0:8]</span><br><span class="line">Y = array[:, 8]</span><br><span class="line">num_trees = 100</span><br><span class="line">max_features = 3</span><br><span class="line">kfold = KFold(n_splits=10, random_state=7)</span><br><span class="line">model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)</span><br><span class="line">results = cross_val_score(model, X, Y, cv=kfold)</span><br><span class="line">print(results.mean())</span><br></pre></td></tr></table></figure><p>13： <strong>Finalize And Save Your</strong>  <strong>Model</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Practice making predictions with your model on new data (data unseen during training</span><br><span class="line">and testing).</span><br><span class="line">❼ Practice saving trained models to file and loading them up again</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># Save Model Using Pickle</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">import pickle</span><br><span class="line"></span><br><span class="line">url = &apos;https://goo.gl/bDdBiA&apos;</span><br><span class="line">names = [&apos;preg&apos;, &apos;plas&apos;, &apos;pres&apos;, &apos;skin&apos;, &apos;test&apos;, &apos;mass&apos;, &apos;pedi&apos;, &apos;age&apos;, &apos;class&apos;]</span><br><span class="line">dataframe = read_csv(url, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:, 0:8]</span><br><span class="line">Y = array[:, 8]</span><br><span class="line">test_size = 0.33</span><br><span class="line">seed = 7</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,</span><br><span class="line">                                                    random_state=seed)</span><br><span class="line"># Fit the model on 33%</span><br><span class="line">model = LogisticRegression(solver=&apos;liblinear&apos;)</span><br><span class="line">model.fit(X_train, Y_train)</span><br><span class="line"># save the model to disk</span><br><span class="line">filename = &apos;finalized_model.sav&apos;</span><br><span class="line">pickle.dump(model, open(filename, &apos;wb&apos;))</span><br><span class="line"></span><br><span class="line"># some time later...</span><br><span class="line"></span><br><span class="line"># load the model from disk</span><br><span class="line">loaded_model = pickle.load(open(filename, &apos;rb&apos;))</span><br><span class="line">result = loaded_model.score(X_test, Y_test)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Machine-Learning-Mastery-With-Python-Mini-Course&quot;&gt;&lt;a href=&quot;#Machine-Learning-Mastery-With-Python-Mini-Course&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Mini_Weka_MachineLearning</title>
    <link href="https://yanyubing.xyz/2020/03/19/Mini_Weka_MachineLearning/"/>
    <id>https://yanyubing.xyz/2020/03/19/Mini_Weka_MachineLearning/</id>
    <published>2020-03-19T09:40:46.695Z</published>
    <updated>2020-03-19T13:02:00.440Z</updated>
    
    <content type="html"><![CDATA[<p>1：Weka</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Weka的全名是怀卡托智能分析环境（Waikato Environment for Knowledge Analysis），是一款免费的，非商业化（与之对应的是SPSS公司商业数据挖掘产品--Clementine ）的，基于JAVA环境下开源的机器学习（machine learning）以及数据挖掘（data mining）软件。它和它的源代码可在其官方网站下载。有趣的是，该软件的缩写WEKA也是新西兰独有的一种鸟名（新西兰秧鸡），而Weka的主要开发者同时恰好来自新西兰的怀卡托大学（The University of Waikato）。</span><br></pre></td></tr></table></figure><p>2：download Weka，下载weka</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://sourceforge.net/projects/weka/</span><br></pre></td></tr></table></figure><p>3：Load Standard Machine Learning Datasets，加载数据集</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. Start Weka (click on the bird icon), this will start the Weka GUI Chooser.</span><br><span class="line">2. Click the Explorer button, this will open the Weka Explorer interface.</span><br><span class="line">3. Click the Open file... button and navigate to the data/ directory in your Weka installation</span><br><span class="line">and load the diabetes.arff dataset.</span><br></pre></td></tr></table></figure><p>4：Descriptive Stats and Visualization，查看数据的特征</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4. Click on different attributes in the Attributes list and review the details in the Selected</span><br><span class="line">attribute pane.</span><br><span class="line">5. Click the Visualize All button to review all attribute distributions.</span><br><span class="line">6. Click the Visualize tab and review the scatter plot matrix for all attributes</span><br></pre></td></tr></table></figure><p>5：Rescale Your Data：数据预处理，过滤，保存预处理完的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/diabetes.arff dataset.</span><br><span class="line">3. Click the Choose button in the Filter pane and select unsupervised.attribute.Normalize.</span><br><span class="line">4. Click the Apply button.</span><br></pre></td></tr></table></figure><p>6： Perform Feature Selection on Your Data，执行特征选择（不是所有的特征都会影响结果，通过方法选择出对结果有影响的特征）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/diabetes.arff dataset.</span><br><span class="line">3. Click the Select attributes tab.</span><br><span class="line">4. Click the Choose button in the Attribute Evaluator pane and select the CorrelationAttributeEval.</span><br><span class="line">(a) You will be presented with a dialog asking you to change to the Ranker search</span><br><span class="line">method, needed when using this feature selection method. Click the Yes button.</span><br><span class="line">5. Click the Start button to run the feature selection method.</span><br></pre></td></tr></table></figure><p>7： Machine Learning Algorithms in Weka(机器学习算法)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/diabetes.arff dataset.</span><br><span class="line">3. Click the Classify tab.</span><br><span class="line">4. Click the Choose button and note the different groupings for algorithms.</span><br><span class="line">5. Click the name of the selected algorithm to configure it.</span><br><span class="line">6. Click the More button on the configuration window to learn more about the implementation.</span><br><span class="line">7. Click the Capabilities button on the configuration window to learn more about how it can</span><br><span class="line">be used.</span><br><span class="line">8. Note the Open and Save buttons on the window where different configurations can be</span><br><span class="line">saved and loaded.</span><br><span class="line">9. Hover on a configuration parameter and note the tooltip help.</span><br><span class="line">10. Click the Start button to run an algorithm.</span><br></pre></td></tr></table></figure><p>8：Estimate Model Performance(估算模型性能，不同的模型对应不同的指标)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Now that you know how to choose and configure different algorithms, you need to know how</span><br><span class="line">to evaluate the performance of an algorithm. In this lesson you are going to learn about the</span><br><span class="line">different ways to evaluate the performance of an algorithm in Weka.</span><br><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/diabetes.arff dataset.</span><br><span class="line">3. Click the Classify tab.</span><br><span class="line">The Test options pane lists the various different techniques that you can use to evaluate the</span><br><span class="line">performance of an algorithm.</span><br><span class="line">❼ The gold standard is 10-fold Cross-Validation. This is selected by default. For a small</span><br><span class="line">dataset, the number of folds can be adjusted from 10 to 5 or even 3.</span><br><span class="line">❼ If your dataset is very large and you want to evaluate algorithms quickly, you can use the</span><br><span class="line">Percentage split option. By default, this option will train on 66% of your dataset and use</span><br><span class="line">the remaining 34% to evaluate the performance of your model.</span><br><span class="line">❼ Alternately, if you have a separate file containing a validation dataset, you can evaluate</span><br><span class="line">your model on that by selecting the Supplied test set option. Your model will be trained</span><br><span class="line">on the entire training dataset and evaluated on the separate dataset.</span><br><span class="line">❼ Finally, you can evaluate the performance of your model on the whole training dataset.</span><br><span class="line">This is useful if you are more interested in a descriptive than a predictive model.</span><br><span class="line">Click the Start button to run a given algorithm with your chosen test option. Experiment</span><br><span class="line">with different Test options. Further refine the test options in the configuration provided by</span><br><span class="line">clicking the More options... button.</span><br></pre></td></tr></table></figure><p>9：Baseline Performance On Your Data(性能的基准线,zeroR算法用来评估基准线，只能选择比这个算法优的算法)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/diabetes.arff dataset.</span><br><span class="line">3. Click the Classify tab. The ZeroR algorithm is chosen by default.</span><br><span class="line">4. Click the Start button.</span><br></pre></td></tr></table></figure><p>10：分类算法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Weka provides a large number of classification algorithms. In this lesson you will discover 5 top</span><br><span class="line">classification algorithms that you can use on your classification problems.</span><br><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/diabetes.arff dataset.</span><br><span class="line">3. Click the Classify tab.</span><br><span class="line">4. Click the Choose button.</span><br><span class="line">5 Top algorithms that you can use for classification include:</span><br><span class="line">❼ Logistic Regression (functions.Logistic).</span><br><span class="line">❼ Naive Bayes (bayes.NaiveBayes).</span><br><span class="line">❼ k-Nearest Neighbors (lazy.IBk).</span><br><span class="line">❼ Classification and Regression Trees (trees.REPTree).</span><br><span class="line">❼ Support Vector Machines (functions.SMO).</span><br><span class="line">Experiment with each of these top algorithms. Try them out on different classification</span><br><span class="line">datasets, such as those with two classes and those with more.</span><br></pre></td></tr></table></figure><p>11：回归算法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/housing.arff dataset.</span><br><span class="line">3. Click the Classify tab.</span><br><span class="line">4. Click the Choose button.</span><br><span class="line">5 Top algorithms that you can use for regression include:</span><br><span class="line">❼ Linear Regression (functions.LinearRegression).</span><br><span class="line">❼ Support Vector Regression (functions.SMOReg).</span><br><span class="line">❼ k-Nearest Neighbors (lazy.IBk).</span><br><span class="line">❼ Classification and Regression Trees (trees.REPTree).</span><br><span class="line">❼ Artificial Neural Network (functions.MultilayerPerceptron).</span><br><span class="line"></span><br><span class="line">回归数据集下载地址http://www.cs.waikato.ac.nz/ml/weka/datasets.html</span><br></pre></td></tr></table></figure><p>12：Ensemble Algorithms（集成算法）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">在统计学和机器学习中，集成学习方法使用多种学习算法来获得比单独使用任何单独的学习算法更好的预测性能。不像统计力学中的系综通常是无限的，机器学习集合仅由一组具体的有限的可替代模型组成，但通常允许在这些可替代方案中存在更灵活的结构。</span><br><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/diabetes.arff dataset.</span><br><span class="line">3. Click the Classify tab.</span><br><span class="line">4. Click the Choose button.</span><br><span class="line">5 Top ensemble algorithms that you can use include:</span><br><span class="line">❼ Bagging (meta.Bagging).</span><br><span class="line">❼ Random Forest (trees.RandomForest).</span><br><span class="line">❼ AdaBoost (meta.AdaBoost).</span><br><span class="line">❼ Voting (meta.Voting).</span><br><span class="line">❼ Stacking (meta.Stacking).</span><br></pre></td></tr></table></figure><p>13： Tune Algorithm Parameters(调整算法参数)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. Open the Weka Chooser GUI.</span><br><span class="line">2. Click the Experimenter button to open the Weka Experiment Environment</span><br><span class="line">3. Click the New button.</span><br><span class="line">4. Click the Add new... button in the Datasets pane and select data/diabetes.arff.</span><br><span class="line">5. Click the Add new... button in the Algorithms pane and add 3 copes of the IBk algorithm.</span><br><span class="line">6. Click each IBk algorithm in the list and click the Edit selected... button and change KNN</span><br><span class="line">to 1, 3, 5 for each of the 3 different algorithms.</span><br><span class="line">7. Click the Run tab and click the Start button.</span><br><span class="line">8. Click the Analyse tab and click the Experiment button and then the Perform test button.</span><br></pre></td></tr></table></figure><p>14：Save Your Model（保存模型）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. Open the Weka GUI Chooser and then the Weka Explorer.</span><br><span class="line">2. Load the data/diabetes.arff dataset.</span><br><span class="line">3. Click the Classify tab.</span><br><span class="line">4. Change the Test options to Use training set and click the Start button.</span><br><span class="line">5. Right click on the results in the Result list and click Save model and enter a filename like</span><br><span class="line">diabetes-final</span><br></pre></td></tr></table></figure><p>15：使用模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. Right-click on the Result list click Load model and select your model file (diabetes-</span><br><span class="line">final.model).</span><br><span class="line">2. Change the Test options to Supplied test set and choose data/diabetes.arff (this could</span><br><span class="line">be a new file for which you do not have predictions)</span><br><span class="line">3. Click More options in the Test options and change Output predictions to Plain Text</span><br><span class="line">4. Right click on the loaded model and choose Re-evaluate model on current test set</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1：Weka&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
