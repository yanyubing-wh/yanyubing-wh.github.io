<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="鄢玉兵的博客" type="application/atom+xml" />






<meta name="description" content="机器学习数学基础1：概率论1:Probability and Machine Learning 123456❼ Noise in observations, e.g. measurement errors and random noise.❼ Incomplete coverage of the domain, e.g. you can never observe all data.❼ Imper">
<meta property="og:type" content="article">
<meta property="og:title" content="MachineLearning_math">
<meta property="og:url" content="https:&#x2F;&#x2F;yanyubing.xyz&#x2F;2020&#x2F;03&#x2F;22&#x2F;MachineLearning_math&#x2F;index.html">
<meta property="og:site_name" content="鄢玉兵的博客">
<meta property="og:description" content="机器学习数学基础1：概率论1:Probability and Machine Learning 123456❼ Noise in observations, e.g. measurement errors and random noise.❼ Incomplete coverage of the domain, e.g. you can never observe all data.❼ Imper">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2020-03-24T18:29:34.818Z">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://yanyubing.xyz/2020/03/22/MachineLearning_math/"/>





  <title>MachineLearning_math | 鄢玉兵的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
<a href="https://yanyubing.xyz"><img width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_red_aa0000.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">鄢玉兵的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yanyubing.xyz/2020/03/22/MachineLearning_math/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="鄢玉兵">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="鄢玉兵的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">MachineLearning_math</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-22T14:42:37+08:00">
                2020-03-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="机器学习数学基础"><a href="#机器学习数学基础" class="headerlink" title="机器学习数学基础"></a>机器学习数学基础</h1><h3 id="1：概率论"><a href="#1：概率论" class="headerlink" title="1：概率论"></a>1：概率论</h3><p>1:<strong>Probability and Machine</strong> <strong>Learning</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❼ Noise in observations, e.g. measurement errors and random noise.</span><br><span class="line">❼ Incomplete coverage of the domain, e.g. you can never observe all data.</span><br><span class="line">❼ Imperfect model of the problem, e.g. all models have errors, some are useful.Uncertainty in applied machine learning is managed using probability.</span><br><span class="line">❼ Probability and statistics help us to understand and quantify the expected value and variability of variables in our observations from the domain.</span><br><span class="line">❼ Probability helps to understand and quantify the expected distribution and density of observations in the domain.</span><br><span class="line">❼ Probability helps to understand and quantify the expected capability and variance in performance of our predictive models when applied to new data.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This is the bedrock of machine learning. On top of that, we may need models to predict a probability, we may use probability to develop predictive models (e.g. Naive Bayes), and we may use probabilistic frameworks to train predictive models (e.g. maximum likelihood estimation).</span><br></pre></td></tr></table></figure>

<p>2:<strong>Three Types of Probability</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">joint:联合概率 A且B一起发生的概率</span><br><span class="line">marginal：边缘概率 在边缘分布中，我们得到只关于一个变量的概率分布，而不再考虑另一变量的影响，实际上进行了降维操作。在实际应用中，例如人工神经网络的神经元互相关联，在计算它们各自的参数的时候，就会使用边缘分布计算得到某一特定神经元（变量）的值。</span><br><span class="line">conditional probability：条件概率  A发生的情况下B发生的概率</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">For this lesson, you must practice calculating joint, marginal, and conditional probabilities. For</span><br><span class="line">example, if a family has two children and the oldest is a boy, what is the probability of this</span><br><span class="line">family having two sons? This is called the Boy or Girl Problem and is one of many common toy</span><br><span class="line">problems for practicing probability.</span><br><span class="line"></span><br><span class="line">解答：两个孩子的性别可能的顺序是，男女，男男，女男，女女。</span><br><span class="line">已知第一个是男孩子，则条件概率的结果为1/4（两个男孩子的概率）除以1/2(第一个是男孩子的概率)=1/2</span><br></pre></td></tr></table></figure>

<p>3:<strong>Probability Distributions</strong> （概率分布）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">随机分布：</span><br><span class="line">❼ Discrete Random Variable. 离散随机分布</span><br><span class="line">❼ Continuous Random Variable.连续随机分布</span><br><span class="line"></span><br><span class="line">Discrete Probability Distributions（离散概率分布）</span><br><span class="line">❼ Poisson distribution.（伯努利分布）</span><br><span class="line">❼ Bernoulli and binomial distributions.（伯努利和二项分布）</span><br><span class="line">❼ Multinoulli and multinomial distributions.（多元分布和多项式分布）</span><br><span class="line"></span><br><span class="line">Continuous Probability Distributions(连续概率分布)</span><br><span class="line">❼ Normal or Gaussian distribution.(高斯分布）</span><br><span class="line">❼ Exponential distribution.</span><br><span class="line">❼ Pareto distribution.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># sample a normal distribution</span><br><span class="line"># Randomly Sample Gaussian Distribution（随机抽样高斯分布）</span><br><span class="line">from numpy.random import normal</span><br><span class="line"></span><br><span class="line"># define the distribution</span><br><span class="line"># 定义结构</span><br><span class="line">mu = 50</span><br><span class="line">sigma = 5</span><br><span class="line">n = 10</span><br><span class="line"># generate the sample</span><br><span class="line">sample = normal(mu, sigma, n)</span><br><span class="line">print(sample)</span><br></pre></td></tr></table></figure>

<p>4：<strong>Naive Bayes Classififier</strong>（朴素贝叶斯分类器）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Naive Bayes Classififier（朴素贝叶斯分类器）</span><br><span class="line"></span><br><span class="line"># example of gaussian naive bayes</span><br><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line"></span><br><span class="line"># generate 2d classification dataset</span><br><span class="line"># Generate isotropic Gaussian blobs for clustering.</span><br><span class="line">X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)</span><br><span class="line"></span><br><span class="line"># define the model</span><br><span class="line">model = GaussianNB()</span><br><span class="line"># fit the model，训练模型</span><br><span class="line">model.fit(X, y)</span><br><span class="line"># select a single sample()选择一个样本</span><br><span class="line">Xsample, ysample = [X[0]], y[0]</span><br><span class="line"># make a probabilistic prediction，概率预测</span><br><span class="line">yhat_prob = model.predict_proba(Xsample)</span><br><span class="line">print(&apos;Predicted Probabilities: &apos;, yhat_prob)</span><br><span class="line"># make a classification prediction</span><br><span class="line">yhat_class = model.predict(Xsample)</span><br><span class="line">print(&apos;Predicted Class: &apos;, yhat_class)</span><br><span class="line">print(&apos;Truth: y=%d&apos; % ysample)</span><br></pre></td></tr></table></figure>

<p>5：<strong>Entropy and Cross-Entropy</strong>（熵和互熵）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Low Probability Event: High Information (surprising).</span><br><span class="line">❼ High Probability Event: Low Information (unsurprising).</span><br><span class="line">低概率事件更具有价值(高信息值)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># example of calculating cross-entropy</span><br><span class="line">from math import log2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># calculate cross-entropy</span><br><span class="line">def cross_entropy(p, q):</span><br><span class="line">    return -sum([p[i] * log2(q[i]) for i in range(len(p))])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># define data</span><br><span class="line">p = [0.10, 0.40, 0.50]</span><br><span class="line">q = [0.80, 0.15, 0.05]</span><br><span class="line"># calculate cross-entropy H(P, Q)</span><br><span class="line">ce_pq = cross_entropy(p, q)</span><br><span class="line">print(&apos;H(P, Q): %.3f bits&apos; % ce_pq)</span><br><span class="line"># calculate cross-entropy H(Q, P)</span><br><span class="line">ce_qp = cross_entropy(q, p)</span><br><span class="line">print(&apos;H(Q, P): %.3f bits&apos; % ce_qp)</span><br></pre></td></tr></table></figure>

<p>6： <strong>Naive Classififiers</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 朴素分类器，一个简单概率验证</span><br><span class="line"></span><br><span class="line"># example of the majority class naive classifier in scikit-learn</span><br><span class="line">from numpy import asarray</span><br><span class="line">from sklearn.dummy import DummyClassifier</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line"># define dataset</span><br><span class="line">X = asarray([0 for _ in range(100)])</span><br><span class="line">class0 = [0 for _ in range(25)]</span><br><span class="line">class1 = [1 for _ in range(75)]</span><br><span class="line">y = asarray(class0 + class1)</span><br><span class="line"># reshape data for sklearn</span><br><span class="line">X = X.reshape((len(X), 1))</span><br><span class="line"># define model</span><br><span class="line">model = DummyClassifier(strategy=&apos;most_frequent&apos;)  # fit model</span><br><span class="line">model.fit(X, y)</span><br><span class="line"># print(X)</span><br><span class="line"># print(y)</span><br><span class="line"># make predictions</span><br><span class="line">yhat = model.predict(X)</span><br><span class="line"># print(yhat)</span><br><span class="line"># calculate accuracy</span><br><span class="line">accuracy = accuracy_score(y, yhat)</span><br><span class="line">print(&apos;Accuracy: %.3f&apos; % accuracy)</span><br></pre></td></tr></table></figure>

<p>7:<strong>Probability Scores</strong> (用于比较预测值和实际值的偏差的两种方式)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1:Log Loss Score</span><br><span class="line">2:Brier Score</span><br><span class="line"># A model with perfect skill has a log loss score of 0.0</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># example of log loss,评价预测值和实际值的差距</span><br><span class="line">from numpy import asarray</span><br><span class="line">from sklearn.metrics import log_loss</span><br><span class="line"># define data</span><br><span class="line">y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</span><br><span class="line">y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]</span><br><span class="line"># define data as expected, e.g. probability for each event &#123;0, 1&#125;</span><br><span class="line">y_true = asarray([[v, 1-v] for v in y_true])</span><br><span class="line">y_pred = asarray([[v, 1-v] for v in y_pred])</span><br><span class="line"># calculate log loss</span><br><span class="line">loss = log_loss(y_true, y_pred)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># example of brier loss</span><br><span class="line">from sklearn.metrics import brier_score_loss</span><br><span class="line"># define data</span><br><span class="line">y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</span><br><span class="line">y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]</span><br><span class="line"># calculate brier score</span><br><span class="line">score = brier_score_loss(y_true, y_pred, pos_label=1)</span><br><span class="line">print(score)</span><br></pre></td></tr></table></figure>

<h3 id="2：统计学"><a href="#2：统计学" class="headerlink" title="2：统计学"></a>2：统计学</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">统计学是应用数学的一个分支，主要通过利用概率论建立数学模型，收集所观察系统的数据，进行量化的分析、总结，并进而进行推断和预测，为相关决策提供依据和参考。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我的理解是：</span><br><span class="line">①为了更好的描述一些指标，让数据看起来更加直观！</span><br><span class="line">②更多的是在对数据的处理环节使用到的一些方法</span><br></pre></td></tr></table></figure>

<p>1：<strong>Statistics and Machine</strong> <strong>Learning</strong></p>
<p>1.1：数据准备过程需要的统计方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❼ Outlier detection.</span><br><span class="line">❼ Missing value imputation.</span><br><span class="line">❼ Data sampling.</span><br><span class="line">❼ Data scaling.</span><br><span class="line">❼ Variable encoding.</span><br><span class="line">❼ And much more.</span><br></pre></td></tr></table></figure>

<p>1.2：模型评估中的统计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Data sampling.</span><br><span class="line">❼ Data resampling.重采样技术进行模型评估，例如k-fold cross-validation(k倍交叉集验证)</span><br><span class="line">❼ Experimental design.</span><br></pre></td></tr></table></figure>

<p>1.3：模型选择中的统计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❼ Checking for a significant difference between results.</span><br><span class="line">❼ Quantifying the size of the difference between results.</span><br><span class="line">This might include the use of statistical hypothesis tests.</span><br></pre></td></tr></table></figure>

<p>1.4：模型总结中的统计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❼ Summarizing the expected skill of the model on average.</span><br><span class="line">❼ Quantifying the expected variability of the skill of the model in practice.</span><br></pre></td></tr></table></figure>

<p>1.5：预测值中的统计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">❼ Quantifying the expected variability for the prediction.</span><br></pre></td></tr></table></figure>

<p>2： <strong>Introduction to Statistics</strong>（统计概论）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❼ Descriptive Statistics: Descriptive statistics refer to methods for summarizing raw observations into information that we can understand and share.</span><br><span class="line">❼ Inferential Statistics: Inferential statistics is a fancy name for methods that aid in quantifying properties of the domain or population from a smaller set of obtained observations called a sample.</span><br></pre></td></tr></table></figure>

<p>3：<strong>Gaussian Distribution and</strong>  <strong>Descriptive Stats</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># calculate summary stats</span><br><span class="line">from numpy.random import seed</span><br><span class="line">from numpy.random import randn</span><br><span class="line">from numpy import mean</span><br><span class="line">from numpy import var</span><br><span class="line">from numpy import std</span><br><span class="line"></span><br><span class="line"># seed the random number generator</span><br><span class="line">seed(1)</span><br><span class="line"># generate univariate observations</span><br><span class="line">data = 5 * randn(10000) + 50</span><br><span class="line"># calculate statistics</span><br><span class="line">print(&apos;Mean: %.3f&apos; % mean(data))</span><br><span class="line">print(&apos;Variance: %.3f&apos; % var(data))</span><br><span class="line">print(&apos;Standard Deviation: %.3f&apos; % std(data))</span><br></pre></td></tr></table></figure>

<p>4:<strong>Correlation Between</strong> <strong>Variables</strong>(变量之间的相关性)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❼ Positive Correlation: Both variables change in the same direction.(正相关)</span><br><span class="line">❼ Neutral Correlation: No relationship in the change of the variables.（无关）</span><br><span class="line">❼ Negative Correlation: Variables change in opposite directions.（负相关）</span><br><span class="line">在数据中如果有两个变量相关性很强，则可以考虑删除其中一个变量，来提高算法的性能(执行效率)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># calculate correlation coefficient</span><br><span class="line">from numpy.random import seed</span><br><span class="line">from numpy.random import randn</span><br><span class="line">from scipy.stats import pearsonr</span><br><span class="line"># seed random number generator</span><br><span class="line">seed(1)</span><br><span class="line"># prepare data</span><br><span class="line">data1 = 20 * randn(1000) + 100</span><br><span class="line">data2 = data1 + (10 * randn(1000) + 50)</span><br><span class="line"># calculate Pearson&apos;s correlation</span><br><span class="line">corr, p = pearsonr(data1, data2)</span><br><span class="line"># display the correlation</span><br><span class="line">print(&apos;Pearsons correlation: %.3f&apos; % corr)</span><br></pre></td></tr></table></figure>

<p>5:<strong>Statistical Hypothesis Tests</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是用来判断样本与样本、样本与总体的差异是由抽样误差引起还是本质差别造成的统计推断方法。显著性检验是假设检验中最常用的一种方法，也是一种最基本的统计推断形式，其基本原理是先对总体的特征做出某种假设，然后通过抽样研究的统计推理，对此假设应该被拒绝还是接受做出推断。常用的假设检验方法有Z检验、t检验、卡方检验、F检验等 [1]  。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 统计假设性原则</span><br><span class="line"># student&apos;s t-test</span><br><span class="line">from numpy.random import seed</span><br><span class="line">from numpy.random import randn</span><br><span class="line">from scipy.stats import ttest_ind</span><br><span class="line"></span><br><span class="line"># seed the random number generator</span><br><span class="line">seed(1)</span><br><span class="line"># generate two independent samples</span><br><span class="line">data1 = 5 * randn(100) + 52</span><br><span class="line">data2 = 5 * randn(100) + 51</span><br><span class="line"># compare samples</span><br><span class="line">stat, p = ttest_ind(data1, data2)</span><br><span class="line">print(&apos;Statistics=%.3f, p=%.3f&apos; % (stat, p))</span><br><span class="line"># interpret</span><br><span class="line">alpha = 0.05</span><br><span class="line">if p &gt; alpha:</span><br><span class="line">    print(&apos;Same distributions (fail to reject H0)&apos;)</span><br><span class="line">else:</span><br><span class="line">    print(&apos;Different distributions (reject H0)&apos;)</span><br></pre></td></tr></table></figure>

<p>6:<strong>Estimation Statistics</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># calculate the confidence interval</span><br><span class="line">from statsmodels.stats.proportion import proportion_confint</span><br><span class="line"></span><br><span class="line"># calculate the interval</span><br><span class="line">lower, upper = proportion_confint(88, 100, 0.05)</span><br><span class="line">print(&apos;lower=%.3f, upper=%.3f&apos; % (lower, upper))</span><br></pre></td></tr></table></figure>

<p>7: <strong>Nonparametric Statistics</strong>(非参数统计)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># example of the mann-whitney u test</span><br><span class="line"># 同样用来验证两个样本的差异性</span><br><span class="line">from numpy.random import seed</span><br><span class="line">from numpy.random import rand</span><br><span class="line">from scipy.stats import mannwhitneyu</span><br><span class="line"></span><br><span class="line"># seed the random number generator</span><br><span class="line">seed(1)</span><br><span class="line"># generate two independent samples</span><br><span class="line">data1 = 50 + (rand(100) * 10)</span><br><span class="line">data2 = 51 + (rand(100) * 10)</span><br><span class="line"># compare samples</span><br><span class="line">stat, p = mannwhitneyu(data1, data2)</span><br><span class="line">print(&apos;Statistics=%.3f, p=%.3f&apos; % (stat, p))</span><br><span class="line"># interpret</span><br><span class="line">alpha = 0.05</span><br><span class="line">if p &gt; alpha:</span><br><span class="line">    print(&apos;Same distribution (fail to reject H0)&apos;)</span><br><span class="line">else:</span><br><span class="line">    print(&apos;Different distribution (reject H0)&apos;)</span><br></pre></td></tr></table></figure>

<h3 id="3：线性代数"><a href="#3：线性代数" class="headerlink" title="3：线性代数"></a>3：线性代数</h3><p>1：Linear Algebra for Machine Learning</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">You Need to Learn Linear Algebra Notation</span><br><span class="line">You Need to Learn Linear Algebra Arithmetic</span><br><span class="line">You Need to Learn Linear Algebra for Statistics</span><br><span class="line">You Need to Learn Matrix Factorization</span><br><span class="line">You Need to Learn Linear Least Squares</span><br><span class="line"></span><br><span class="line">您需要学习线性代数符号</span><br><span class="line">您需要学习线性代数算法</span><br><span class="line">您需要学习用于统计的线性代数</span><br><span class="line">您需要学习矩阵分解</span><br><span class="line">您需要学习线性最小二乘</span><br></pre></td></tr></table></figure>

<p>2： <strong>Linear Algebra</strong></p>
<p> <strong>Linear Algebra</strong>(线性代数)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">线性代数是数学的一个分支，但事实是线性代数是数据的数学。 矩阵和向量是数据的语言。 线性代数是关于线性组合的。 也就是说，对称为向量的数字列和称为矩阵的数字2D数组进行算术运算，以创建新的数字列和数组。</span><br></pre></td></tr></table></figure>

<p><strong>Numerical Linear Algebra</strong> （数值线性代数）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">线性代数在计算机中的应用通常称为数值线性代数。 它不仅仅是在代码库中实现线性代数运算； 它还包括认真处理应用数学的问题，例如使用数字计算机有限的浮点精度进行处理。</span><br></pre></td></tr></table></figure>

<p><strong>Applications of Linear Algebra</strong>（应用线性代数）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">❼ Matrices in Engineering, such as a line of springs.</span><br><span class="line">❼ Graphs and Networks, such as analyzing networks.</span><br><span class="line">❼ Markov Matrices, Population, and Economics, such as population growth.</span><br><span class="line">❼ Linear Programming, the simplex optimization method.</span><br><span class="line">❼ Fourier Series, Linear Algebra for functions, used widely in signal processing.</span><br><span class="line">❼ Linear Algebra for statistics and probability, such as least squares for regression.</span><br><span class="line">❼ Computer Graphics, such as the various translation, scaling and rotation of images.</span><br></pre></td></tr></table></figure>

<p>3:<strong>Vectors</strong>(向量) </p>
<p><strong>Defifining a Vector</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">We can represent a vector in Python as a NumPy array. A NumPy array can be created from a list of numbers. For example, below we defifine a vector with the length of 3 and the integer values 1, 2 and 3.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create a vector</span><br><span class="line">from numpy import array</span><br><span class="line">v = array([1, 2, 3])</span><br><span class="line">print(v)</span><br></pre></td></tr></table></figure>

<p><strong>Vector Multiplication</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># multiply vectors</span><br><span class="line">from numpy import array</span><br><span class="line">a = array([1, 2, 3])</span><br><span class="line">print(a)</span><br><span class="line">b = array([1, 2, 3])</span><br><span class="line">print(b)</span><br><span class="line">c = a * b</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>

<p><strong>Defifining a Matrix</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create matrix</span><br><span class="line">from numpy import array</span><br><span class="line">A = array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">print(A)</span><br></pre></td></tr></table></figure>

<p><strong>Matrix Addition</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># add matrices</span><br><span class="line">from numpy import array</span><br><span class="line">A = array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">print(A)</span><br><span class="line">B = array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">print(B)</span><br><span class="line">C = A + B</span><br><span class="line">print(C)</span><br></pre></td></tr></table></figure>

<p><strong>Matrix Dot Product</strong> (矩阵点集,乘法)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">C(m, k) = A(m, n) × B(n, k)</span><br><span class="line"></span><br><span class="line"># matrix dot product</span><br><span class="line">from numpy import array</span><br><span class="line">A = array([[1, 2], [3, 4], [5, 6]])</span><br><span class="line">print(A)</span><br><span class="line">B = array([[1, 2], [3, 4]])</span><br><span class="line">print(B)</span><br><span class="line">C = A.dot(B)</span><br><span class="line">print(C)</span><br></pre></td></tr></table></figure>

<p> 4:Matrix Types and Operations</p>
<p><strong>Transpose</strong>(转置)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># transpose matrix</span><br><span class="line">from numpy import array</span><br><span class="line">A = array([[1, 2], [3, 4], [5, 6]])</span><br><span class="line">print(A)</span><br><span class="line">C = A.T</span><br><span class="line">print(C)</span><br></pre></td></tr></table></figure>

<p><strong>Inversion</strong>(逆)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># invert matrix</span><br><span class="line">from numpy import array</span><br><span class="line">from numpy.linalg import inv</span><br><span class="line"># define matrix</span><br><span class="line">A = array([[1.0, 2.0], [3.0, 4.0]])</span><br><span class="line">print(A)</span><br><span class="line"># invert matrix</span><br><span class="line">B = inv(A)</span><br><span class="line">print(B)</span><br></pre></td></tr></table></figure>

<p><strong>Square Matrix</strong> (方阵)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A square matrix is a matrix where the number of rows (n) equals the number of columns (m).</span><br></pre></td></tr></table></figure>

<p><strong>Symmetric Matrix</strong>(对称矩阵)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A symmetric matrix is a type of square matrix where the top-right triangle is the same as the bottom-left triangle. To be symmetric, the axis of symmetry is always the main diagonal of the matrix, from the top left to the bottom right. A symmetric matrix is always square and equal to its own transpose.</span><br></pre></td></tr></table></figure>

<p><strong>Triangular Matrix</strong>(三角矩阵)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A triangular matrix is a type of square matrix that has all values in the upper-right or lower-left of the matrix with the remaining elements filled with zero values. A triangular matrix with values only above the main diagonal is called an upper triangular matrix. Whereas, a triangular matrix with values only below the main diagonal is called a lower triangular matrix.</span><br></pre></td></tr></table></figure>

<p><strong>Diagonal Matrix</strong>(对角矩阵)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A diagonal matrix is one where values outside of the main diagonal have a zero value, where the main diagonal is taken from the top left of the matrix to the bottom right. A diagonal matrix is often denoted with the variable D and may be represented as a full matrix or as a vector of values on the main diagonal.</span><br></pre></td></tr></table></figure>

<p>5:<strong>Matrix Factorization</strong> (矩阵分解)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">矩阵分解是将矩阵还原成其组成部分的一种方式。 这是一种可以简化更复杂的矩阵运算的方法，该运算可以在分解矩阵上执行，而不能在原始矩阵本身上执行。 矩阵分解的一个常见类比是数的分解，例如将25分解为5×5。因此,像分解实数值一样，有许多方法可以分解矩阵，因此存在多种不同的矩阵分解技术。</span><br></pre></td></tr></table></figure>

<p><strong>LU Matrix Decomposition</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">The LU decomposition is for square matrices and decomposes a matrix into L and U components.</span><br><span class="line"></span><br><span class="line">A = L · U (12)</span><br><span class="line"></span><br><span class="line">Where A is the square matrix that we wish to decompose, L is the lower triangle matrix and U is the upper triangle matrix. A variation of this decomposition that is numerically more stable to solve in practice is called the LUP decomposition, or the LU decomposition with partial pivoting.</span><br><span class="line"></span><br><span class="line">A = P · L · U (13)</span><br><span class="line"></span><br><span class="line">The rows of the parent matrix are re-ordered to simplify the decomposition process and the additional P matrix specifies a way to permute the result or return the result to the original order. There are also other variations of the LU. The LU decomposition is often used to simplify the solving of systems of linear equations, such as finding the coefficients in a linear regression.</span><br><span class="line">The LU decomposition can be implemented in Python with the lu() function. More specifically,this function calculates an LPU decomposition.</span><br></pre></td></tr></table></figure>

<p>Singular-Value Decomposition(奇异值分解)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler.</span><br><span class="line"></span><br><span class="line">A = U · Σ · V T (14)</span><br><span class="line"></span><br><span class="line">Where A is the real m × n matrix that we wish to decompose, U is an m × m matrix, Σ</span><br><span class="line">(sigma) is an m × n diagonal matrix, and V T is the transpose of an n × n matrix where T is a superscript.</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

	<div>
  
    ﻿<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

    

		

    

    


	
    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/03/22/MachineLearning_Test/" rel="next" title="MachineLearning_Test">
                <i class="fa fa-chevron-left"></i> MachineLearning_Test
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/03/25/MachineLearningAlgorithms/" rel="prev" title="MachineLearningAlgorithms">
                MachineLearningAlgorithms <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">鄢玉兵</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">59</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习数学基础"><span class="nav-number">1.</span> <span class="nav-text">机器学习数学基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1：概率论"><span class="nav-number">1.0.1.</span> <span class="nav-text">1：概率论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2：统计学"><span class="nav-number">1.0.2.</span> <span class="nav-text">2：统计学</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3：线性代数"><span class="nav-number">1.0.3.</span> <span class="nav-text">3：线性代数</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">鄢玉兵</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
