<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="鄢玉兵的博客" type="application/atom+xml" />






<meta name="description" content="1：神经元 123构建：包含初始化[w]，b；激活函数，前馈网络完成inputs→outputs的功能  2：神经网络 123构建：多个神经元构成，初始化[w],[b]；前馈网络完成inputs→outputs的功能  3：神经网络的训练 1构建：定义损失函数，随机初始化[w],[b];前向传播;训练方法（通过随机梯度下降更新权重和偏置的过程，包含学习率，偏导，和批次数）;每10个epoch打印一">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络学习记录（已掌握）">
<meta property="og:url" content="https:&#x2F;&#x2F;yanyubing.xyz&#x2F;2020&#x2F;04&#x2F;20&#x2F;%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E5%B7%B2%E6%8E%8C%E6%8F%A1%EF%BC%89&#x2F;index.html">
<meta property="og:site_name" content="鄢玉兵的博客">
<meta property="og:description" content="1：神经元 123构建：包含初始化[w]，b；激活函数，前馈网络完成inputs→outputs的功能  2：神经网络 123构建：多个神经元构成，初始化[w],[b]；前馈网络完成inputs→outputs的功能  3：神经网络的训练 1构建：定义损失函数，随机初始化[w],[b];前向传播;训练方法（通过随机梯度下降更新权重和偏置的过程，包含学习率，偏导，和批次数）;每10个epoch打印一">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2020-05-15T17:20:51.864Z">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://yanyubing.xyz/2020/04/20/神经网络学习记录（已掌握）/"/>





  <title>神经网络学习记录（已掌握） | 鄢玉兵的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
<a href="https://yanyubing.xyz"><img width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_red_aa0000.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">鄢玉兵的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yanyubing.xyz/2020/04/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E5%B7%B2%E6%8E%8C%E6%8F%A1%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="鄢玉兵">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="鄢玉兵的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">神经网络学习记录（已掌握）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-20T01:02:14+08:00">
                2020-04-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>1：神经元</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">构建：包含初始化[w]，b；激活函数，前馈网络</span><br><span class="line"></span><br><span class="line">完成inputs→outputs的功能</span><br></pre></td></tr></table></figure>

<p>2：神经网络</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">构建：多个神经元构成，初始化[w],[b]；前馈网络</span><br><span class="line"></span><br><span class="line">完成inputs→outputs的功能</span><br></pre></td></tr></table></figure>

<p>3：神经网络的训练</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">构建：定义损失函数，随机初始化[w],[b];前向传播;训练方法（通过随机梯度下降更新权重和偏置的过程，包含学习率，偏导，和批次数）;每10个epoch打印一次准确率或者损失值等，方便可视化</span><br></pre></td></tr></table></figure>

<p>4：神经网络的预测</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">就是神经网络的前向传播</span><br></pre></td></tr></table></figure>

<p>5：计算机视觉发展史</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如何让计算机理解视觉(圆柱表示法,直线表示法)→人脸识别（2000年）→SIFT（对象识别，基于特征（因为匹配整个物体非常困难（有很多干扰因素））））→图像金字塔，不同的分辨率带有同样的特征（2006）→梯度直方图（HoG，用来描述特征，表示向量机）→（2006-2012数据集的发展）开始对象识别（发展基于标准数据集的产生,PASCAL;imageNet(最大的数据集项目)）→大赛中卷积神经网络展现身手（2012年，使得错误率下降了10%，前五类识别）</span><br></pre></td></tr></table></figure>

<p>6：数据增强</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1：文字扭曲（贝塞尔曲线实现）</span><br><span class="line">2：背景噪声（椒盐）</span><br><span class="line">椒盐噪声 = 椒噪声 + 盐噪声 ，椒盐噪声的值为0(黑色)或者255(白色)</span><br><span class="line">3：笔画粘连（膨胀）</span><br><span class="line">4：笔画断裂（腐蚀）</span><br><span class="line">5：风格迁移</span><br><span class="line">6：ImageEnhance（增强亮度，色泽等）</span><br><span class="line">7：滤镜</span><br><span class="line">8：加下划线（适用于下划线上面是答案的情况）</span><br><span class="line">9：底色加数字（适用于答案会写在数字上的情况）</span><br><span class="line">10：左右镜像</span><br><span class="line">11：随机裁剪</span><br><span class="line">12：随机擦除</span><br><span class="line">13:旋转</span><br><span class="line">14：剪切（shearing）</span><br><span class="line">15：局部扭曲变换（Local warping）</span><br><span class="line">16：color shifting(调整rgb的值，保持失真)</span><br><span class="line">17:PCA主成分分析（颜色增强），使得RGB相对一致</span><br><span class="line"></span><br><span class="line">一般不做本地数据增强，在代码中动态增强，数据增强过程中使用的到参数，也可以作为超参数来调整</span><br></pre></td></tr></table></figure>

<p>7：物体识别的问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：物体相机位置不一样，像素不一样</span><br><span class="line">2：物体光照，阴影等不一样</span><br><span class="line">3：物体只有部分可视</span><br><span class="line">4：背景和物体颜色类似</span><br><span class="line">5：物体的品种不一样（猫）</span><br></pre></td></tr></table></figure>

<p>8：数据驱动方法(Data-driver method)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1:收集数据</span><br><span class="line">2:训练模型</span><br><span class="line">3：使用模型预测</span><br><span class="line"></span><br><span class="line">def train(images,labels):</span><br><span class="line">#meching learning</span><br><span class="line">	return model</span><br><span class="line">	</span><br><span class="line">def predict(test_image,model):</span><br><span class="line">	return test_labels</span><br></pre></td></tr></table></figure>

<p>9：第一个分类器(nearest neighbor)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据集使用CIFAR10</span><br><span class="line">原理就是对图片进行对比</span><br><span class="line">两个图片的对比第一种方式：L1曼哈顿距离(围成了矩形，会随着坐标旋转改变)</span><br><span class="line">sum(|Test[:]-train[:]|)，两个对应坐标像素的绝对值的和</span><br><span class="line">两个图片的对比第二种方式：L2欧几里得距离（围成的是圆，坐标不变）</span><br><span class="line">sum((Test[:]-train[:])的平方开根号）</span><br><span class="line"></span><br><span class="line">对于knn，你需要决策的是k取值和距离的取值，距离的标识L1或者L2（超参数）</span><br></pre></td></tr></table></figure>

<p>10：对于超参数的选择</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对于knn：</span><br><span class="line">K=1的时候训练更好（错误，需要多调整）</span><br><span class="line">超参数在测试集上更好就选择（错误，需要均衡）</span><br><span class="line">数据分为训练集，验证集和测试集：在验证集上选择超参数，测试集上测试（？暂时不理解）</span><br><span class="line">交叉验证是一个很好的设置超参数方式(但是不适合大型数据集，时间昂贵)</span><br></pre></td></tr></table></figure>

<p>11:SVM损失函数（Loss function）hinge loss(铰接损失)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">在训练集上，通过得分来衡量不准确率。对于不同给定的权值w，来衡量不同w的优劣。</span><br><span class="line">label:标签，用来定义输出类型,数据类型为Interage</span><br><span class="line">		score</span><br><span class="line">label   True 	cat  	car		frog</span><br><span class="line">cat 			3.2		1.3		2.2</span><br><span class="line">car				5.1		4.9		2.5</span><br><span class="line">frog			-1.7	2.0		-3.1</span><br><span class="line"></span><br><span class="line">(每一类：如frog)Loss=max(2.2-(-3.1)+1,0)+max(2.5-(-3.1)+1,0)（True得分减去其他类得分&gt;1则置0，小于0取实际值）</span><br><span class="line">Li=∑max(Sj-syi+1,0)			(j≠i)</span><br><span class="line">(Sj为预测的类别得分，Syi为预测为实际类别的得分):目的是让我们通过权值w得到的准确分类，真实类别的得分要比错误类别的得分大于1;继而损失函数=0的时候，刚好等于1。</span><br><span class="line">平均损失：sum/N</span><br><span class="line"></span><br><span class="line">1:如果car的得分改变一点，对损失函数是否有影响？</span><br><span class="line">没有影响，因为car的分数已经超过了其他的分数，改变一点之后，损失函数还是0</span><br><span class="line"></span><br><span class="line">2:损失函数的取值范围</span><br><span class="line">0~+∞</span><br><span class="line"></span><br><span class="line">3:如果使用的是平方损失？意义是什么</span><br><span class="line">平方损失的意义在于，我们改变不同的w，看看到底那个地方对于损失的影响更大</span><br><span class="line"></span><br><span class="line">4:这种损失函数在numpy中的表示</span><br><span class="line"></span><br><span class="line">5:得到损失函数为0的权值并不唯一</span><br><span class="line">w得到的损失为0，那么2W同样得到损失函数也为0</span><br><span class="line"></span><br><span class="line">6:我们并不关心在训练集上的损失函数，而是在测试集上面的分类准确情况</span><br><span class="line"></span><br><span class="line">7:hinge loss(铰接损失)有时候又被称为最大边界损失（max-margin loss）</span><br></pre></td></tr></table></figure>

<p>12:Regularization(损失函数的正则化)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">正则化是为了使得事情变得简单化:</span><br><span class="line">L1 regularization</span><br><span class="line">L2 regularization</span><br><span class="line">Dropout regularization:常用,防止过拟合，可以区别共同特征;但是会增加训练时间，因为每次会丢弃一部分的权重更新</span><br><span class="line">Max-Norm Regularization</span><br><span class="line">随机深度：不属于对损失函数的正则化，（对于很深的网络），思路是在训练的时候舍弃一些层，但是在测试的时候使用原样，效果好</span><br><span class="line">等等...</span><br><span class="line">都是通过调整损失函数来实现正则化的</span><br></pre></td></tr></table></figure>

<p>13:softmax classifier</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">score取值范围调整为0-1,表达式为:np.exp(a) / np.sum(np.exp(a)) </span><br><span class="line">强调了最大得分，忽略了比较小的得分（通常情况下，并非标量不变）:</span><br><span class="line">例如：</span><br><span class="line">得分为：[1、2、3、4、1、2、3]</span><br><span class="line">softmax之后为：[0.024、0.064、0.175、0.475、0.024、0.064、0.175]</span><br><span class="line">最高值的比例为4/16&lt;0.475</span><br><span class="line"></span><br><span class="line">但是有时候会出现如下情况</span><br><span class="line">得分为：[0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3] </span><br><span class="line">softmax之后为：[0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153]</span><br><span class="line">最大值0.4/1.6&gt;0.169</span><br></pre></td></tr></table></figure>

<p>14:softMax的lossfunction</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sofeMax的lossfunction:-logSj   </span><br><span class="line">(sj为softmax之后对应种类的概率)，</span><br><span class="line">例如样本为[0,0,0,0,0,0,1]</span><br><span class="line">softmax之后的概率为[0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153]</span><br><span class="line">则对于该次预测的loss值为-log(0.153)，Sj越大，证明预测性能更好，对应的loss越低，最低趋近于0</span><br><span class="line"></span><br><span class="line">1:lossfunction的取值</span><br><span class="line">lossfunction的取值为0-+∞</span><br><span class="line"></span><br><span class="line">2:初始化权值都很小的时候，lossfunction的值</span><br><span class="line">-log(1/c)  c为类别的个数</span><br><span class="line"></span><br><span class="line">3:所以如果你的概率是通过softmax公式得到的，那么cross entropy就是softmax loss</span><br></pre></td></tr></table></figure>

<p>15：SVM loss  vs  Softmax loss</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">常用损失函数</span><br><span class="line">常见的损失误差有五种：</span><br><span class="line">1. 铰链损失（Hinge Loss）：主要用于支持向量机（SVM） 中；</span><br><span class="line">2. 互熵损失 （Cross Entropy Loss，Softmax Loss ）：用于Logistic 回归与Softmax 分类中；</span><br><span class="line">3. 平方损失（Square Loss）：主要是最小二乘法（OLS）中；</span><br><span class="line">4. 指数损失（Exponential Loss） ：主要用于Adaboost 集成学习算法中；</span><br><span class="line">5. 其他损失（如0-1损失，绝对值损失）</span><br></pre></td></tr></table></figure>

<p>16:优化器（optimization）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">方案1:随机搜索,随机化权重，使得损失函数最小（实现困难，并且耗时巨大，准确率低）</span><br><span class="line"></span><br><span class="line">方案2:梯度（但是太过于缓慢，如果w的维度为1000万）</span><br><span class="line">十分类问题，w为1*10的矩阵，loss在固定的W为固定值，那么每一个w（其他w值固定不变）增加为W+h(first dim)，得到对应的loss，∆loss/∆w就为对应的梯度，处理10次就得到每一个的梯度。</span><br><span class="line"></span><br><span class="line">方案3:Calculus（微积分）</span><br><span class="line">直接就可以得到斜率（不需要每个增加微小值h(0.001)）</span><br><span class="line"></span><br><span class="line">总结：使用分析梯度下降（快，但是有错误情况），结合数值梯度来进行检查，称为gradient check(梯度检查,debug可以用到)</span><br><span class="line"></span><br><span class="line">4:step_size(leraning rate)作为超参数</span><br><span class="line"></span><br><span class="line">5:SGD(随机梯度下降)</span><br><span class="line">我们处理图片训练的时候，可能参数有数百万个，随机梯度下降解决的问题是不需要进行一整个循环来得到梯度，而是每个minibatch(32,64,128)就可以得到梯度;代码上多增加了一行采样训练数据</span><br><span class="line"></span><br><span class="line">6:更加优秀的梯度下降</span><br><span class="line">Adaptive Learning Algorithms(自适应学习率算法):Adagrad，Adadelta，RMSprop，Adam，优于SGD（随机梯度下降）算法</span><br><span class="line"></span><br><span class="line">7：具有动量的SGD可以越过局部最小值和鞍点，一般使用Adam</span><br></pre></td></tr></table></figure>

<p>17:线性分类svm，损失函数的直观了解</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/</span><br></pre></td></tr></table></figure>

<p>18:反向传播</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">f(x,y,z)=(x+y)*z</span><br><span class="line">e.g. x=-2 y=5 z=-4</span><br><span class="line"></span><br><span class="line">表达式分解为：</span><br><span class="line">q=x+y</span><br><span class="line">f=q*z</span><br><span class="line"></span><br><span class="line">反向传播的导数遵循链式法则：</span><br><span class="line">∂q/∂x=1</span><br><span class="line">∂q/∂y=1</span><br><span class="line"></span><br><span class="line">∂f/∂q=z</span><br><span class="line">∂f/∂z=q</span><br><span class="line"></span><br><span class="line">即∂f/∂x=∂f/∂q * ∂q/∂x=z*1=z=-4</span><br></pre></td></tr></table></figure>

<p>19:Patterns in backward flow(反向传播的模式)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">add gate(加法):gradient distributor(梯度分布:两个值梯度一样)</span><br><span class="line">max gate(取最大值):gradient router(梯度路由:其中一个值的梯度置0)</span><br><span class="line">mul gate(乘法):gradient switcher(梯度切换:梯度分别为另一个值)</span><br><span class="line"></span><br><span class="line">Gradient add at branches：分支的时候梯度为求分支求和</span><br><span class="line"></span><br><span class="line">向量计算的梯度：</span><br><span class="line">向量中元素的梯度总是一样的，</span><br></pre></td></tr></table></figure>

<p>20:Modeularized implementation(模块化)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">框架中：caffe torch TensorFlow 都是在实现正向和方向传播</span><br></pre></td></tr></table></figure>

<p>21:Convolutional Neural Networks(CNN)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">全连接层：input为32*32*3的图片</span><br><span class="line">那么input展开就为3072*1的矩阵；需要的w就为10*3072的矩阵，最终经过激活函数(可以是sigmod)，得到10个种类的得分</span><br><span class="line">需要的参数为30720个</span><br><span class="line"></span><br><span class="line">卷积层:input是32*32*3的图片</span><br><span class="line">保留原结构，经过5*5*3的卷积，进行运算，得到28*28的特征图;如果有6个过滤器，则可以得到6个特征图，合起来就是28*28*6；再经过10个5*5*6的过滤器，得到10个24*24的特征图</span><br><span class="line"></span><br><span class="line">输出层的大小=（N-F）/stride+1 </span><br><span class="line">N:输入层的大小</span><br><span class="line">F:过滤器的大小</span><br><span class="line">stride：步长</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">输出层的大小=（N+2p-F）/stride+1 </span><br><span class="line">pad:填充，一般使用0像素填充(为了保持输出图像的大小，再深层网络中多个卷积下来之后图片大小会急剧下降)</span><br><span class="line"></span><br><span class="line">练习：input：32*32*3  10个5*5的过滤器 步长为1  pad为2 输出是？参数的个数是？</span><br><span class="line">(32+4-5+1)/1的10个，最终为32*32*10;</span><br><span class="line">参数的个数为（5*5*3+1）*10=760个，每个过滤器都会加一个偏置项Bias</span><br><span class="line"></span><br><span class="line">过滤器的数量一般为2的次方个,32,64,128...</span><br><span class="line"></span><br><span class="line">池化层：也叫下采样层(减小特征图的大小),一般采用最大池化,池化的步长一般也和过滤器大小一致，保证不会有重叠区域被池化；目的是进行下采样</span><br><span class="line"></span><br><span class="line">直观演示地址：</span><br><span class="line">https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html</span><br></pre></td></tr></table></figure>

<p>22:激活函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">激活函数一般用在求出了h=w1*x1+w2*x2+b，之后f(h)</span><br><span class="line">有下面的种类f(x)：</span><br><span class="line"></span><br><span class="line">Sigmoid:取值为0~1</span><br><span class="line">问题点:</span><br><span class="line">1：当x=10,或者x=-10的时候，d(f(x))/d(x)梯度≈0；则d(f(x))/d(w)也≈0，会产生梯度消失，不利于参数更新</span><br><span class="line">2：对于向量w而言；输入x为正，梯度始终为正；或者输入x为负，梯度始终为负；这意味着我们需要使用的输入数据是均值为0</span><br><span class="line">3:Sigmoid计算昂贵（并不是很严重的问题）</span><br><span class="line"></span><br><span class="line">tanh:取值为-1~1</span><br><span class="line">问题点：</span><br><span class="line">解决了Sigmoid的第二个问题,但是还是存在第一个问题</span><br><span class="line"></span><br><span class="line">ReLU:取值0~正无穷(第一次使用是在2012 AlexNet)</span><br><span class="line">1：存在负值梯度为0的情况</span><br><span class="line">2：收敛较快（大概是sigmoid的6倍）</span><br><span class="line">3：初始化weight可能存在永远都不更新w的情况，对于所有的输入（w1*x1+w2*x2+b&lt;0），使得f(x)/d(w)=0;部分神经元死掉（不更新，不起作用）</span><br><span class="line"></span><br><span class="line">Leaky ReLu:取值-无穷~正无穷</span><br><span class="line">max(0.01x,x),解决了h的取值为负数，梯度为0的情况</span><br><span class="line"></span><br><span class="line">PRelu:max(αx,x)</span><br><span class="line">α通过超参数给定，更加灵活</span><br><span class="line"></span><br><span class="line">ELU：Rlue的另一个变种</span><br><span class="line"></span><br><span class="line">Maxout:max(w1Tx+b1,w2Tx+b2)</span><br><span class="line">1：需要两组权值</span><br><span class="line"></span><br><span class="line">使用激活函数的经验：</span><br><span class="line">1:不使用sigmoid</span><br><span class="line">2:是要ReLU,注意学习率</span><br><span class="line">3:尝试ReLU的变种</span><br><span class="line">4:尝试tanh（不做期待）</span><br></pre></td></tr></table></figure>

<p>23:数据预处理（data Preprocess）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1:zero-mean(0均值):</span><br><span class="line">I=D-Imean</span><br><span class="line">Imean可以是整个图片的均值（3通道）AlexNet，也可以是（每个通道的均值）vgg网络中是这样的</span><br><span class="line"></span><br><span class="line">2:Normalized data:归一化(CNN不常见)</span><br><span class="line">I = Imin + (Imax-Imin)*(D-Dmin)/(Dmax-Dmin)</span><br><span class="line"></span><br><span class="line">3:PCA和白化(CNN不常见)</span><br><span class="line"></span><br><span class="line">注意点：</span><br><span class="line">1:正确做法是计算训练数据的均值，然后分别把它从训练/验证/测试数据中减去。</span><br><span class="line">2:数据预处理不能解决sigmoid的劣势（梯度消失），因为预处理只能处理第一层的输入数据结构</span><br><span class="line">3:数据预处理可以使得数据对损失不太敏感，更好的处理;偏离中心的数据移动到中心</span><br></pre></td></tr></table></figure>

<p>24：（权值初始化）weight Initalization</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">问题</span><br><span class="line">1：初始化W=0会出现什么?</span><br><span class="line">大部分神经元的梯度相同，做了一样的事情，激活函数一样的时候,w=0，则得到的损失函数一致，反向梯度也一致，即会用同样的方式更新。</span><br><span class="line"></span><br><span class="line">2：W=0.01*np.random.randn(D,N)？</span><br><span class="line">均值为0,标准差为0.01;网络比较小的时候可行，深层网络不可行;网络的深入，所有的梯度都会变为0</span><br><span class="line"></span><br><span class="line">3:W=np.random.randn(fan_in.fan_out)</span><br><span class="line">标准差为1，所有神经元都将处于饱和状态，梯度消失</span><br><span class="line"></span><br><span class="line">4:Xavier初始化(最常用)</span><br><span class="line">W=np.random.randn(fan_in,fan_out)/np.sqrt(fan_in)</span><br><span class="line"></span><br><span class="line">5:note additional/2</span><br><span class="line">W=np.random.randn(fan_in,fan_out)/np.sqrt(fan_in/2)</span><br><span class="line"></span><br><span class="line">6:batch Normalization（批处理归一化）</span><br><span class="line">BN算法在Mini-batch中使用，一般在全连接层或者卷积层之后使用，或者在非线性层之前</span><br></pre></td></tr></table></figure>

<p>25：怎么做迁移学习</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">以分类问题而言：</span><br><span class="line">一：你的训练集小，类别少</span><br><span class="line">①你要做的是5分类问题</span><br><span class="line">②下载github上面imageNet网络和对应的权重</span><br><span class="line">③思路一：修改最后一层的softmax层(对应自己的输出类别)，同时冻结前面的权重，只需要训练最后一层的权重</span><br><span class="line">④思路二：把输入图片直到最后一层前的激活值储存到硬盘中，然后每次只需要训练一个一层网络即可(读取激活值进行softmax)</span><br><span class="line"></span><br><span class="line">经验而言：你的训练集越大，你需要冻结的层数减少。足够大的时候，可能不需要冻结任何一层</span><br></pre></td></tr></table></figure>

<p>26：GPU和CPU的比较</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GPU：内核多，适合做并行运算，例如矩阵</span><br><span class="line">CPU：内核少</span><br><span class="line"></span><br><span class="line">openCL使用英伟达或者AMD的GPU，或者可以在CPU上跑，但是整体而言不行</span><br></pre></td></tr></table></figure>

<p>17：深度学习框架</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">caffe caffe2:Facebook</span><br><span class="line">PyTorch:Facebook</span><br><span class="line">TensorFlow:Google</span><br><span class="line">Paddle:Baidu</span><br><span class="line">CNTK:Microsoft</span><br><span class="line">MXNet:(Amazon)</span><br><span class="line"></span><br><span class="line">框架的优势：</span><br><span class="line">①简单的构建大型的计算图</span><br><span class="line">②自动计算梯度</span><br><span class="line">③有效的再GPU上运行(numpy无法在GPU上运行)</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

	<div>
  
    ﻿<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

    

		

    

    


	
    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/04/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%EF%BC%88%E5%8D%B7%E7%A7%AF%EF%BC%89/" rel="next" title="神经网络相关（卷积）">
                <i class="fa fa-chevron-left"></i> 神经网络相关（卷积）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/05/01/python_gui/" rel="prev" title="python_gui">
                python_gui <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">鄢玉兵</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">101</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">鄢玉兵</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
